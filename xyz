https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=0
found
 LINK 
https://labs.yahoo.com/publications/8514/when-crowd-not-enough-improving-user-experience-social-media-through-automatic
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the experi- ence for the majority of users. To address the latter issue, the prevailing solution is to rely on the wisdom of the crowds to promote good content (e.g., via votes or like buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this pa- per, we investigate the effects, on the users, of automatically filtering question-answering content, using a combination of syntactic, semantic, and social signals. Using this filtering, a large-scale experiment with real users was performed to mea- sure the resulting engagement and satisfaction. To our knowl- edge, this experiment represents the first reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality filtering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the experi- ence for the majority of users. To address the latter issue, the prevailing solution is to rely on the wisdom of the crowds to promote good content (e.g., via votes or like buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this pa- per, we investigate the effects, on the users, of automatically filtering question-answering content, using a combination of syntactic, semantic, and social signals. Using this filtering, a large-scale experiment with real users was performed to mea- sure the resulting engagement and satisfaction. To our knowl- edge, this experiment represents the first reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality filtering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the experi- ence for the majority of users. To address the latter issue, the prevailing solution is to rely on the wisdom of the crowds to promote good content (e.g., via votes or like buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this pa- per, we investigate the effects, on the users, of automatically filtering question-answering content, using a combination of syntactic, semantic, and social signals. Using this filtering, a large-scale experiment with real users was performed to mea- sure the resulting engagement and satisfaction. To our knowl- edge, this experiment represents the first reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality filtering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments.
  </p>
 </div>
</div>

<div>
 <p>
  Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the experi- ence for the majority of users. To address the latter issue, the prevailing solution is to rely on the wisdom of the crowds to promote good content (e.g., via votes or like buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this pa- per, we investigate the effects, on the users, of automatically filtering question-answering content, using a combination of syntactic, semantic, and social signals. Using this filtering, a large-scale experiment with real users was performed to mea- sure the resulting engagement and satisfaction. To our knowl- edge, this experiment represents the first reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality filtering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments.
 </p>
</div>

<p>
 Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the experi- ence for the majority of users. To address the latter issue, the prevailing solution is to rely on the wisdom of the crowds to promote good content (e.g., via votes or like buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this pa- per, we investigate the effects, on the users, of automatically filtering question-answering content, using a combination of syntactic, semantic, and social signals. Using this filtering, a large-scale experiment with real users was performed to mea- sure the resulting engagement and satisfaction. To our knowl- edge, this experiment represents the first reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality filtering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/aqs-cscw.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/aqs-cscw.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/aqs-cscw.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
When the Crowd is Not Enough: Improving User Experience with Social Media through Automatic Quality Analysis
The 19th ACM conference on Computer-Supported Cooperative Work and Social Computing (CSCW 2016)
[u'Dan Pelleg', u'Oleg Rokhlenko', u'Idan Szpektor', u'Eugene Agichten', u'Ido Guy']
Human-Computer Interaction
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8513/dect-distributed-evolving-context-tree-understanding-user-behavior-pattern
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Internet user behavior models characterize user browsing dynamics or the transitions among web pages. The models help Internet companies improve their services by accurately targeting customers and providing them the information they want. For instance, specific web pages can be customized and prefetched for individuals based on sequences of web pages they have visited. Existing user behavior models abstracted as time-homogeneous Markov models cannot efficiently model user behavior variation through time. This demo presents DECT, a scalable time-inhomogeneous variable-order Markov model. DECT digests terabytes of user session data and yields user behavior patterns through time. We realize DECT using Apache Spark and deploy it on top of Yahoo! infrastructure. We demonstrate the benefits of DECT with anomaly detection and ad click rate prediction applications. DECT enables the detection of higher-order path anomalies that are masked out by existing models. DECT also provides deep insights into ad click rates with respect to user visiting paths.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Internet user behavior models characterize user browsing dynamics or the transitions among web pages. The models help Internet companies improve their services by accurately targeting customers and providing them the information they want. For instance, specific web pages can be customized and prefetched for individuals based on sequences of web pages they have visited. Existing user behavior models abstracted as time-homogeneous Markov models cannot efficiently model user behavior variation through time. This demo presents DECT, a scalable time-inhomogeneous variable-order Markov model. DECT digests terabytes of user session data and yields user behavior patterns through time. We realize DECT using Apache Spark and deploy it on top of Yahoo! infrastructure. We demonstrate the benefits of DECT with anomaly detection and ad click rate prediction applications. DECT enables the detection of higher-order path anomalies that are masked out by existing models. DECT also provides deep insights into ad click rates with respect to user visiting paths.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Internet user behavior models characterize user browsing dynamics or the transitions among web pages. The models help Internet companies improve their services by accurately targeting customers and providing them the information they want. For instance, specific web pages can be customized and prefetched for individuals based on sequences of web pages they have visited. Existing user behavior models abstracted as time-homogeneous Markov models cannot efficiently model user behavior variation through time. This demo presents DECT, a scalable time-inhomogeneous variable-order Markov model. DECT digests terabytes of user session data and yields user behavior patterns through time. We realize DECT using Apache Spark and deploy it on top of Yahoo! infrastructure. We demonstrate the benefits of DECT with anomaly detection and ad click rate prediction applications. DECT enables the detection of higher-order path anomalies that are masked out by existing models. DECT also provides deep insights into ad click rates with respect to user visiting paths.
  </p>
 </div>
</div>

<div>
 <p>
  Internet user behavior models characterize user browsing dynamics or the transitions among web pages. The models help Internet companies improve their services by accurately targeting customers and providing them the information they want. For instance, specific web pages can be customized and prefetched for individuals based on sequences of web pages they have visited. Existing user behavior models abstracted as time-homogeneous Markov models cannot efficiently model user behavior variation through time. This demo presents DECT, a scalable time-inhomogeneous variable-order Markov model. DECT digests terabytes of user session data and yields user behavior patterns through time. We realize DECT using Apache Spark and deploy it on top of Yahoo! infrastructure. We demonstrate the benefits of DECT with anomaly detection and ad click rate prediction applications. DECT enables the detection of higher-order path anomalies that are masked out by existing models. DECT also provides deep insights into ad click rates with respect to user visiting paths.
 </p>
</div>

<p>
 Internet user behavior models characterize user browsing dynamics or the transitions among web pages. The models help Internet companies improve their services by accurately targeting customers and providing them the information they want. For instance, specific web pages can be customized and prefetched for individuals based on sequences of web pages they have visited. Existing user behavior models abstracted as time-homogeneous Markov models cannot efficiently model user behavior variation through time. This demo presents DECT, a scalable time-inhomogeneous variable-order Markov model. DECT digests terabytes of user session data and yields user behavior patterns through time. We realize DECT using Apache Spark and deploy it on top of Yahoo! infrastructure. We demonstrate the benefits of DECT with anomaly detection and ad click rate prediction applications. DECT enables the detection of higher-order path anomalies that are masked out by existing models. DECT also provides deep insights into ad click rates with respect to user visiting paths.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/main-24.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/main-24.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/main-24.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
DECT: Distributed Evolving Context Tree for Understanding User Behavior Pattern Evolution
ASSOCIATION FOR THE ADVANCEMENT OF ARTIFICIAL INTELLIGENCE (AAAI 2016)
[u'Nikolay Laptev']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8593/web-scale-image-clustering-revisited
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.
 </p>
</p>

<p>
 Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/iqm (1).pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/iqm (1).pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/iqm (1).pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Web-Scale Image Clustering Revisited
International Converence on Computer Vision (ICCV 2015)
[u'Yannis Avrithis', u'Yannis Kalantidis', u'Evangelos Anagnostopoulos', u'Ioannis Z. Emiris']
Computer Vision
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8552/non-convex-statistical-optimization-sparse-tensor-graphical-model
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.
 </p>
</p>

<p>
 We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/Tlasso_nips2015_WeiSun.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/Tlasso_nips2015_WeiSun.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/Tlasso_nips2015_WeiSun.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Non-convex Statistical Optimization for Sparse Tensor Graphical Model
Annual Conference on Neural Information Processing Systems (NIPS 2015)
[u'Wei Sun', u'Zhaoran Wang', u'Han Liu', u'Guang Cheng']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8599/when-structural-model-meets-big-data-examining-multi-device-attribution-native-ads
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  In this study, we combine behavior-driven structural model with machine learning tools and apply them to data on a large scale. We investigate the important question of channel interdependence in the context of multi-device advertising. In particular, we explore how the distribution of ads on multiple digital devices (i.e. tablet, smartphone and PC) works together to affect the final conversion. We model the marginal impact of an advertisement impression on an individual consumers behavior based on the conversion funnel theory. To handle the sheer volume of the impression level data and the iterative nature of the estimation procedure, we develop a novel estimation algorithm that can distribute the data and computational burden in parallel on cloud computing infrastructure. Our preliminary results show channel diversity stimulates disengaged consumers to transition to engaged stages. But it does not encourage the already engaged consumers to stay engaged. In addition, the channel diversity is more effective for the early impressions than for the late impressions.
 </p>
</p>

<p>
 In this study, we combine behavior-driven structural model with machine learning tools and apply them to data on a large scale. We investigate the important question of channel interdependence in the context of multi-device advertising. In particular, we explore how the distribution of ads on multiple digital devices (i.e. tablet, smartphone and PC) works together to affect the final conversion. We model the marginal impact of an advertisement impression on an individual consumers behavior based on the conversion funnel theory. To handle the sheer volume of the impression level data and the iterative nature of the estimation procedure, we develop a novel estimation algorithm that can distribute the data and computational burden in parallel on cloud computing infrastructure. Our preliminary results show channel diversity stimulates disengaged consumers to transition to engaged stages. But it does not encourage the already engaged consumers to stay engaged. In addition, the channel diversity is more effective for the early impressions than for the late impressions.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/LargeScaleStructualAnalysis.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/LargeScaleStructualAnalysis.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/LargeScaleStructualAnalysis.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
When Structural Model Meets Big Data: Examining Multi-Device Attribution for Native Ads Using TB-Sized Data
Workshop on Information Systems and Economics 2015 (WISE 2015)
[u'Quan Wang', u'Beibei Li', u'Pengyuan Wang', u'Jimmy Yang']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8558/copeland-dueling-bandits
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.
  </p>
 </div>
</div>

<div>
 <p>
  A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.
 </p>
</div>

<p>
 A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T ) without requiring such assumptions. Our results offer the best of both worlds: O(K log T ) bounds without restrictive assumptions.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/copeland.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/copeland.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/copeland.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Copeland Dueling Bandits
Neural Information Processing Systems (NIPS 2015)
[u'Masrour Zoghi', u'Zohar Karnin', u'Shimon Whiteson', u'Maarten De Rijke']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8567/combinatorial-cascading-bandits
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.
 </p>
</p>

<p>
 We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.
</p>
<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Combinatorial Cascading Bandits
Annual Conference on Neural Information Processing Systems (NIPS 2015)
[u'Branislav Kveton', u'Zheng Wen', u'Azin Ashkan', u'Csaba Szepesvri']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
IN PAGE
*********************
Overview of the TREC 2015 LiveQA Track
TREC 2015
[u'Branislav Kveton', u'Zheng Wen', u'Azin Ashkan', u'Csaba Szepesvri', u'Eugene Agichten', u'David Carmel', u'Donna Harman', u'Dan Pelleg', u'Yuval Pinter']
Undefined
Not found                              
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8494/large-scale-unusual-time-series-detection
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series. For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
    </p>
    <p>
     We compute a vector of features on each time series, measuring characteristics of the series. The features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and -hulls.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series. For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
   </p>
   <p>
    We compute a vector of features on each time series, measuring characteristics of the series. The features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and -hulls.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series. For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
  </p>
  <p>
   We compute a vector of features on each time series, measuring characteristics of the series. The features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and -hulls.
  </p>
 </div>
</div>

<div>
 <p>
  It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series. For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
 </p>
 <p>
  We compute a vector of features on each time series, measuring characteristics of the series. The features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and -hulls.
 </p>
</div>

<p>
 It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series. For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
</p>

<p>
 We compute a vector of features on each time series, measuring characteristics of the series. The features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and -hulls.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Large-Scale Unusual Time Series Detection
International Conference on Data Mining series (ICDM 2015)
[u'Nikolay Laptev', u'Rob Hyndman', u'Earo Wang']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8524/visual-affect-around-world-large-scale-multilingual-visual-sentiment-ontology
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of >15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, >7.36M images and their metadata is also released.
 </p>
</p>

<p>
 Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of >15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, >7.36M images and their metadata is also released.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/bjou2015.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/bjou2015.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/bjou2015.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology
ACM International Conference on Multimedia (ACM MM)
[u'Breandan Jou', u'Tao Chen', u'Nikolaos Pappas', u'Miriam Redi', u'Mercan Topkara', u'Shih-fu Chang']
Computer Vision
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=1
found
 LINK 
https://labs.yahoo.com/publications/8473/learning-entity-types-query-logs-graph-based-modeling
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Entities (e.g., person, movie or place) play an important role in real-world applications so learning entity types has attracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news articles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with specific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word patterns and clickthroughs in search logs are not found in text corpora, thus providing a complemental source for discovering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs. However, it is a non-trivial task because: (1) Queries are short texts and information related to entities is usually very sparse; (2) Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. In order to address the issues, we first model query logs into a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then a graph-based framework, ELP, is proposed to simultaneously learn types of both entities and auxiliary signals. In ELP, two separate strategies LPA and LPD are designed to fix the problems of sparsity and noise in query logs. Extensive empirical studies are conducted on real Yahoo! search logs to evaluate the effectiveness of the proposed ELP framework.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Entities (e.g., person, movie or place) play an important role in real-world applications so learning entity types has attracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news articles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with specific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word patterns and clickthroughs in search logs are not found in text corpora, thus providing a complemental source for discovering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs. However, it is a non-trivial task because: (1) Queries are short texts and information related to entities is usually very sparse; (2) Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. In order to address the issues, we first model query logs into a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then a graph-based framework, ELP, is proposed to simultaneously learn types of both entities and auxiliary signals. In ELP, two separate strategies LPA and LPD are designed to fix the problems of sparsity and noise in query logs. Extensive empirical studies are conducted on real Yahoo! search logs to evaluate the effectiveness of the proposed ELP framework.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Entities (e.g., person, movie or place) play an important role in real-world applications so learning entity types has attracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news articles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with specific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word patterns and clickthroughs in search logs are not found in text corpora, thus providing a complemental source for discovering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs. However, it is a non-trivial task because: (1) Queries are short texts and information related to entities is usually very sparse; (2) Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. In order to address the issues, we first model query logs into a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then a graph-based framework, ELP, is proposed to simultaneously learn types of both entities and auxiliary signals. In ELP, two separate strategies LPA and LPD are designed to fix the problems of sparsity and noise in query logs. Extensive empirical studies are conducted on real Yahoo! search logs to evaluate the effectiveness of the proposed ELP framework.
  </p>
 </div>
</div>

<div>
 <p>
  Entities (e.g., person, movie or place) play an important role in real-world applications so learning entity types has attracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news articles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with specific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word patterns and clickthroughs in search logs are not found in text corpora, thus providing a complemental source for discovering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs. However, it is a non-trivial task because: (1) Queries are short texts and information related to entities is usually very sparse; (2) Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. In order to address the issues, we first model query logs into a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then a graph-based framework, ELP, is proposed to simultaneously learn types of both entities and auxiliary signals. In ELP, two separate strategies LPA and LPD are designed to fix the problems of sparsity and noise in query logs. Extensive empirical studies are conducted on real Yahoo! search logs to evaluate the effectiveness of the proposed ELP framework.
 </p>
</div>

<p>
 Entities (e.g., person, movie or place) play an important role in real-world applications so learning entity types has attracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news articles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with specific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word patterns and clickthroughs in search logs are not found in text corpora, thus providing a complemental source for discovering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs. However, it is a non-trivial task because: (1) Queries are short texts and information related to entities is usually very sparse; (2) Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. In order to address the issues, we first model query logs into a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then a graph-based framework, ELP, is proposed to simultaneously learn types of both entities and auxiliary signals. In ELP, two separate strategies LPA and LPD are designed to fix the problems of sparsity and noise in query logs. Extensive empirical studies are conducted on real Yahoo! search logs to evaluate the effectiveness of the proposed ELP framework.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Entity Types from Query Logs Via Graph-Based Modeling
The 24th ACM International Conference on Information and Knowledge Management (CIKM 2015)
[u'Jingyuan Zhang', u'Roger Jie Luo', u'Altaf Rahman', u'Yi Chang', u'Philip S Yu']
Information and Knowledge Management
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8481/fast-and-secure-three-party-computation-garbled-circuit-approach
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Many deployments of secure multi-party computation (MPC) in practice have used information-theoretic three-party protocols that tolerate a {\em single, semi-honest} corrupt party, since these protocols enjoy very high efficiency.
 </p>
 <p>
  We propose a new approach for secure three-party computation (3PC) that improves security while maintaining practical efficiency that is competitive with traditional information-theoretic protocols. Our protocol is based on garbled circuits and provides security against a single, {\em malicious} corrupt party. Unlike information-theoretic 3PC protocols, ours uses a constant number of rounds. Our protocol only uses inexpensive symmetric-key cryptography: hash functions, block ciphers, pseudorandom generators (in particular, no oblivious transfers) and has performance that is comparable to that of Yao's (semi-honest) 2PC protocol.

    
 We demonstrate the practicality of our protocol with an implementation based on the JustGarble framework of Bellare et al.\ (S\&P 2013). The implementation incorporates various optimizations including the most recent techniques for efficient circuit garbling. We perform experiments on several benchmarking circuits, in different setups. Our experiments confirm that, despite providing a more demanding security guarantee, our protocol has performance comparable to existing information-theoretic 3PC.
 </p>
</p>

<p>
 Many deployments of secure multi-party computation (MPC) in practice have used information-theoretic three-party protocols that tolerate a {\em single, semi-honest} corrupt party, since these protocols enjoy very high efficiency.
</p>

<p>
 We propose a new approach for secure three-party computation (3PC) that improves security while maintaining practical efficiency that is competitive with traditional information-theoretic protocols. Our protocol is based on garbled circuits and provides security against a single, {\em malicious} corrupt party. Unlike information-theoretic 3PC protocols, ours uses a constant number of rounds. Our protocol only uses inexpensive symmetric-key cryptography: hash functions, block ciphers, pseudorandom generators (in particular, no oblivious transfers) and has performance that is comparable to that of Yao's (semi-honest) 2PC protocol.

    
 We demonstrate the practicality of our protocol with an implementation based on the JustGarble framework of Bellare et al.\ (S\&P 2013). The implementation incorporates various optimizations including the most recent techniques for efficient circuit garbling. We perform experiments on several benchmarking circuits, in different setups. Our experiments confirm that, despite providing a more demanding security guarantee, our protocol has performance comparable to existing information-theoretic 3PC.
</p>
<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Fast and Secure Three-party Computation: The Garbled Circuit Approach
The ACM Conference on Computer and Communications Security (CCS) (ACM CCS 2015)
[u'Payman Mohassel', u'Mike Rosulek']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8476/towards-automatic-lock-removal-scalable-synchronization-full-version
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We present a code transformation for concurrent data structures, which increases their scalability without sacricing correctness. Our transformation takes lock-based code and replaces some of the locking steps therein with optimistic synchronization in order to reduce contention. The main idea is to have each operation perform an optimistic traversal of the data structure as long as no shared memory locations are updated, and then proceed with pessimistic code. The transformed code inherits essential properties of the original one, including linearizability, serializability, and deadlock freedom.
 </p>
 <p>
  Our work complements existing pessimistic transformations that make sequential code thread-safe by adding locks. In essence, we provide a way to optimize such transformations by reducing synchronization bottlenecks (for example, locking the root of a tree). The resulting code scales well and signicantly outperforms pessimistic approaches. We further compare our synthesized code to state-of-the-art data structures implemented by experts. We nd that its performance is comparable to that achieved by the custom-tailored implementations. Our work thus shows the promise that automated approaches bear for overcoming the diculty involved in manually hand-crafting concurrent data structures.
 </p>
</p>

<p>
 We present a code transformation for concurrent data structures, which increases their scalability without sacricing correctness. Our transformation takes lock-based code and replaces some of the locking steps therein with optimistic synchronization in order to reduce contention. The main idea is to have each operation perform an optimistic traversal of the data structure as long as no shared memory locations are updated, and then proceed with pessimistic code. The transformed code inherits essential properties of the original one, including linearizability, serializability, and deadlock freedom.
</p>

<p>
 Our work complements existing pessimistic transformations that make sequential code thread-safe by adding locks. In essence, we provide a way to optimize such transformations by reducing synchronization bottlenecks (for example, locking the root of a tree). The resulting code scales well and signicantly outperforms pessimistic approaches. We further compare our synthesized code to state-of-the-art data structures implemented by experts. We nd that its performance is comparable to that achieved by the custom-tailored implementations. Our work thus shows the promise that automated approaches bear for overcoming the diculty involved in manually hand-crafting concurrent data structures.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/document.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/document.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/document.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Towards Automatic Lock Removal for Scalable Synchronization (Full Version)
International Symposium on Distributed Computing
[u'Guy Gueta', u'Eshcar Hillel', u'Idit Keidar', u'Maya Arbel']
Systems
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8497/transfer-learning-through-greedy-subset-selection
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We study the binary transfer learning problem, focusing on how to select sources from a large pool and how to combine them to yield a good performance on a target task.
In particular, we consider the transfer learning setting where one does not have direct access to the source data, but rather employs the source hypotheses trained from them.
Building on the literature on the best subset selection problem, we propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously.
On three computer vision datasets we achieve state-of-the-art results, substantially outperforming transfer learning and popular feature selection baselines in a small-sample setting.
Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples.
 </p>
 <p>
 </p>
</p>

<p>
 We study the binary transfer learning problem, focusing on how to select sources from a large pool and how to combine them to yield a good performance on a target task.
In particular, we consider the transfer learning setting where one does not have direct access to the source data, but rather employs the source hypotheses trained from them.
Building on the literature on the best subset selection problem, we propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously.
On three computer vision datasets we achieve state-of-the-art results, substantially outperforming transfer learning and popular feature selection baselines in a small-sample setting.
Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples.
</p>

<p>
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/KuzOraCa15.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/KuzOraCa15.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/KuzOraCa15.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Transfer Learning through Greedy Subset Selection
International Conference on Image Analysis and Processing (ICIAP 2015)
[u'Francesco Orabona']
Computer Vision
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5688/graphical-passwords-wild-%E2%80%93-understanding-how-users-choose-pictures-and-passwords
found
<h6>
 Abstract
</h6>

<p class="leading">
 Common user authentication methods on smartphones, such as lock patterns, PINs, or passwords, impose a trade-off between security and password memorability. Image-based passwords were proposed as a secure and usable alternative. As of today, however, it remains unclear how such schemes are used in the wild. We present the first study to investigate how image-based passwords are used over long periods of time in the real world. Our analyses are based on data from 2318 unique devices collected over more than one year using a custom application released in the Android Play store. We present an in-depth analysis of what kind of images users select, how they define their passwords, and how secure these passwords are. Our findings provide valuable insights into real-world use of image-based passwords and inform the design of future graphical authentication schemes.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/alt2015mobilehci-1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/alt2015mobilehci-1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/alt2015mobilehci-1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Graphical Passwords in the Wild  Understanding How Users Choose Pictures and Passwords in Image-based Authentication Schemes
MobileHCI 2015
[u'Ali Sahami', u'Florian Alt', u'Stefan Schneegass', u'Mariam Hassib', u'Andreas Bulling']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5686/efficient-zero-knowledge-proofs-non-algebraic-statements-sublinear-amortized-cost
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We describe a zero-knowledge proof system in which a prover holds a large dataset M and can repeatedly prove NP relations about that dataset. That is, for any (public) relation $R$ and $x$, the prover can prove that  there exists $w: R(M,x,w)=1$. After an initial setup phase (which depends only on $M$), each proof requires only a constant number of rounds and has communication/computation cost proportional to that of a {\em random-access machine (RAM)} implementation of $R$ up to polylogarithmic factors. In particular, the cost per proof in many applications is sublinear in $M|$ Additionally, the storage requirement between proofs for the verifier is constant.
 </p>
</p>

<p>
 We describe a zero-knowledge proof system in which a prover holds a large dataset M and can repeatedly prove NP relations about that dataset. That is, for any (public) relation $R$ and $x$, the prover can prove that  there exists $w: R(M,x,w)=1$. After an initial setup phase (which depends only on $M$), each proof requires only a constant number of rounds and has communication/computation cost proportional to that of a {\em random-access machine (RAM)} implementation of $R$ up to polylogarithmic factors. In particular, the cost per proof in many applications is sublinear in $M|$ Additionally, the storage requirement between proofs for the verifier is constant.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main9.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main9.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main9.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Efficient Zero-Knowledge Proofs of Non-Algebraic Statements with Sublinear Amortized Cost
Advances in Cryptology--CRYPTO 2015
[u'Payman Mohassel', u'Mike Rosulek', u'Zhangxiang Hu']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8480/e-commerce-your-inbox-product-recommendations-scale
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
  </p>
 </div>
</div>

<div>
 <p>
  In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
 </p>
</div>

<p>
 In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic_mail_kdd.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic_mail_kdd.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/grbovic_mail_kdd.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
E-commerce in Your Inbox: Product Recommendations at Scale
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2015)
[u'Mihajlo Grbovic', u'Vladan Radosavljevic', u'Nemanja Djuric', u'Narayan Bhamidipati']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8475/set-cover-web-scale
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  The classic \textsc{Set Cover} problem requires selecting a minimum size subset $\mathcal{A} \subseteq \mathcal{F}$ from a family of finite subsets $\mathcal{F}$ of $\mathcal{U}$ such that the elements covered by $\mathcal{A}$ are the ones covered by $\mathcal{F}$. It naturally occurs in many settings in web search, web mining and web advertising. The greedy algorithm that iteratively selects a set in $\mathcal{F}$ that covers the most uncovered elements, yields an optimum $(1+\ln |\mathcal{U}|)$-approximation but is inherently sequential. In this work we give the first MapReduce \textsc{Set Cover} algorithm that scales to problem sizes of $\sim1$ trillion elements and runs in $\log_p\Delta$ iterations for a nearly optimum approximation ratio of $p\ln\Delta$, where $\Delta$ is the cardinality of the largest set in $\mathcal{F}$.
 </p>
 <p>
  A web crawler is a system for bulk downloading of web pages. Given a set of seed URLs, the crawler downloads and extracts the hyperlinks embedded in them and schedules the crawling of the pages addressed by those hyperlinks for a subsequent iteration. While the average page out-degree is $\sim50$, the crawled corpus  grows at a much smaller rate, implying a significant outlink overlap. Using our MapReduce \textsc{Set Cover} heuristic as a building block, we present the first large-scale seed generation algorithm that scales to $\sim20$ billion nodes and discovers new pages at a rate $\sim4x$ faster than that obtained by prior art heuristics.
 </p>
</p>

<p>
 The classic \textsc{Set Cover} problem requires selecting a minimum size subset $\mathcal{A} \subseteq \mathcal{F}$ from a family of finite subsets $\mathcal{F}$ of $\mathcal{U}$ such that the elements covered by $\mathcal{A}$ are the ones covered by $\mathcal{F}$. It naturally occurs in many settings in web search, web mining and web advertising. The greedy algorithm that iteratively selects a set in $\mathcal{F}$ that covers the most uncovered elements, yields an optimum $(1+\ln |\mathcal{U}|)$-approximation but is inherently sequential. In this work we give the first MapReduce \textsc{Set Cover} algorithm that scales to problem sizes of $\sim1$ trillion elements and runs in $\log_p\Delta$ iterations for a nearly optimum approximation ratio of $p\ln\Delta$, where $\Delta$ is the cardinality of the largest set in $\mathcal{F}$.
</p>

<p>
 A web crawler is a system for bulk downloading of web pages. Given a set of seed URLs, the crawler downloads and extracts the hyperlinks embedded in them and schedules the crawling of the pages addressed by those hyperlinks for a subsequent iteration. While the average page out-degree is $\sim50$, the crawled corpus  grows at a much smaller rate, implying a significant outlink overlap. Using our MapReduce \textsc{Set Cover} heuristic as a building block, we present the first large-scale seed generation algorithm that scales to $\sim20$ billion nodes and discovers new pages at a rate $\sim4x$ faster than that obtained by prior art heuristics.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="http://labs.yahoo.com/mobstor/publication_attachments/kdd15_0.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="http://labs.yahoo.com/mobstor/publication_attachments/kdd15_0.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="http://labs.yahoo.com/mobstor/publication_attachments/kdd15_0.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Set Cover at Web Scale
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2015)
[u'Stergios Stergiou', u'Kostas Tsioutsiouliklis']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6773/generic-and-scalable-framework-automated-time-series-anomaly-detection
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of persons data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of persons data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
  </div>
 </div>
</div>
<div>
 <div>
  This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of persons data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
 </div>
</div>

<div>
 This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of persons data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd2015.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd2015.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd2015.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Generic and Scalable Framework for Automated Time-series Anomaly Detection
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2015)
[u'Nikolay Laptev', u'Saeed Amizadeh', u'Ian Flint']
Web Search and Data Mining
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8486/gender-and-interest-targeting-sponsored-post-advertising-tumblr
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and can promote that content by sponsoring it to appear as an advertisement in the users' live feeds. In this paper, we present a framework that enabled two of the key targeted advertising components for Tumblr, gender and interest targeting. We describe the main challenges encountered during the development of the framework, which include the creation of a ground truth for training gender prediction models, as well as mapping Tumblr content to a predefined interest taxonomy. For purposes of inferring user interests, we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6.8 billion user posts, with a very limited amount of categorized keywords, and was shown to have superior performance over the baseline approaches. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that covers more than 90% of daily activities on Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% increase in user engagement with sponsored posts in comparison to untargeted campaigns.
 </p>
</p>

<p>
 As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and can promote that content by sponsoring it to appear as an advertisement in the users' live feeds. In this paper, we present a framework that enabled two of the key targeted advertising components for Tumblr, gender and interest targeting. We describe the main challenges encountered during the development of the framework, which include the creation of a ground truth for training gender prediction models, as well as mapping Tumblr content to a predefined interest taxonomy. For purposes of inferring user interests, we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6.8 billion user posts, with a very limited amount of categorized keywords, and was shown to have superior performance over the baseline approaches. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that covers more than 90% of daily activities on Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% increase in user engagement with sponsored posts in comparison to untargeted campaigns.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015kddA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015kddA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015kddA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Gender and Interest Targeting for Sponsored Post Advertising at Tumblr
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2015)
[u'Mihajlo Grbovic', u'Vladan Radosavljevic', u'Nemanja Djuric', u'Narayan Bhamidipati']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=2
found
 LINK 
https://labs.yahoo.com/publications/8488/dynamic-matrix-factorization-priors-unknown-values
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (not missing at random). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (not missing at random). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (not missing at random). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.
  </p>
 </div>
</div>

<div>
 <p>
  Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (not missing at random). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.
 </p>
</div>

<p>
 Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (not missing at random). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/kdd2015-dynamicMFwithPriors.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/kdd2015-dynamicMFwithPriors.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/kdd2015-dynamicMFwithPriors.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
 Dynamic Matrix Factorization with Priors on Unknown Values
21st ACM SIGKDD Conference 2015 (KDD 2015)
[u'Robin Devooght']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8470/adaqac-adaptive-query-auto-completion-implicit-negative-feedback
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
    </p>
    <p>
     Users preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
    </p>
    <p>
     We take the initiative towards studying implicit negative feedback during user-QAC interactions. This motivates re-designing QAC in the more general (static) relevance(adaptive) implicit negative feedback framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users implicit negative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
   </p>
   <p>
    Users preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
   </p>
   <p>
    We take the initiative towards studying implicit negative feedback during user-QAC interactions. This motivates re-designing QAC in the more general (static) relevance(adaptive) implicit negative feedback framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users implicit negative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
  </p>
  <p>
   Users preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
  </p>
  <p>
   We take the initiative towards studying implicit negative feedback during user-QAC interactions. This motivates re-designing QAC in the more general (static) relevance(adaptive) implicit negative feedback framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users implicit negative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
  </p>
 </div>
</div>

<div>
 <p>
  Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
 </p>
 <p>
  Users preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
 </p>
 <p>
  We take the initiative towards studying implicit negative feedback during user-QAC interactions. This motivates re-designing QAC in the more general (static) relevance(adaptive) implicit negative feedback framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users implicit negative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
 </p>
</div>

<p>
 Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
</p>

<p>
 Users preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
</p>

<p>
 We take the initiative towards studying implicit negative feedback during user-QAC interactions. This motivates re-designing QAC in the more general (static) relevance(adaptive) implicit negative feedback framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users implicit negative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/SIGIR15.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/SIGIR15.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/SIGIR15.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
adaQAC: Adaptive Query Auto-Completion via Implicit Negative Feedback 
38th Annual ACM Special Interest Group on Information Retrieval (SIGIR) Conference (SIGIR 2015)
[u'Amit Goyal', u'Hongbo Deng', u'Yi Chang']
Personalization and Contextual Search
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8479/context-and-content-aware-embeddings-query-rewriting-sponsored-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    <p>
     Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through the issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within an online session. As a result, queries with similar content and context are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search in the projected space. The method was trained on more than 12 billion sessions, one of the largest corpuses reported thus far, and evaluated on both public TREC data set and in-house sponsored search data set. The results show the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and the monetization potential.
    </p>
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   <p>
    Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through the issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within an online session. As a result, queries with similar content and context are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search in the projected space. The method was trained on more than 12 billion sessions, one of the largest corpuses reported thus far, and evaluated on both public TREC data set and in-house sponsored search data set. The results show the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and the monetization potential.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through the issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within an online session. As a result, queries with similar content and context are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search in the projected space. The method was trained on more than 12 billion sessions, one of the largest corpuses reported thus far, and evaluated on both public TREC data set and in-house sponsored search data set. The results show the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and the monetization potential.
  </p>
 </div>
</div>

<div>
 <p>
  Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through the issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within an online session. As a result, queries with similar content and context are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search in the projected space. The method was trained on more than 12 billion sessions, one of the largest corpuses reported thus far, and evaluated on both public TREC data set and in-house sponsored search data set. The results show the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and the monetization potential.
 </p>
</div>

<p>
 Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through the issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within an online session. As a result, queries with similar content and context are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search in the projected space. The method was trained on more than 12 billion sessions, one of the largest corpuses reported thus far, and evaluated on both public TREC data set and in-house sponsored search data set. The results show the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and the monetization potential.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015sigirconf.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015sigirconf.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/grbovic2015sigirconf.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search
38th Annual ACM Special Interest Group on Information Retrieval (SIGIR) Conference (SIGIR 2015)
[u'Mihajlo Grbovic', u'Nemanja Djuric', u'Vladan Radosavljevic', u'Fabrizio Silvestri', u'Narayan Bhamidipati']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5689/analyzing-users-sequential-behavior-query-auto-completion-markov-processes
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Query auto-completion (QAC) plays an important role in assisting users typing less while submitting a query. The QAC engine generally offers a list of suggested queries that start with a users input as a prefix, and the list of suggestions is changed to match the updated input after the user types each keystroke. Therefore rich user interactions can be observed along with each keystroke until a user clicks a suggestion or types the entire query manually. It becomes increasingly important to analyze and understand users interactions with the QAC engine, to improve its performance. Existing works on QAC either ignored users interaction data, or assumed that their interaction at each keystroke is independent from others. Our paper pays high attention to users sequential interactions with a QAC engine in and across QAC sessions, rather than users interactions at each keystroke of each QAC session separately. Analyzing the dependencies in users sequential interactions improves our understanding of the following three questions: 1) how is a users skipping/viewing move at the current keystroke influenced by that at the previous keystroke? 2) how to improve search engines query suggestions at short keystrokes based on those at latter long keystrokes? and 3) facing a targeted query shown in the suggestion list, why does a user decide to continue typing rather than click the intended suggestion? We propose a probabilistic model that addresses those three questions in a unified way, and illustrate how the model determines users final click decisions. A variational inference algorithm is designed for parameter estimation of the proposed model. We evaluate our method based on real-world QAC logs. By comparing with state-of-the-art methods, our proposed model does suggest queries that better satisfy users intents.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Query auto-completion (QAC) plays an important role in assisting users typing less while submitting a query. The QAC engine generally offers a list of suggested queries that start with a users input as a prefix, and the list of suggestions is changed to match the updated input after the user types each keystroke. Therefore rich user interactions can be observed along with each keystroke until a user clicks a suggestion or types the entire query manually. It becomes increasingly important to analyze and understand users interactions with the QAC engine, to improve its performance. Existing works on QAC either ignored users interaction data, or assumed that their interaction at each keystroke is independent from others. Our paper pays high attention to users sequential interactions with a QAC engine in and across QAC sessions, rather than users interactions at each keystroke of each QAC session separately. Analyzing the dependencies in users sequential interactions improves our understanding of the following three questions: 1) how is a users skipping/viewing move at the current keystroke influenced by that at the previous keystroke? 2) how to improve search engines query suggestions at short keystrokes based on those at latter long keystrokes? and 3) facing a targeted query shown in the suggestion list, why does a user decide to continue typing rather than click the intended suggestion? We propose a probabilistic model that addresses those three questions in a unified way, and illustrate how the model determines users final click decisions. A variational inference algorithm is designed for parameter estimation of the proposed model. We evaluate our method based on real-world QAC logs. By comparing with state-of-the-art methods, our proposed model does suggest queries that better satisfy users intents.
  </div>
 </div>
</div>
<div>
 <div>
  Query auto-completion (QAC) plays an important role in assisting users typing less while submitting a query. The QAC engine generally offers a list of suggested queries that start with a users input as a prefix, and the list of suggestions is changed to match the updated input after the user types each keystroke. Therefore rich user interactions can be observed along with each keystroke until a user clicks a suggestion or types the entire query manually. It becomes increasingly important to analyze and understand users interactions with the QAC engine, to improve its performance. Existing works on QAC either ignored users interaction data, or assumed that their interaction at each keystroke is independent from others. Our paper pays high attention to users sequential interactions with a QAC engine in and across QAC sessions, rather than users interactions at each keystroke of each QAC session separately. Analyzing the dependencies in users sequential interactions improves our understanding of the following three questions: 1) how is a users skipping/viewing move at the current keystroke influenced by that at the previous keystroke? 2) how to improve search engines query suggestions at short keystrokes based on those at latter long keystrokes? and 3) facing a targeted query shown in the suggestion list, why does a user decide to continue typing rather than click the intended suggestion? We propose a probabilistic model that addresses those three questions in a unified way, and illustrate how the model determines users final click decisions. A variational inference algorithm is designed for parameter estimation of the proposed model. We evaluate our method based on real-world QAC logs. By comparing with state-of-the-art methods, our proposed model does suggest queries that better satisfy users intents.
 </div>
</div>

<div>
 Query auto-completion (QAC) plays an important role in assisting users typing less while submitting a query. The QAC engine generally offers a list of suggested queries that start with a users input as a prefix, and the list of suggestions is changed to match the updated input after the user types each keystroke. Therefore rich user interactions can be observed along with each keystroke until a user clicks a suggestion or types the entire query manually. It becomes increasingly important to analyze and understand users interactions with the QAC engine, to improve its performance. Existing works on QAC either ignored users interaction data, or assumed that their interaction at each keystroke is independent from others. Our paper pays high attention to users sequential interactions with a QAC engine in and across QAC sessions, rather than users interactions at each keystroke of each QAC session separately. Analyzing the dependencies in users sequential interactions improves our understanding of the following three questions: 1) how is a users skipping/viewing move at the current keystroke influenced by that at the previous keystroke? 2) how to improve search engines query suggestions at short keystrokes based on those at latter long keystrokes? and 3) facing a targeted query shown in the suggestion list, why does a user decide to continue typing rather than click the intended suggestion? We propose a probabilistic model that addresses those three questions in a unified way, and illustrate how the model determines users final click decisions. A variational inference algorithm is designed for parameter estimation of the proposed model. We evaluate our method based on real-world QAC logs. By comparing with state-of-the-art methods, our proposed model does suggest queries that better satisfy users intents.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp254-li.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp254-li.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp254-li.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Analyzing User's Sequential Behavior in Query Auto-Completion via Markov Processes
SIGIR 2015
[u'Hongbo Deng', u'Yi Chang', u'Ricardo Baeza-yates', u'Liangda Li', u'Anlei Dong', u'Hongyuan Zha']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6765/how-fair-your-protocol-utility-based-approach-protocol-optimality
found
<h6>
 Abstract
</h6>

<p class="leading">
 The security of distributed cryptographic protocols typically requires the following properties: privacy (the inputs of the honest parties remain hidden), correctness (the adversary cannot affectthe outcome of the computation any more than choosing the inputs of the corrupt parties), and --the focus of this paper -- fairness (whenever the adversary gets his output from the computation,all honest parties also do).

However, and as implied by Cleve's seminal result [STOC'86], satisfying these properties simultaneouslyis impossible in the presence of dishonest majorities, leading to a generous number ofproposals for relaxed notions of fairness, by weakening in various ways the desired security guarantees.While these works also suggest completeness results (i.e., the ability to design protocolswhich achieve their fairness notion), their assessment is typically of an all-or-nothing nature. Thatis, when presented with a protocol which is not designed to be fair according to their respectivenotion, they most likely would render it unfair and make no further statement about it.

In this work we put forth a comparative approach to fairness.We present notions that whenpresented with two arbitrary protocols, provide the means to answer the question &quot;Which of theprotocols is fairer?&quot; The basic idea is that we can use an appropriate utility function to expressthe preferences of an adversary who wants to break fairness. Thus, we can compare protocols withrespect to how fair they are, placing them in a partial order according to this relative fairnessrelation.

After formulating such utility-based fairness notions, we turn to the question of finding optimalprotocols -- i.e., maximal elements in the above partial order. We investigate -- and answer -- thisquestion for secure function evaluation, both in the two-party and multi-party settings.To our knowledge, the only other fairness notion providing some sort of comparative statementis that of 1/p-security (aka &quot;partial fairness&quot;) by Gordon and Katz [Eurocrypt'10]. We also showin this paper that for a special class of utilities our notion strictly implies 1/p-security. In addition,we fix a shortcoming of the definition which is exposed by our comparison, thus strengthening thatresult.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fairness1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fairness1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fairness1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
How Fair is Your Protocol? A Utility-based Approach to Protocol Optimality
34th Annual ACM Symposium on Principles of Distributed Computing -- PODC 2015
[u'Juan A. Garay', u'Jonathan Katz', u'Bjoern Tackmann', u'Vassilis Zikas']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8515/distributed-convex-thresholding
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Over the last fifteen years, a large group of algorithms emerged which compute various predicates from distributed data with a focus on communication efficiency. These algorithms are often called &quot;communication-efficient&quot;, &quot;geometric-monitoring&quot;, or &quot;local&quot; algorithms. We jointly call them distributed convex thresholding algorithms, for reasons which will be explained in this work. Distributed convex thresholding algorithms have found their applications in domains in which bandwidth is a scarce resource, such as wireless sensor networks and peer-to-peer systems, or in scenarios in which data rapidly streams to the different processors but outcome of the predicate rarely changes. Common to all of these algorithms is the use of a data dependent criteria to determine when further messaging is required.
 </p>
 <p>
  This work presents two very simple yet exceedingly general theorems from which the correctness of all distributed convex thresholding algorithms can be elicited, and demonstrates that for key examples. Because the theorems are general, they extend the range of predicates which can be computed in a communication efficient manner beyond what is currently known. Unlike the previous correction proofs given to these algorithms, the proofs of the theorems presented here do not depend on the communication infrastructure. So the correctness of any distributed convex thresholding algorithm is immediately extended from broadcast enabled networks or from cycle free networks to general networks. Inspecting existing algorithms in light of the new theorems reveals that they contain redundant requirements, which cause them to send messages when indeed none are needed.
 </p>
</p>

<p>
 Over the last fifteen years, a large group of algorithms emerged which compute various predicates from distributed data with a focus on communication efficiency. These algorithms are often called &quot;communication-efficient&quot;, &quot;geometric-monitoring&quot;, or &quot;local&quot; algorithms. We jointly call them distributed convex thresholding algorithms, for reasons which will be explained in this work. Distributed convex thresholding algorithms have found their applications in domains in which bandwidth is a scarce resource, such as wireless sensor networks and peer-to-peer systems, or in scenarios in which data rapidly streams to the different processors but outcome of the predicate rarely changes. Common to all of these algorithms is the use of a data dependent criteria to determine when further messaging is required.
</p>

<p>
 This work presents two very simple yet exceedingly general theorems from which the correctness of all distributed convex thresholding algorithms can be elicited, and demonstrates that for key examples. Because the theorems are general, they extend the range of predicates which can be computed in a communication efficient manner beyond what is currently known. Unlike the previous correction proofs given to these algorithms, the proofs of the theorems presented here do not depend on the communication infrastructure. So the correctness of any distributed convex thresholding algorithm is immediately extended from broadcast enabled networks or from cycle free networks to general networks. Inspecting existing algorithms in light of the new theorems reveals that they contain redundant requirements, which cause them to send messages when indeed none are needed.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/main.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distributed Convex Thresholding
34th Annual ACM Symposium on Principles of Distributed Computing (PODC 2015)
[u'Ran Wolff']
Systems
Not FOUND
^^^^^^^^^^^^^^^^^^^^
IN PAGE
*********************
Display advertising impact: search lift and social influence
KDD
[u'Ran Wolff', u'Prabhakar Krishnamurthy']
Undefined
Not found                              
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6782/optimal-and-adaptive-algorithms-online-boosting
found
<h6>
 Abstract
</h6>

<p class="leading">
 We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icml2015.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icml2015.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icml2015.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Optimal and Adaptive Algorithms for Online Boosting
ICML-2015
[u'Alina Beygelzimer', u'Satyen Kale', u'Haipeng Luo']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/8568/efficient-learning-large-scale-combinatorial-semi-bandits
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the of- fline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.
 </p>
</p>

<p>
 A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the of- fline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.
</p>
<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Efficient Learning in Large-Scale Combinatorial Semi-Bandits
International Conference on Machine Learning (ICML 2015)
[u'Zheng Wen', u'Branislav Kveton', u'Azin Ashkan']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6779/verifiable-stream-computation-and-arthur-merlin-communication
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the setting of streaming interactive proofs (SIPs), a client (verifier) needs to compute a given function on a massive stream of data, arriving online, but is unable to store even a small fraction of the data. It outsources the processing to a third party service (prover), but is unwilling to blindly trust answers returned by this service. Thus, the service cannot simply supply the desired answer; it must convince the verifier of its correctness via a short interaction after the stream has been seen.

In this work we study &quot;barely interactive&quot; SIPs. Specifically, we show that two or three rounds of interaction suffice to solve several query problems --- including Index, Median, Nearest Neighbor Search, Pattern Matching, and Range Counting --- with polylogarithmic space and communication costs. Such efficiency with O(1) rounds of interaction was thought to be impossible based on previous work.

On the other hand, we initiate a formal study of the limitations of constant-round SIPs by introducing a new hierarchy of communication models called Online Interactive Proofs (OIPs). The online nature of these models is analogous to the streaming restriction placed upon the verifier in an SIP. We give upper and lower bounds that (1) characterize, up to quadratic blowups, every finite level of the OIP hierarchy in terms of other well-known communication complexity classes, (2) separate the first four levels of the hierarchy, and (3) reveal that the hierarchy collapses to the fourth level. Our study of OIPs reveals marked contrasts and some parallels with the classic Turing Machine theory of interactive proofs, establishes limits on the power of existing techniques for developing constant-round SIPs, and provides a new characterization of (non-online) Arthur-Merlin communication in terms of an online model.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/oipccc.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/oipccc.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/oipccc.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Verifiable Stream Computation and Arthur-Merlin Communication
Computational Complexity Conference
[u'Justin Thaler', u'Amit Chakrabarti', u'Grahamcormode', u'Graham Cormode', u'Suresh Venkatasubramanian']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=3
found
 LINK 
https://labs.yahoo.com/publications/6778/profiling-warehouse-scale-computer
found
<h6>
 Abstract
</h6>

<p class="leading">
 With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live datacenter jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of different applications.

We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate application variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify common building blocks in the lower levels of the software stack. This datacenter tax can comprise nearly 30% of cycles across jobs running in the fleet, which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors, especially in the cache hierarchy. Typical workloads place significant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often, but compute heavily in bursts. These observations motivate several interesting directions for future warehouse-scale computers.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/isca15wsc.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/isca15wsc.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/isca15wsc.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Profiling a Warehouse-Scale Computer
International Symposium on Computer Architecture
[u'Kim Hazelwood', u'Svilen Kanev', u'Juan Pablo Darago', u'Parthasarathy Ranganathan', u'Tipp Moseley', u'Gu Yeon Wei', u'David Brooks']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6776/video-co-summarization-video-summarization-visual-co-occurrence
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present video co-summarization, a novel perspective to video summarization that exploits visual co-occurrence across multiple videos. Motivated by the observation that important visual concepts tend to appear repeatedly across videos of the same topic, we propose to summarize a video by finding shots that co-occur most frequently across videos collected using a topic keyword. The main technical challenge is dealing with the sparsity of co-occurring patterns, out of hundreds to possibly thousands of irrelevant shots in videos being considered. To deal with this challenge, we developed a Maximal Biclique Finding (MBF) algorithm that is optimized to find sparsely co-occurring patterns, discarding less co-occurring patterns even if they are dominant in one video. Our algorithm is parallelizable with closed-form updates, thus can easily scale up to handle a large number of videos simultaneously. We demonstrate the effectiveness of our approach on motion capture and self-compiled YouTube datasets. Our results suggest that summaries generated by visual co-occurrence tend to match more closely with human generated summaries, when compared to several popular unsupervised techniques.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cosum_final.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cosum_final.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cosum_final.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Video Co-summarization: Video Summarization by Visual Co-occurrence
CVPR 2015, IEEE International Conference on Computer Vision and Pattern Recognition
[u'Yale Song', u'Alejandro Jaimes', u'Wen Sheng Chu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6777/tvsum-summarizing-web-videos-using-titles
found
<h6>
 Abstract
</h6>

<p class="leading">
 Video summarization is a challenging problem in part because knowing which part of a video is important requires prior knowledge about its main topic. We present TVSum, an unsupervised video summarization framework that uses title-based image search results to find visually important shots. We observe that a video title is often carefully chosen to be maximally descriptive of its main topic, and hence images related to the title can serve as a proxy for important visual concepts of the main topic. However, because titles are free-formed, unconstrained, and often written ambiguously, images searched using the title can contain noise (images irrelevant to video content) and variance (images of different topics). To deal with this challenge, we developed a novel co-archetypal analysis technique that learns canonical visual concepts shared between video and images, but not in either alone, by finding a joint-factorial representation of two data sets. We introduce a new benchmark dataset, TVSum50, that contains 50 videos and their shot-level importance scores annotated via crowdsourcing. Experimental results on two datasets, SumMe and TVSum50, suggest our approach produces superior quality summaries compared to several recently proposed approaches.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tvsum-cvpr15.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tvsum-cvpr15.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tvsum-cvpr15.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
TVSum: Summarizing Web Videos Using Titles
CVPR 2015, IEEE International Conference on Computer Vision and Pattern Recognition
[u'Yale Song', u'Jordi Vallmitjana', u'Amanda Stent', u'Alejandro Jaimes']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6775/partying-your-face-says-it-all-predicting-ambiance-places-profile-pictures
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a places ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state- of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a places ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state- of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.
  </div>
 </div>
</div>
<div>
 <div>
  To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a places ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state- of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.
 </div>
</div>

<div>
 To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a places ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state- of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICWSM15-2761.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICWSM15-2761.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICWSM15-2761.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures
THE 9TH INTERNATIONAL AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM-15)
[u'Miriam Redi', u'Daniele Quercia', u'Lindsay Graham', u'Samuel Gosling']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6758/detection-anomalies-large-collections-time-series
found
<h6>
 Abstract
</h6>

<p class="leading">
 It is becoming increasingly common for organizations to collect very large amounts of data over time, and to need to detect unusual or anomalous time series.

For example, Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.

We compute a vector of features on each time series, measuring characteristics of the series. For example, the features may include lag correlation, strength of seasonality, spectral entropy, etc. Then we use a robust principal component decomposition on the features, and use various bivariate outlier detection methods applied to the first two principal components. This enables the most unusual series, based on their feature vectors, to be identified. The bivariate outlier detection methods used are based on highest density regions and alpha-hulls. Data from Yahoo mail servers will be used to illustrate the proposed methods.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iasc20151.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iasc20151.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iasc20151.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Detection of Anomalies in Large Collections of Time Series
IASC 2015
[u'Nikolay Laptev', u'Rob J Hyndman', u'Earo Wang']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6774/image-worth-more-thousand-favorites-surfacing-hidden-beauty-flickr-pictures
found
<h6>
 Abstract
</h6>

<p class="leading">
 The dynamics of attention in social media tend to obey power laws. Attention concentrates on a relatively small number of popular items and neglecting the vast majority of content produced by the crowd. Although popularity can be an indication of the perceived value of an item within its community, previous research has hinted to the fact that popularity is distinct from intrinsic quality. As a result, content with low visibility but high quality lurks in the tail of the popularity distribution. This phenomenon can be particularly evident in the case of photo-sharing communities, where valuable photographers who are not highly engaged in online social interactions contribute with high-quality pictures that remain unseen. We propose to use a computer vision method to surface beautiful pictures from the immense pool of near-zero-popularity items, and we test it on a large dataset of creative-commons photos on Flickr. By gathering a large crowdsourced ground truth of aesthetics scores for Flickr images, we show that our method retrieves photos whose median perceived beauty score is equal to the most popular ones, and whose average is lower by only 1.5%.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sociopixels.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sociopixels.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sociopixels.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures
THE 9TH INTERNATIONAL AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM-15)
[u'Miriam Redi', u'Luca Maria Aiello', u'Rossano Schifanella']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6755/why-we-filter-our-photos-and-how-it-impacts-engagement
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    A variety of simple graphical filters are available to camera phone users to enhance their photos on the fly; these filters often stylize, saturate or age a photo. In this paper, we present a combination of large-scale data analysis and small scale in-depth interviews to understand filter-work. We look at producers practices of photo filtering and gain insights in the roles filters play in engaging photo consumers by driving their social interactions. We first interviewed 15 Flickr mobile app users (photo producers) to understand their use and perception of filters. Next, we analyzed how filters affect a photos engagement (consumers perspective) using a corpus of 7.6 million Flickr photos. We find two groups of serious and casual photographers among filter users. The serious see filters as correction tools and prefer milder effects. Casual photographers, by contrast, use filters to significantly transform their photos with bolder effects. We also find that filtered photos are 21% more likely to be viewed and 45% more likely to be commented on by consumers of photographs. Specifically, filters that increase warmth, exposure and contrast boost engagement the most. Towards the ongoing research in social engagement and photo-work, these findings suggest several practical implications such as designing filters for both serious and casual photographers or designing methods to prioritize and rank content in order to maximize engagement.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   A variety of simple graphical filters are available to camera phone users to enhance their photos on the fly; these filters often stylize, saturate or age a photo. In this paper, we present a combination of large-scale data analysis and small scale in-depth interviews to understand filter-work. We look at producers practices of photo filtering and gain insights in the roles filters play in engaging photo consumers by driving their social interactions. We first interviewed 15 Flickr mobile app users (photo producers) to understand their use and perception of filters. Next, we analyzed how filters affect a photos engagement (consumers perspective) using a corpus of 7.6 million Flickr photos. We find two groups of serious and casual photographers among filter users. The serious see filters as correction tools and prefer milder effects. Casual photographers, by contrast, use filters to significantly transform their photos with bolder effects. We also find that filtered photos are 21% more likely to be viewed and 45% more likely to be commented on by consumers of photographs. Specifically, filters that increase warmth, exposure and contrast boost engagement the most. Towards the ongoing research in social engagement and photo-work, these findings suggest several practical implications such as designing filters for both serious and casual photographers or designing methods to prioritize and rank content in order to maximize engagement.
  </div>
 </div>
</div>
<div>
 <div>
  A variety of simple graphical filters are available to camera phone users to enhance their photos on the fly; these filters often stylize, saturate or age a photo. In this paper, we present a combination of large-scale data analysis and small scale in-depth interviews to understand filter-work. We look at producers practices of photo filtering and gain insights in the roles filters play in engaging photo consumers by driving their social interactions. We first interviewed 15 Flickr mobile app users (photo producers) to understand their use and perception of filters. Next, we analyzed how filters affect a photos engagement (consumers perspective) using a corpus of 7.6 million Flickr photos. We find two groups of serious and casual photographers among filter users. The serious see filters as correction tools and prefer milder effects. Casual photographers, by contrast, use filters to significantly transform their photos with bolder effects. We also find that filtered photos are 21% more likely to be viewed and 45% more likely to be commented on by consumers of photographs. Specifically, filters that increase warmth, exposure and contrast boost engagement the most. Towards the ongoing research in social engagement and photo-work, these findings suggest several practical implications such as designing filters for both serious and casual photographers or designing methods to prioritize and rank content in order to maximize engagement.
 </div>
</div>

<div>
 A variety of simple graphical filters are available to camera phone users to enhance their photos on the fly; these filters often stylize, saturate or age a photo. In this paper, we present a combination of large-scale data analysis and small scale in-depth interviews to understand filter-work. We look at producers practices of photo filtering and gain insights in the roles filters play in engaging photo consumers by driving their social interactions. We first interviewed 15 Flickr mobile app users (photo producers) to understand their use and perception of filters. Next, we analyzed how filters affect a photos engagement (consumers perspective) using a corpus of 7.6 million Flickr photos. We find two groups of serious and casual photographers among filter users. The serious see filters as correction tools and prefer milder effects. Casual photographers, by contrast, use filters to significantly transform their photos with bolder effects. We also find that filtered photos are 21% more likely to be viewed and 45% more likely to be commented on by consumers of photographs. Specifically, filters that increase warmth, exposure and contrast boost engagement the most. Towards the ongoing research in social engagement and photo-work, these findings suggest several practical implications such as designing filters for both serious and casual photographers or designing methods to prioritize and rank content in order to maximize engagement.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper9.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper9.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper9.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Why We Filter Our Photos and How It Impacts Engagement
THE 9TH INTERNATIONAL AAAI CONFERENCE ON WEB AND SOCIAL MEDIA
[u'Saeideh Bakhshi', u'David Ayman Shamma', u'Lyndon Kennedy', u'Eric Gilbert']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6771/budget-constrained-item-cold-start-handling-collaborative-filtering-recommenders
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    It is well known that collaborative filtering (CF) based recommender systems provide better modeling of users and items associated with considerable rating history. The lack of historical ratings results in the user and the item cold- start problems. The latter is the main focus of this work. Most of the current literature addresses this problem by integrating content-based recommendation techniques to model the new item. However, in many cases such content is not available, and the question arises is whether this problem can be mitigated using CF techniques only. We formalize this problem as an optimization problem: given a new item, a pool of available users, and a budget constraint, select which users to assign with the task of rating the new item in order to minimize the prediction error of our model. We show that the objective function is monotone-supermodular, and propose efficient optimal design based algorithms that attain an approximation to its optimum. Our findings are verified by an empirical study using the Netflix dataset, where the proposed algorithms outperform several baselines for the problem at hand.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   It is well known that collaborative filtering (CF) based recommender systems provide better modeling of users and items associated with considerable rating history. The lack of historical ratings results in the user and the item cold- start problems. The latter is the main focus of this work. Most of the current literature addresses this problem by integrating content-based recommendation techniques to model the new item. However, in many cases such content is not available, and the question arises is whether this problem can be mitigated using CF techniques only. We formalize this problem as an optimization problem: given a new item, a pool of available users, and a budget constraint, select which users to assign with the task of rating the new item in order to minimize the prediction error of our model. We show that the objective function is monotone-supermodular, and propose efficient optimal design based algorithms that attain an approximation to its optimum. Our findings are verified by an empirical study using the Netflix dataset, where the proposed algorithms outperform several baselines for the problem at hand.
  </div>
 </div>
</div>
<div>
 <div>
  It is well known that collaborative filtering (CF) based recommender systems provide better modeling of users and items associated with considerable rating history. The lack of historical ratings results in the user and the item cold- start problems. The latter is the main focus of this work. Most of the current literature addresses this problem by integrating content-based recommendation techniques to model the new item. However, in many cases such content is not available, and the question arises is whether this problem can be mitigated using CF techniques only. We formalize this problem as an optimization problem: given a new item, a pool of available users, and a budget constraint, select which users to assign with the task of rating the new item in order to minimize the prediction error of our model. We show that the objective function is monotone-supermodular, and propose efficient optimal design based algorithms that attain an approximation to its optimum. Our findings are verified by an empirical study using the Netflix dataset, where the proposed algorithms outperform several baselines for the problem at hand.
 </div>
</div>

<div>
 It is well known that collaborative filtering (CF) based recommender systems provide better modeling of users and items associated with considerable rating history. The lack of historical ratings results in the user and the item cold- start problems. The latter is the main focus of this work. Most of the current literature addresses this problem by integrating content-based recommendation techniques to model the new item. However, in many cases such content is not available, and the question arises is whether this problem can be mitigated using CF techniques only. We formalize this problem as an optimization problem: given a new item, a pool of available users, and a budget constraint, select which users to assign with the task of rating the new item in order to minimize the prediction error of our model. We show that the objective function is monotone-supermodular, and propose efficient optimal design based algorithms that attain an approximation to its optimum. Our findings are verified by an empirical study using the Netflix dataset, where the proposed algorithms outperform several baselines for the problem at hand.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/itemColdStart_WWW2015_sub2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/itemColdStart_WWW2015_sub2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/itemColdStart_WWW2015_sub2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Budget-Constrained Item Cold-Start Handling in Collaborative Filtering Recommenders via Optimal Design
WWW 2015
[u'Oren Somekh', u'Nadav Golbandi', u'Zohar Karnin', u'Oleg Rokhlenko', u'Shahar Golan', u'Ronny Lempel', u'Oren Anava']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6756/hate-speech-detection-comment-embeddings
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 We address the problem of hate speech detection in online user comments. Hate speech, defined as an &quot;abusive speech targeting specific group characteristics, such as ethnicity, religion, or gender'', is an important problem plaguing websites that allow users to leave feedback, having a negative impact on their online business and overall user experience. We propose to learn distributed low-dimensional representations of comments using recently proposed neural language models, that can then be fed as inputs to a classification algorithm. Our approach addresses issues of high-dimensionality and sparsity that impact the current state-of-the-art, resulting in highly efficient and effective hate speech detectors.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Hate Speech Detection with Comment Embeddings
International World Wide Web Conference (WWW)
[u'Nemanja Djuric', u'Mihajlo Grbovic', u'Vladan Radosavljevic', u'Narayan Bhamidipati', u'Jing Zhou', u'Robin Morris']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6757/evolution-conversations-age-email-overload
found
<h6>
 Abstract
</h6>

<p class="leading">
 Email is a ubiquitous communications tool in the workplace and plays an important role in social interactions. Previous studies of email were largely based on surveys and limited to relatively small populations of email users within organizations. In this paper, we report results of a large-scale study of more than 2 million users exchanging 16 billion emails over several months. We quantitatively characterize the replying behavior in conversations within pairs of users. In particular, we study the time it takes the user to reply to a received message and the length of the reply sent. We consider a variety of factors that affect the reply time and length, such as the stage of the conversation, user demographics, and use of portable devices. In addition, we study how increasing load affects emailing behavior. We find that as users receive more email messages in a day, they reply to a smaller fraction of them, using shorter replies. However, their responsiveness remains intact, and they may even reply to emails faster. Finally, we predict the time to reply, length of reply, and whether the reply ends a conversation. We demonstrate considerable improvement over the baseline in all three prediction tasks, showing the significant role that the factors that we uncover play, in determining replying behavior. We rank these factors based on their predictive power. Our findings have important implications for understanding human behavior and designing better email management applications for tasks like ranking unread emails.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Evolution of Conversations in the Age of Email Overload
International World Wide Web Conference
[u'Luca Maria Aiello', u'Mihajlo Grbovic', u'Amin Mantrach', u'Farshad Kooti', u'Kristina Lerman']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=4
found
 LINK 
https://labs.yahoo.com/publications/6749/search-retargeting-using-directed-query-embeddings
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 Determining user audience for online ad campaigns is a critical problem to companies competing in online advertising space. One of the most popular strategies is search retargeting, which involves targeting users that issued search queries related to advertiser's core business, commonly specified by advertisers themselves. However, advertisers often fail to include many relevant queries, which results in suboptimal campaigns and negatively impacts revenue for both advertisers and publishers. To address this issue, we use recently proposed neural language models to learn low-dimensional, distributed query embeddings, which can be used to expand query lists with related queries through simple nearest neighbor searches in the embedding space. Experiments on real-world data set strongly suggest benefits of the approach.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Search Retargeting using Directed Query Embeddings
International World Wide Web Conference (WWW)
[u'Mihajlo Grbovic', u'Nemanja Djuric', u'Vladan Radosavljevic', u'Narayan Bhamidipati']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6750/querycategorizr-large-scale-semi-supervised-system-categorization-web-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 Understanding interests expressed through user's search query is a task of critical importance for many internet applications. To help identify user interests, web engines commonly utilize classification of queries into one or more pre-defined interest categories. However, majority of the queries are noisy short texts, making accurate classification a challenging task. In this demonstration, we present queryCategorizr, a novel semi-supervised learning system that embeds queries into low-dimensional vector space using a neural language model applied on search log sessions, and classifies them into general interest categories while relying on a small set of labeled queries. Empirical results on large-scale data show that queryCategorizr outperforms the current state-of-the-art approaches. In addition, we describe a Graphical User Interface (GUI) that allows users to query the system and explore classification results in an interactive manner.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwB.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwB.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic2015wwwB.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
queryCategorizr: A Large-Scale Semi-Supervised System for Categorization of Web Search Queries
International World Wide Web Conference (WWW) 2015
[u'Mihajlo Grbovic', u'Nemanja Djuric', u'Vladan Radosavljevic', u'Narayan Bhamidipati', u'Jordan Hawker', u'Caleb Johnson']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6757/evolution-conversations-age-email-overload
found
<h6>
 Abstract
</h6>

<p class="leading">
 Email is a ubiquitous communications tool in the workplace and plays an important role in social interactions. Previous studies of email were largely based on surveys and limited to relatively small populations of email users within organizations. In this paper, we report results of a large-scale study of more than 2 million users exchanging 16 billion emails over several months. We quantitatively characterize the replying behavior in conversations within pairs of users. In particular, we study the time it takes the user to reply to a received message and the length of the reply sent. We consider a variety of factors that affect the reply time and length, such as the stage of the conversation, user demographics, and use of portable devices. In addition, we study how increasing load affects emailing behavior. We find that as users receive more email messages in a day, they reply to a smaller fraction of them, using shorter replies. However, their responsiveness remains intact, and they may even reply to emails faster. Finally, we predict the time to reply, length of reply, and whether the reply ends a conversation. We demonstrate considerable improvement over the baseline in all three prediction tasks, showing the significant role that the factors that we uncover play, in determining replying behavior. We rank these factors based on their predictive power. Our findings have important implications for understanding human behavior and designing better email management applications for tasks like ranking unread emails.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www15mail.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Evolution of Conversations in the Age of Email Overload
International World Wide Web Conference
[u'Luca Maria Aiello', u'Mihajlo Grbovic', u'Amin Mantrach', u'Farshad Kooti', u'Kristina Lerman']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6756/hate-speech-detection-comment-embeddings
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 We address the problem of hate speech detection in online user comments. Hate speech, defined as an &quot;abusive speech targeting specific group characteristics, such as ethnicity, religion, or gender'', is an important problem plaguing websites that allow users to leave feedback, having a negative impact on their online business and overall user experience. We propose to learn distributed low-dimensional representations of comments using recently proposed neural language models, that can then be fed as inputs to a classification algorithm. Our approach addresses issues of high-dimensionality and sparsity that impact the current state-of-the-art, resulting in highly efficient and effective hate speech detectors.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2015wwwB.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Hate Speech Detection with Comment Embeddings
International World Wide Web Conference (WWW)
[u'Nemanja Djuric', u'Mihajlo Grbovic', u'Vladan Radosavljevic', u'Narayan Bhamidipati', u'Jing Zhou', u'Robin Morris']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6759/graphsc-parallel-secure-computation-made-easy
found
<h6>
 Abstract
</h6>

<p class="leading">
 AbstractWe propose introducing modern parallel programming paradigms to secure computation, enabling their secure execution on large datasets. To address this challenge, we present GraphSC, a framework that (i) provides a programming paradigm that allows non-cryptography experts to write secure code; (ii) brings parallelism to such secure implementations; and (iii) meets the needs for obliviousness, thereby not leaking any private information. Using GraphSC, developers can efficiently implement an oblivious version of graph-based algorithms (including sophisticated data mining and machine learning algorithms) that execute in parallel with minimal communication overhead. Importantly, our secure version of graph-based algorithms incurs a small logarithmic overhead in comparison with the non-secure parallel version. We build GraphSC and demonstrate, using several algorithms as examples, that secure computation can be brought into the realm of practicality for big data analysis. Our secure matrix factorization implementation can process 1 million ratings in 13 hours, which is a multiple order-of-magnitude improvement over the only other existing attempt, which requires 3 hours to process 16K ratings.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GraphSC.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GraphSC.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GraphSC.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
GraphSC: Parallel Secure Computation Made Easy
IEEE Security and Privacy
[u'Stratis Ioannidis', u'Kartik Nayak', u'Xiao Shaun Wang', u'Udi Weinsberg', u'Nina Taft', u'Elaine Runting Shi']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6772/serving-ads-%E2%80%9Cyahoo-answers%E2%80%9D-occasional-visitors
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    Modern ad serving systems can benefit when allowed to accumu- late user information and use it as part of the serving algorithm. However, this often does not coincide with how the web is used. Many domains will see users for only brief interactions, as users enter a domain through a search result or social media link and then leave. Having access to little or no user information and no ability to assemble a user profile over a prolonged period of use, we would still like to leverage the information we have to the best of our ability. In this paper we attempt several methods of improving ad serving for occasional users, including leveraging user information that is still available, content analysis of the page, information about the pages content generators and historical breakdown of visits to the page. We compare and combine these methods in a framework of a collaborative filtering algorithm, test them on real data collected from Yahoo Answers, and achieve significant improvements over baseline algorithms.
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   Modern ad serving systems can benefit when allowed to accumu- late user information and use it as part of the serving algorithm. However, this often does not coincide with how the web is used. Many domains will see users for only brief interactions, as users enter a domain through a search result or social media link and then leave. Having access to little or no user information and no ability to assemble a user profile over a prolonged period of use, we would still like to leverage the information we have to the best of our ability. In this paper we attempt several methods of improving ad serving for occasional users, including leveraging user information that is still available, content analysis of the page, information about the pages content generators and historical breakdown of visits to the page. We compare and combine these methods in a framework of a collaborative filtering algorithm, test them on real data collected from Yahoo Answers, and achieve significant improvements over baseline algorithms.
  </div>
 </div>
</div>
<div>
 <div>
  Modern ad serving systems can benefit when allowed to accumu- late user information and use it as part of the serving algorithm. However, this often does not coincide with how the web is used. Many domains will see users for only brief interactions, as users enter a domain through a search result or social media link and then leave. Having access to little or no user information and no ability to assemble a user profile over a prolonged period of use, we would still like to leverage the information we have to the best of our ability. In this paper we attempt several methods of improving ad serving for occasional users, including leveraging user information that is still available, content analysis of the page, information about the pages content generators and historical breakdown of visits to the page. We compare and combine these methods in a framework of a collaborative filtering algorithm, test them on real data collected from Yahoo Answers, and achieve significant improvements over baseline algorithms.
 </div>
</div>

<div>
 Modern ad serving systems can benefit when allowed to accumu- late user information and use it as part of the serving algorithm. However, this often does not coincide with how the web is used. Many domains will see users for only brief interactions, as users enter a domain through a search result or social media link and then leave. Having access to little or no user information and no ability to assemble a user profile over a prolonged period of use, we would still like to leverage the information we have to the best of our ability. In this paper we attempt several methods of improving ad serving for occasional users, including leveraging user information that is still available, content analysis of the page, information about the pages content generators and historical breakdown of visits to the page. We compare and combine these methods in a framework of a collaborative filtering algorithm, test them on real data collected from Yahoo Answers, and achieve significant improvements over baseline algorithms.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/adClickPredictionForUnknownUsers_WWW2015_TargetAdWorkshop_final2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/adClickPredictionForUnknownUsers_WWW2015_TargetAdWorkshop_final2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/adClickPredictionForUnknownUsers_WWW2015_TargetAdWorkshop_final2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Serving Ads to Yahoo Answers Occasional Visitors
Ad Targeting Workshop, WWW 2015 (WWW 2015)
[u'Michal Aharon', u'Amit Kagian', u'Raz Nissim', u'Oren Somekh', u'Yohay Kaplan']
Information and Knowledge Management
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6769/improved-theoretical-and-practical-guarantees-chromatic-correlation-clustering
found
<h6>
 Abstract
</h6>

<p class="leading">
 We study a natural generalization of the correlation clustering problem to graphs in which the pairwise relations between objects are categorical instead of binary. This problem was recently introduced by Bonchi et al. under the name of chromatic correlation clustering, and is motivated by many real-world applications in data-mining and social networks, including community detection, link classification, and entity de-duplication. Our main contribution is a fast and easy-to-implement constant approximation framework for the problem, which builds on a novel reduction of the problem to that of correlation clustering. This result significantly progresses the current state of knowledge for the problem, improving on a previous result that only guaranteed linear approximation in the input size. We complement the above result by developing a linear programming-based algorithm that achieves an improved approximation ratio of 4. Although this algorithm cannot be considered to be practical, it further extends our theoretical understanding of chromatic correlation clustering. We also present a fast heuristic algorithm that is motivated by real-life scenarios in which there is a groundtruth clustering that is obscured by noisy observations. We test our algorithms on both synthetic and real datasets, like social networks data. Our experiments reinforce the theoretical findings by demonstrating that our algorithms generally outperform previous approaches, both in terms of solution cost and reconstruction of an underlying ground-truth clustering.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Improved-Theoretical-and-Practical-Guarantees-for-Chromatic-Correlation-Clustering1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Improved-Theoretical-and-Practical-Guarantees-for-Chromatic-Correlation-Clustering1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Improved-Theoretical-and-Practical-Guarantees-for-Chromatic-Correlation-Clustering1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Improved Theoretical and Practical Guarantees for Chromatic Correlation Clustering
24th International World Wide Web Conference (WWW 2015)
[u'Iftah Gamzu', u'Yael Anava', u'Noa Avigdor Elgrabli']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6770/you-will-get-mail-predicting-arrival-future-email
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  The majority of Web email is known to be generated by machines even when one excludes spam. Many machine-generated email messages such as invoices or travel itineraries are critical to users. Recent research studies establish that causality relations between certain types of machine-generated email messages exist and can be mined. These relations exhibit a link between a given message to a past message that gave rise to its creation. For example, a shipment notification message can often be linked to a past online purchase message. Instead of studying how an incoming message can be linked to the past, we propose here to focus on predicting future email arrival as implied by causality relations. Such a prediction method has several potential applications, ranging from improved ad targeting in up sell scenarios to reducing false positives in spam detection. We introduce a novel approach for predicting which types of machine-generated email messages, represented by so-called email templates, a user should receive in future time windows. Our prediction approach relies on (1) statistically inferring causality relations between email templates, (2) building a generative model that explains the inbox of each user using those causality relations, and (3) combining those results to predict which email templates are likely to appear in future time frames. We present preliminary experimental results and some data insights obtained by analyzing several million inboxes of Yahoo Mail users, who voluntarily opted-in for such research.
 </p>
</p>

<p>
 The majority of Web email is known to be generated by machines even when one excludes spam. Many machine-generated email messages such as invoices or travel itineraries are critical to users. Recent research studies establish that causality relations between certain types of machine-generated email messages exist and can be mined. These relations exhibit a link between a given message to a past message that gave rise to its creation. For example, a shipment notification message can often be linked to a past online purchase message. Instead of studying how an incoming message can be linked to the past, we propose here to focus on predicting future email arrival as implied by causality relations. Such a prediction method has several potential applications, ranging from improved ad targeting in up sell scenarios to reducing false positives in spam detection. We introduce a novel approach for predicting which types of machine-generated email messages, represented by so-called email templates, a user should receive in future time windows. Our prediction approach relies on (1) statistically inferring causality relations between email templates, (2) building a generative model that explains the inbox of each user using those causality relations, and (3) combining those results to predict which email templates are likely to appear in future time frames. We present preliminary experimental results and some data insights obtained by analyzing several million inboxes of Yahoo Mail users, who voluntarily opted-in for such research.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/future_email.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/future_email.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/future_email.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
You Will Get Mail! Predicting the Arrival of Future Email
5th Temporal Web Analytics Workshop (TempWeb 2015)
[u'Iftah Gamzu', u'Zohar Karnin', u'Yoelle Maarek', u'David Wajc']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6764/consistent-collective-matrix-completion-under-joint-low-rank-structure
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 We address the collective matrix completion problem of jointly recovering a collection of matrices with shared structure from partial (and potentially noisy) observations. To ensure well--posedness of the problem, we impose a joint low rank structure, wherein each component matrix is low rank and the latent space of the low rank factors corresponding to each entity is shared across the entire collection. We first develop a rigorous algebra for representing and manipulating collective--matrix structure, and identify sufficient conditions for consistent estimation of collective matrices. We then propose a tractable convex estimator for solving the collective matrix completion problem, and provide the first non--trivial theoretical guarantees for consistency of collective matrix completion. We show that under reasonable assumptions stated in Sec. 3.1, with high probability, the proposed estimator exactly recovers the true matrices whenever sample complexity requirements dictated by Theorem 1 are met. The sample complexity requirement derived in the paper are optimum up to logarithmic factors, and significantly improve upon the requirements obtained by trivial extensions of standard matrix completion. Finally, we propose a scalable approximate algorithm to solve the proposed convex program, and corroborate our results through simulated and real life experiments.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/448.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/448.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/448.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Consistent Collective Matrix Completion Under Joint Low Rank Structure
The 18th International Conference on Artificial Intelligence and Statistics (AISTATS 2015)
[u'Makoto Yamada', u'Dawei Yin', u'Yi Chang', u'Suriya Gunasekar']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6766/effect-human-computer-interfaces-language-expression
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  Language expression is known to be dependent on attributes intrinsic to the author. To date, however, little attention has been devoted to the effect of interfaces used to articulate language on its expression. Here we study a large corpus of text written using different input devices and show that writers unconsciously prefer different letters depending on the interplay between their individual traits (e.g., hand laterality and injuries) and the layout of keyboards. Our results show, for the first time, how the interplay between technology and its users modifies language expression.
 </div>
</p>

<div>
 Language expression is known to be dependent on attributes intrinsic to the author. To date, however, little attention has been devoted to the effect of interfaces used to articulate language on its expression. Here we study a large corpus of text written using different input devices and show that writers unconsciously prefer different letters depending on the interplay between their individual traits (e.g., hand laterality and injuries) and the layout of keyboards. Our results show, for the first time, how the interplay between technology and its users modifies language expression.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1505.00092v1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1505.00092v1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1505.00092v1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On the Effect of Human-Computer Interfaces on Language Expression
arXiv.org
[u'Dan Pelleg', u'Elad Yom Tov', u'Egabrilovich']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=5
found
 LINK 
https://labs.yahoo.com/publications/6739/exploiting-sparsity-and-co-occurrence-structure-action-unit-recognition
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present a novel Bayesian framework for facial action unit recognition. The first key observation behind this work is sparsity: out of possible 45 (and more) facial action units, only very few are active at any moment. The second is the strong statistical co-occurrence structure: most facial expressions are made by common combinations of facial action units, so knowing the presence of one can act as a strong prior for inferring the presence of others. We developed a novel Bayesian graphical model that encodes these two natural aspects of facial action units via compressed sensing and group-wise sparsity inducing priors. One crucial aspect of our approach is the allowance of overlapping group structures, which proves useful in dealing with action units that occur frequently across multiple groups. We derive an efficient inference scheme and show how such sparsity and co-occurrence can be automatically learned from data. Experiments on three standard benchmark datasets show superiority over the state-of-the-art.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/exploiting-sparsity-fg15.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/exploiting-sparsity-fg15.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/exploiting-sparsity-fg15.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Exploiting Sparsity and Co-occurrence Structure for Action Unit Recognition
IEEE International Conference on Automatic Face and Gesture Recognition, FG 2015
[u'Yale Song', u'Daniel Mcduff', u'Deepak Vasisht', u'Ashish Kapoor']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6737/beauty-capturing-faces-rating-quality-digital-portraits
found
<h6>
 Abstract
</h6>

<p class="leading">
 Digital portrait photographs are everywhere, and while the number of face pictures keeps growing, not much work has been done to on automatic portrait beauty assessment.In this paper, we design a specific framework to automatically evaluate the beauty of digital portraits. To this end, we procure a large dataset of face images annotated not only with aesthetic scores but also with information about the traits of the subject portrayed. We design a set of visual features based on portrait photography literature, and extensively analyze their relation with portrait beauty, exposing interesting findings about what makes a portrait beautiful. We find that the beauty of a portrait is linked to its artistic value, and independent from age, race and gender of the subject. We also show that a classifier trained with our features to separate beautiful portraits from non-beautiful portraits outperforms generic aesthetic classifiers.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/FG20152.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/FG20152.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/FG20152.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Beauty of Capturing Faces: Rating the Quality of Digital Portraits
IEEE International Conference on Face and Gesture Recognition, FG 2015
[u'Miriam Redi', u'Alejandro Jaimes', u'Gaurav Aggarwal', u'Gaurav Aggarwa']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6740/how-efficiently-evaluate-ram-programs-malicious-security
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Secure 2-party computation (2PC) is becoming practical for some applications. However, most approaches are limited by the fact that the desired functionality must be represented as a boolean circuit. In response, random-access machines (RAM programs) have recently been investigated as a promising alternative representation. In this work, we present the first practical protocols for evaluating RAM programs with security against malicious adversaries. A useful efficiency measure is to divide the cost of malicious-secure evaluation of $f$ by the cost of semi-honest-secure evaluation of $f$. Our RAM protocols achieve ratios matching the state of the art for circuit-based 2PC. For statistical security $2^{-s}$, our protocol without preprocessing achieves a ratio of $s$; our online-offline protocol has a pre-processing phase and achieves online ratio $\sim 2 s / \log T$, where $T$ is the total execution time of the RAM program. To summarize, our solutions show that the ``extra overhead&quot; of obtaining malicious security for RAM programs (beyond what is needed for circuits) is minimal and does not grow with the running time of the program.
 </p>
</p>

<p>
 Secure 2-party computation (2PC) is becoming practical for some applications. However, most approaches are limited by the fact that the desired functionality must be represented as a boolean circuit. In response, random-access machines (RAM programs) have recently been investigated as a promising alternative representation. In this work, we present the first practical protocols for evaluating RAM programs with security against malicious adversaries. A useful efficiency measure is to divide the cost of malicious-secure evaluation of $f$ by the cost of semi-honest-secure evaluation of $f$. Our RAM protocols achieve ratios matching the state of the art for circuit-based 2PC. For statistical security $2^{-s}$, our protocol without preprocessing achieves a ratio of $s$; our online-offline protocol has a pre-processing phase and achieves online ratio $\sim 2 s / \log T$, where $T$ is the total execution time of the RAM program. To summarize, our solutions show that the ``extra overhead&quot; of obtaining malicious security for RAM programs (beyond what is needed for circuits) is minimal and does not grow with the running time of the program.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main8.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main8.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main8.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
How to Efficiently Evaluate RAM Programs with Malicious Security
Advances in Cryptology--EUROCRYPT (EUROCRYPT 2015)
[u'Payman Mohassel', u'Arash Afshar', u'Zhangxiang Hu', u'Mike Rosulek']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6742/bitcoin-backbone-protocol-analysis-and-applications
found
<h6>
 Abstract
</h6>

<p class="leading">
 Bitcoin is the first and most popular decentralized cryptocurrency todate. In this work, we extract and analyze the core of the Bitcoinprotocol, which we term the Bitcoin {\em backbone}, and prove two of
its fundamental properties which we call {\em common prefix} and {\emchain quality} in the static setting where the number of playersremains fixed. Our proofs hinge on appropriate and novel assumptionson the ''hashing power'' of the adversary relative to networksynchronicity; we show our results to be tight under highsynchronization.

Next, we propose and analyze applications that can be built &quot;on top''of the backbone protocol, specifically focusing on Byzantine agreement(BA) and on the notion of a public transaction ledger. Regarding BA,we observe that Nakamoto's suggestion falls short of solving it, andpresent a simple alternative which works assuming that the adversary'shashing power is bounded by $1/3$. The public transaction ledgercaptures the essence of Bitcoin's operation as a cryptocurrency, inthe sense that it guarantees the liveness and persistence of committedtransactions. Based on this notion we describe and analyze theBitcoin system as well as a more elaborate BA protocol, proving themsecure assuming high network synchronicity and that the adversary'shashing power is strictly less than $1/2$, while the adversarial boundneeded for security decreases as the network desynchronizes.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/bbb1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/bbb1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/bbb1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Bitcoin Backbone Protocol: Analysis and Applications
Eurocrypt 2015
[u'Juan A. Garay', u'Aggelos Kiayias', u'Nikos Leonardos']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6746/scaling-concurrent-log-structured-data-stores
found
<h6>
 Abstract
</h6>

<p class="leading">
 Log-structured data stores (LSM-DSs) are widely acceptedas the state-of-the-art implementation of key-value stores.They replace random disk writes with sequential I/O, byaccumulating large batches of updates in an in-memorydata structure and merging it with the on-disk store in thebackground. While LSM-DS implementations proved to behighly successful at masking the I/O bottleneck, scalingthem up on multicore CPUs remains a challenge. This isnontrivial due to their often rich APIs, as well as the need tocoordinate the RAM access with the background I/O.

We present cLSM, an algorithm for scalable concurrencyin LSM-DS, which exploits multiprocessor-friendly datastructures and non-blocking synchronization. cLSM supportsa rich API, including consistent snapshot scans andgeneral non-blocking read-modify-write operations.

We implement cLSM based on the popular LevelDB key-valuestore, and evaluate it using intensive synthetic workloadsas well as ones from production web-serving applications.Our algorithm outperforms state of the art LSM-DSimplementations, improving throughput by 1.5x to 2.5x.Moreover, cLSM demonstrates superior scalability with thenumber of cores (successfully exploiting twice as manycores as the competition).
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/clsm.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/clsm.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/clsm.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Scaling Concurrent Log-Structured Data Stores
EuroSys 2015
[u'Edward Bortnikov', u'Eshcar Hillel', u'Guy Gueta', u'Idit Keidar']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6736/large-scale-study-user-image-search-behavior-web
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this study, we analyze user image search behavior on a large-scale query log from Yahoo Image Search, based on the hypothesis that behavior is dependent on query type. We categorize queries using two orthogonal taxonomies (subject-based and facet-based) and identify important query types at the intersection of these taxonomies. We study user search behavior on a large-scale set of search sessions for each query type, examining characteristics of sessions, query reformulation patterns, click patterns, and page view patterns. We identify important behavioral differences across query types, in particular showing that some query types are more exploratory, while others correspond to focused search. We also supplement our study with a survey to link the behavioral differences to image search intent. Our findings shed light on the importance of considering query categories to better understand user behavior on image search platforms.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/chi_2015_imagesearch3.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/chi_2015_imagesearch3.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/chi_2015_imagesearch3.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Large-Scale Study of User Image Search Behavior on the Web
CHI 2015
[u"Neil O'hare", u'Alejandro Jaimes', u'Jaimie Y Park', u'Rossano Schifanella', u'Chin Wan Chung']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6738/situ-study-mobile-app-mobile-search-interactions
found
<h6>
 Abstract
</h6>

<p class="leading">
 When trying to satisfy an information need, smartphone users frequently transition from mobile search engines to mobile apps and vice versa. However, little is known about the nature of these transitions nor how mobile search and mobile apps interact. We report on a 2-week, mixed-method study involving 18 Android users, where we collected real-world mobile search and mobile app usage data alongside subjective insights on why certain interactions between apps and mobile search occur. Our results show that when people engage with mobile search they tend to interact with more mobile apps and for longer durations. We found that certain categories of apps are used more intensely alongside mobile search. Furthermore we found differences in app usage before and after mobile search and show how mobile app interactions can both prompt mobile search and enable users to take action. We conclude with a discussion on what these patterns mean for mobile search and how we might design mobile search experiences that take these app interactions into account.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mobisense-chi2015-CR.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mobisense-chi2015-CR.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mobisense-chi2015-CR.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
An In-Situ Study of Mobile App & Mobile Search Interactions
CHI 2015
[u'Karen Church', u'Juan Pablo Carrascal']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6763/multi-view-face-detection-using-deep-convolutional-neural-networks
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.
  </div>
 </div>
</div>
<div>
 <div>
  In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.
 </div>
</div>

<div>
 In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/multi-view-face_ICMR.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/multi-view-face_ICMR.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/multi-view-face_ICMR.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Multi-view Face Detection Using Deep Convolutional Neural Networks
International Conference on Multimedia Retrieval (ICMR)
[u'Sachin Sudhakar Farfade', u'Mohammad (ehsan) Saberian', u'Li Jia Li']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6760/data-compression-cost-optimization
found
<h6>
 Abstract
</h6>

<p class="leading">
 This paper proposes a general optimization framework to allocate computing resources to the compression of massive and heterogeneous data sets incident upon a communication or storage system. The framework is formulated using abstract parameters, and builds on rigorous tools from optimization theory. The outcome is a set of algorithms that together can reach optimal compression allocation in a realistic scenario involving a multitude of content types and compression tools. This claim is demonstrated by running the optimization algorithms on publicly available data sets, and showing up to 25% size reduction, with equal compute-time budget using standard compression tools.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/DCC-2015-Data-Compression-Cost-Optimization-8430a393.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/DCC-2015-Data-Compression-Cost-Optimization-8430a393.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/DCC-2015-Data-Compression-Cost-Optimization-8430a393.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Data Compression Cost Optimization
DCC 2015
[u'Eyal Zohar', u'Yuval Cassuto']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6754/yara-parser-fast-and-accurate-dependency-parser
found
<h6>
 Abstract
</h6>

<p class="leading">
 Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https: //github.com/yahoo/YaraParser.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1503.06733v1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1503.06733v1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1503.06733v1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Yara Parser: A Fast and Accurate Dependency Parser

[u'Joel Tetreault', u'Mohammad Sadegh Rasooli']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=6
found
 LINK 
https://labs.yahoo.com/publications/6751/little-honesty-goes-long-way-two-tier-model-secure-multiparty-computation
found
<h6>
 Abstract
</h6>

<p class="leading">
 Secure multiparty computation (MPC) as a service is becoming atangible reality. In such a service, a population of clients wish toutilize a set of servers to delegate privately and reliably a givencomputation on their inputs. MPC protocols have a number of desiredproperties including tolerating active misbehavior by some of theservers and guaranteed output delivery. A fundamental result is thatin order to achieve the above, an honest majority among servers isnecessary. There are settings, however, where this condition might beoverly restrictive, making it important to investigate models wherethis impossibility result can be circumvented, allowing securecomputation to be performed even when the number of maliciousparticipants outweighs the number of honest participants.

To this end, we introduce the two-tier model for MPC, where aset of m parties that are guaranteed to be honest (the firsttier) remains &quot;hidden&quot; within a set of n-m servers which are ofdubious trustworthiness (the second tier), and where theobjective is to perform MPC withstanding a number of activemisbehaviors that is larger than m/2. Indeed, assuming \alpha nof the second-tier servers are dishonest (where \alpha belongs to(0,1)), we present an MPC protocol that can withstand up to(1-\epsilon)(1-\alpha)n/2 additional faults, for any \epsilon>0and m = \omega(\log n). Somewhat surprisingly, this allowsthe total number of faulty parties to exceed n/2 across bothtiers.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/HiddenSet-0-main-full.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/HiddenSet-0-main-full.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/HiddenSet-0-main-full.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Little Honesty Goes a Long Way: The Two-Tier Model for Secure Multiparty Computation
The Twelfth Theory of Cryptography Conference -- TCC 2015
[u'Juan A. Garay', u'Ran Gelles', u'Aggelos Kiayias', u'David Johnson', u'Moti Yung']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6708/understanding-online-reviews-funny-cool-or-useful
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Increasingly online reviews are relied upon to make choices about the purchases and services we use daily. Businesses, on the other hand, depend on online review sites to find new customers and understand peoples perception of them. In order for an online review community to be effective to both users and businesses, it is important to understand what constitutes a high quality review as perceived by people, and how to maximize quality of reviews in the community. In this paper, we study Yelp to answer these questions. We analyze about 230,000 reviews and member interaction (votes) with these reviews. We find that active and regular members are the highest contributors to good quality reviews and longer reviews have higher chances of being popular in the community. We find that reviews voted useful tend to be the early ones reviews for a specific business. Our findings have implications on enabling high quality member contributions and community effectiveness. We discuss the implications to de- sign of social systems with diverse feedback signals.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Increasingly online reviews are relied upon to make choices about the purchases and services we use daily. Businesses, on the other hand, depend on online review sites to find new customers and understand peoples perception of them. In order for an online review community to be effective to both users and businesses, it is important to understand what constitutes a high quality review as perceived by people, and how to maximize quality of reviews in the community. In this paper, we study Yelp to answer these questions. We analyze about 230,000 reviews and member interaction (votes) with these reviews. We find that active and regular members are the highest contributors to good quality reviews and longer reviews have higher chances of being popular in the community. We find that reviews voted useful tend to be the early ones reviews for a specific business. Our findings have implications on enabling high quality member contributions and community effectiveness. We discuss the implications to de- sign of social systems with diverse feedback signals.
  </div>
 </div>
</div>
<div>
 <div>
  Increasingly online reviews are relied upon to make choices about the purchases and services we use daily. Businesses, on the other hand, depend on online review sites to find new customers and understand peoples perception of them. In order for an online review community to be effective to both users and businesses, it is important to understand what constitutes a high quality review as perceived by people, and how to maximize quality of reviews in the community. In this paper, we study Yelp to answer these questions. We analyze about 230,000 reviews and member interaction (votes) with these reviews. We find that active and regular members are the highest contributors to good quality reviews and longer reviews have higher chances of being popular in the community. We find that reviews voted useful tend to be the early ones reviews for a specific business. Our findings have implications on enabling high quality member contributions and community effectiveness. We discuss the implications to de- sign of social systems with diverse feedback signals.
 </div>
</div>

<div>
 Increasingly online reviews are relied upon to make choices about the purchases and services we use daily. Businesses, on the other hand, depend on online review sites to find new customers and understand peoples perception of them. In order for an online review community to be effective to both users and businesses, it is important to understand what constitutes a high quality review as perceived by people, and how to maximize quality of reviews in the community. In this paper, we study Yelp to answer these questions. We analyze about 230,000 reviews and member interaction (votes) with these reviews. We find that active and regular members are the highest contributors to good quality reviews and longer reviews have higher chances of being popular in the community. We find that reviews voted useful tend to be the early ones reviews for a specific business. Our findings have implications on enabling high quality member contributions and community effectiveness. We discuss the implications to de- sign of social systems with diverse feedback signals.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cscwf630.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cscwf630.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cscwf630.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Understanding Online Reviews: Funny, Cool or Useful?
CSCW'15
[u'Saeideh Bakhshi', u'Partha Kanuparthy', u'David Ayman Shamma']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6552/absence-time-and-user-engagement-evaluating-ranking-functions
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    In the online industry, user engagement is measured with various engagement metrics used to assess users depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner  a reflection of their engagement with the site  than if this is not the case. This assumption has the advantage of being simple and intuitive and appli- cable to a large number of settings. As a case study, we use a community Q&A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   In the online industry, user engagement is measured with various engagement metrics used to assess users depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner  a reflection of their engagement with the site  than if this is not the case. This assumption has the advantage of being simple and intuitive and appli- cable to a large number of settings. As a case study, we use a community Q&A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.
  </div>
 </div>
</div>
<div>
 <div>
  In the online industry, user engagement is measured with various engagement metrics used to assess users depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner  a reflection of their engagement with the site  than if this is not the case. This assumption has the advantage of being simple and intuitive and appli- cable to a large number of settings. As a case study, we use a community Q&A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.
 </div>
</div>

<div>
 In the online industry, user engagement is measured with various engagement metrics used to assess users depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner  a reflection of their engagement with the site  than if this is not the case. This assumption has the advantage of being simple and intuitive and appli- cable to a large number of settings. As a case study, we use a community Q&A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm20131.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm20131.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm20131.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Absence time and user engagement: Evaluating Ranking Functions
Sixth ACM International Conference on Web Search and Data Mining (WSDM), Rome, Italy
[u'Mounia Lalmas', u'Georges Dupret']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6752/%E2%80%9Cselena-gomez%E2%80%9D-%E2%80%9Cmarlon-brando%E2%80%9D-understanding-explorative-entity-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Consider a user who submits a search query &quot;Shakira&quot; having a specific search goal in mind (such as her age) but at the same time willing to explore information for other entities related to her, such as comparable singers. In previous work, a system called Spark, was developed to provide such search experience. Given a query submitted to the Yahoo search engine, Spark provides related entity suggestions for the query, exploiting, among else, public knowledge bases from the Semantic Web. We refer to this search scenario as explorative entity search. The effectiveness and efficiency of the approach has been demonstrated in previous work. The way users interact with these related entity suggestions and whether this interaction can be predicted have however not been studied. In this paper, we perform a large-scale analysis into how users interact with the entity results returned by Spark. We characterize the users, queries and sessions that appear to promote an explorative behavior. Based on this analysis, we develop a set of query and user-based features that reflect the click behavior of users and explore their effectiveness in the context of a prediction task.
 </p>
</p>

<p>
 Consider a user who submits a search query &quot;Shakira&quot; having a specific search goal in mind (such as her age) but at the same time willing to explore information for other entities related to her, such as comparable singers. In previous work, a system called Spark, was developed to provide such search experience. Given a query submitted to the Yahoo search engine, Spark provides related entity suggestions for the query, exploiting, among else, public knowledge bases from the Semantic Web. We refer to this search scenario as explorative entity search. The effectiveness and efficiency of the approach has been demonstrated in previous work. The way users interact with these related entity suggestions and whether this interaction can be predicted have however not been studied. In this paper, we perform a large-scale analysis into how users interact with the entity results returned by Spark. We characterize the users, queries and sessions that appear to promote an explorative behavior. Based on this analysis, we develop a set of query and user-based features that reflect the click behavior of users and explore their effectiveness in the context of a prediction task.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2015.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2015.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2015.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
From Selena Gomez to Marlon Brando: Understanding Explorative Entity Search
24th International World Wide Web Conference (WWW 2015), Florence, Italy
[u'Iris Miliaraki', u'Roi Blanco', u'Mounia Lalmas']
Human-Computer Interaction
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6753/friendship-maintenance-digital-age-applying-relational-lens-online-social
found
<h6>
 Abstract
</h6>

<p class="leading">
 CSCW research has explored mobile technologies to support social activity on the go to produce greater feelings of connectedness. Much of this has focused on different mobile devices, individual preferences and modes of use. Yet social activity and connectedness are about ongoing enactments of relationships across technologies. In this paper we propose a focus on friends and friendship maintenance in addition to individual preferences in the design and analysis of mobile communication technologies. We discuss three strategies people use to manage tensions in their friendships: selection, segmentation and integration. Our data show that use of social technologies can at times destabilize social relations and occasion relational tensions, forcing users to renegotiate how they enact these relationships.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/friendship-maintainance-digital-age-cscw2015-for-sharing.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/friendship-maintainance-digital-age-cscw2015-for-sharing.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/friendship-maintainance-digital-age-cscw2015-for-sharing.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Friendship Maintenance in the Digital Age: Applying a Relational Lens to Online Social Interaction
Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing (Proc. CSCW '15)
[u'Jofish Kaye', u'Irina Shklovski', u'Louise Barkhuus', u'Nis Boernoe']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6732/richer-efficiencysecurity-trade-offs-2pc
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  The dual-execution protocol of Mohassel and Franklin (PKC 2006) is a highly efficient (each party garbling only one circuit) 2PC protocol that achieves malicious security apart from leaking an  arbitrary, adversarially-chosen predicate about the honest party's input. We present two practical and orthogonal approaches to improve the security of the dual-execution technique. First, we show how to greatly restrict the predicate that an adversary can learn in the protocol, to a natural notion of ``only computation leaks''-style leakage. Along the way, we identify a natural security property of garbled circuits called  property-enforcing that may be of independent interest. Second, we address a complementary direction of reducing the probability that the leakage occurs. We propose a new dual-execution protocol --- with a very light cheating-detection phase and each party garbling s+1 circuits --- in which a cheating party learns a bit with probability only 2^{-s}. Our concrete measurements show approximately 35% reduction in communication for the AES circuit, compared to the best combination of state of the art techniques for achieving the same security notion. Combining the two results, we achieve a rich continuum of practical trade-offs between efficiency  and security, connecting the covert, dual-execution and full-malicious guarantees.
 </p>
</p>

<p>
 The dual-execution protocol of Mohassel and Franklin (PKC 2006) is a highly efficient (each party garbling only one circuit) 2PC protocol that achieves malicious security apart from leaking an  arbitrary, adversarially-chosen predicate about the honest party's input. We present two practical and orthogonal approaches to improve the security of the dual-execution technique. First, we show how to greatly restrict the predicate that an adversary can learn in the protocol, to a natural notion of ``only computation leaks''-style leakage. Along the way, we identify a natural security property of garbled circuits called  property-enforcing that may be of independent interest. Second, we address a complementary direction of reducing the probability that the leakage occurs. We propose a new dual-execution protocol --- with a very light cheating-detection phase and each party garbling s+1 circuits --- in which a cheating party learns a bit with probability only 2^{-s}. Our concrete measurements show approximately 35% reduction in communication for the AES circuit, compared to the best combination of state of the art techniques for achieving the same security notion. Combining the two results, we achieve a rich continuum of practical trade-offs between efficiency  and security, connecting the covert, dual-execution and full-malicious guarantees.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main7.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main7.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main7.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Richer Efficiency/Security Trade-offs in 2PC.
Theory of Cryptography Conference (TCC) (TCC 2015)
[u'Payman Mohassel', u'Vladimir Kolesnikov', u'Ben Riva', u'Mike Rosulek']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6743/emergent-privacy
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Defining privacy is a long sought goal for philosophers and legal scholars alike. Current definitions lack mathematical rigor. They are therefore impracticable for domains such as economics and computer science in which privacy needs to be quantified and computed. This paper describes a game theoretic framework in which privacy requires no definition per se. Rather, it is an emergent property of specific games, the strategy by which players maximize their reward. In this context, key activities related to privacy, such as methods for its protection and ways in which it is traded, are given concrete meaning. Based in game theory, emergent privacy demonstrates that the right to privacy can be derived, at least in part, on a utilitarian philosophical basis.
 </p>
</p>

<p>
 Defining privacy is a long sought goal for philosophers and legal scholars alike. Current definitions lack mathematical rigor. They are therefore impracticable for domains such as economics and computer science in which privacy needs to be quantified and computed. This paper describes a game theoretic framework in which privacy requires no definition per se. Rather, it is an emergent property of specific games, the strategy by which players maximize their reward. In this context, key activities related to privacy, such as methods for its protection and ways in which it is traded, are given concrete meaning. Based in game theory, emergent privacy demonstrates that the right to privacy can be derived, at least in part, on a utilitarian philosophical basis.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/EP.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/EP.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/EP.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Emergent Privacy
Journal of Philosophy
[u'Ran Wolff']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6741/automatic-scalable-atomicity-semantic-locking
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this paper, we consider concurrent programs in which the shared state consists of instances of linearizable ADTs (abstract data types). We present an automated approach to concurrency control that addresses a common need: the need to atomically execute a code fragment, which may contain multiple ADT operations on multiple ADT instances.

We present a synthesis algorithm that automatically enforces atomicity of given code fragments (in a client program) by inserting pessimistic synchronization that guarantees atomicity and deadlock-freedom (without using any rollback mechanism). Our algorithm takes a commutativity specification as an extra input. This specification indicates for every pair of ADT operations the conditions under which the operations commute. Our algorithm enables greater parallelism by permitting commuting operations to execute concurrently.

We have implemented the synthesis algorithm in a Java compiler, and applied it to several Java programs. Our results show that our approach produces efficient and scalable synchronization.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p31-golangueta.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p31-golangueta.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p31-golangueta.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Automatic Scalable Atomicity via Semantic Locking
20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
[u'Guy Gueta', u'G Ramalingam', u'Mooly Sagiv', u'Eran Yahav']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6989/red-purple-and-pink-colors-diffusion-pinterest
found
<h6>
 Abstract
</h6>

<p class="leading">
 Many lab studies have shown that colors can evoke powerful emotions and impact human behavior. Might these phenomena drive how we act online? A key research challenge for image-sharing communities is uncovering the mechanisms by which content spreads through the community. In this paper, we investigate whether there is link between color and diffusion. Drawing on a corpus of one million images crawled from Pinterest, we find that color significantly impacts the diffusion of images and adoption of content on image sharing communities such as Pinterest, even after partially controlling for network structure and activity. Specifically, Red, Purple and pink seem to promote diffusion, while Green, Blue, Black and Yellow suppress it. To our knowledge, our study is the first to investigate how colors relate to online user behavior. In addition to contributing to the research conversation surrounding diffusion, these findings suggest future work using sophisticated computer vision techniques. We conclude with a discussion on the theoretical, practical and design implications suggested by this worke.g. design of engaging image filters.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/journal.pone_.01171481.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/journal.pone_.01171481.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/journal.pone_.01171481.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Red, Purple and Pink: The Colors of Diffusion on Pinterest
PLOS ONE
[u'Saeideh Bakhshi', u'Eric Gilbert']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6722/robust-tree-based-causal-inference-complex-ad-effectiveness-analysis
found
<h6>
 Abstract
</h6>

<p class="leading">
 As the online advertising industry has evolved into an age of diverse ad formats and delivery channels, users are exposed to complex ad treatments involving various ad characteristics. The diversity and generality of ad treatments call for accurate and causal measurement of ad effectiveness, i.e., how the ad treatment {\it causes} the changes in outcomes without the confounding effect by user characteristics. Various causal inference approaches have been proposed to measure the causal effect of ad treatments. However, most existing causal inference methods focus on univariate and binary treatment and are not well suited for complex ad treatments. Moreover, to be practical in the data-rich online environment, the measurement needs to be highly general and efficient, which is not addressed in conventional causal inference approaches.In this paper we propose a novel causal inference framework for assessing the impact of general advertising treatments. Our new framework enables analysis on uni- or multi-dimensional ad treatments, where each dimension (ad treatment factor) could be discrete or continuous. We prove that our approach is able to provide an unbiased estimation of the ad effectiveness by controlling the confounding effect of user characteristics. The framework is computationally efficient by employing a tree structure that specifies the relationship between user characteristics and the corresponding ad treatment. This tree-based framework is robust to model misspecification and highly flexible with minimal manual tuning. To demonstrate the efficacy of our approach, we apply it to two advertising campaigns. In the first campaign we evaluate the impact of different ad frequencies, and in the second one we consider the synthetic ad effectiveness across TV and online platforms. Our framework successfully provides the causal impact of ads with different frequencies in both campaigns. Moreover, it shows that the ad frequency usually has a treatment effect cap, which is usually over-estimated by naive estimation.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wang_621.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wang_621.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wang_621.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Robust Tree-based Causal Inference for Complex Ad Effectiveness Analysis
WSDM 2015
[u'Pengyuan Wang', u'Dawei Yin', u'Jimmy Yang', u'Yi Chang', u'Wei Sun']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=7
found
 LINK 
https://labs.yahoo.com/publications/6733/predicting-next-app-you-are-going-use
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Given the large number of installed apps and the limited screen size of mobile devices, it is often tedious for users to search for the app they want to use. Although some mobile OSs provide categorization schemes that enhance the visibility of useful apps among those installed, the emerging category of homescreen apps aims to take one step further by automatically organizing the installed apps in a more intelligent and personalized way. In this paper, we study how to improve homescreen apps usage experience through a prediction mechanism that allows to show to users which app she is going to use in the immediate future. The prediction technique is based on a set of features representing the real-time spatiotemporal contexts sensed by the homescreen app. We model the prediction of the next app as a classification problem and propose an effective personalized method to solve it that takes full advantage of human-engineered features and automatically derived features. Furthermore, we study how to solve the two naturally associated cold-start problems: app cold-start and user cold-start. We conduct large-scale experiments on log data obtained from Yahoo Aviate, showing that our approach can accurately predict the next app that a person is going to use.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Given the large number of installed apps and the limited screen size of mobile devices, it is often tedious for users to search for the app they want to use. Although some mobile OSs provide categorization schemes that enhance the visibility of useful apps among those installed, the emerging category of homescreen apps aims to take one step further by automatically organizing the installed apps in a more intelligent and personalized way. In this paper, we study how to improve homescreen apps usage experience through a prediction mechanism that allows to show to users which app she is going to use in the immediate future. The prediction technique is based on a set of features representing the real-time spatiotemporal contexts sensed by the homescreen app. We model the prediction of the next app as a classification problem and propose an effective personalized method to solve it that takes full advantage of human-engineered features and automatically derived features. Furthermore, we study how to solve the two naturally associated cold-start problems: app cold-start and user cold-start. We conduct large-scale experiments on log data obtained from Yahoo Aviate, showing that our approach can accurately predict the next app that a person is going to use.
  </div>
 </div>
</div>
<div>
 <div>
  Given the large number of installed apps and the limited screen size of mobile devices, it is often tedious for users to search for the app they want to use. Although some mobile OSs provide categorization schemes that enhance the visibility of useful apps among those installed, the emerging category of homescreen apps aims to take one step further by automatically organizing the installed apps in a more intelligent and personalized way. In this paper, we study how to improve homescreen apps usage experience through a prediction mechanism that allows to show to users which app she is going to use in the immediate future. The prediction technique is based on a set of features representing the real-time spatiotemporal contexts sensed by the homescreen app. We model the prediction of the next app as a classification problem and propose an effective personalized method to solve it that takes full advantage of human-engineered features and automatically derived features. Furthermore, we study how to solve the two naturally associated cold-start problems: app cold-start and user cold-start. We conduct large-scale experiments on log data obtained from Yahoo Aviate, showing that our approach can accurately predict the next app that a person is going to use.
 </div>
</div>

<div>
 Given the large number of installed apps and the limited screen size of mobile devices, it is often tedious for users to search for the app they want to use. Although some mobile OSs provide categorization schemes that enhance the visibility of useful apps among those installed, the emerging category of homescreen apps aims to take one step further by automatically organizing the installed apps in a more intelligent and personalized way. In this paper, we study how to improve homescreen apps usage experience through a prediction mechanism that allows to show to users which app she is going to use in the immediate future. The prediction technique is based on a set of features representing the real-time spatiotemporal contexts sensed by the homescreen app. We model the prediction of the next app as a classification problem and propose an effective personalized method to solve it that takes full advantage of human-engineered features and automatically derived features. Furthermore, we study how to solve the two naturally associated cold-start problems: app cold-start and user cold-start. We conduct large-scale experiments on log data obtained from Yahoo Aviate, showing that our approach can accurately predict the next app that a person is going to use.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper113baezayates2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper113baezayates2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper113baezayates2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Predicting The Next App That You Are Going To Use
The Eighth ACM International Web Search and Data Mining Conference - WSDM 2015
[u'Ricardo Baeza-yates', u'Fabrizio Silvestri', u'Di Jiang', u'Beverly Harrison']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6734/finding-subgraphs-maximum-total-density-and-limited-overlap
found
<h6>
 Abstract
</h6>

<p class="leading">
 Finding dense subgraphs in large graphs is a key primitivein a variety of real-world application domains, encompassing social network analytics, event detection, biology, andfinance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs whichmight correspond to communities in social networks or interesting events. While a large amount of work is devotedto finding a single densest subgraph, perhaps surprisingly,the problem of finding several dense subgraphs with limitedoverlap has not been studied in a principled way, to the bestof our knowledge. In this work we define and study a naturalgeneralization of the densest subgraph problem, where themain goal is to find at most k subgraphs with maximum total aggregate density, while satisfying an upper bound on thepairwise Jaccard coefficient between the sets of nodes of thesubgraphs. After showing that such a problem is NP-Hard,we devise an efficient algorithm that comes with provableguarantees in some cases of interest, as well as, an efficientpractical heuristic. Our extensive evaluation on large realworld graphs confirms the efficiency and effectiveness of ouralgorithms.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm105-sozioA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm105-sozioA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm105-sozioA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Finding Subgraphs with Maximum Total Density and Limited Overlap
Proceedings of the International Conference on Web Search and Data Mining (WSDM 15)
[u'Francesco Bonchi', u'Francesco Gullo', u'Oana Balalau', u'T H Hubert Chan', u'Mauro Sozio']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6735/fast-and-space-efficient-entity-linking-queries
found
<h6>
 Abstract
</h6>

<p class="leading">
 Entity linking deals with identifying entities from a knowledge base in a given piece of text and has become a fundamental building block for web search engines, enabling numerous downstream improvements from better document ranking to enhanced search results pages. A key problem in the context of web search queries is that this process needs to run under severe time constraints as it has to be performed before any actual retrieval takes place, typically within milliseconds. In this paper we propose a probabilistic model that leverages user-generated information on the web to link queries to entities in a knowledge base. There are three key ingredients that make the algorithm fast and space-efficient. First, the linking process ignores any dependencies between the different entity candidates, which allows for a O(k^2) implementation in the number of query terms. Second, we leverage hashing and compression techniques to reduce the memory footprint. Finally, to equip the algorithm with contextual knowledge without sacrificing speed, we factor the distance between distributional semantics of the query words and entities into the model. We show that our solution significantly outperforms several state-of-the-art baselines by more than 14% while being able to process queries in sub-millisecond timesat least two orders of magnitude faster than existing systems.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WSDM-2015-blanco.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WSDM-2015-blanco.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WSDM-2015-blanco.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Fast and Space-Efficient Entity Linking in Queries
WSDM 2015: Proceedings of the eighth ACM international conference on Web search and data mining
[u'Roi Blanco', u'Edgar Meij', u'Giuseppe Ottaviano']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6727/exploiting-task-feature-co-clusters-multi-task-learning
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    In multi-task learning, multiple related tasks are considered simultaneously, with the goal to improve the generalization performance by utilizing the intrinsic sharing of information across tasks. This paper presents a multi-task learning approach by modeling the task-feature relationships. Specifically, instead of assuming that similar tasks have similar weights on all the features, we start with the motivation that the tasks should be related in terms of subsets of features, which implies a co-cluster structure. We design a novel regularization term to capture this task-feature co-cluster structure. A proximal algorithm is adopted to solve the optimization problem. Convincing experimental results demonstrate the effectiveness of the proposed algorithm and justify the idea of exploiting the task-feature relationships.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   In multi-task learning, multiple related tasks are considered simultaneously, with the goal to improve the generalization performance by utilizing the intrinsic sharing of information across tasks. This paper presents a multi-task learning approach by modeling the task-feature relationships. Specifically, instead of assuming that similar tasks have similar weights on all the features, we start with the motivation that the tasks should be related in terms of subsets of features, which implies a co-cluster structure. We design a novel regularization term to capture this task-feature co-cluster structure. A proximal algorithm is adopted to solve the optimization problem. Convincing experimental results demonstrate the effectiveness of the proposed algorithm and justify the idea of exploiting the task-feature relationships.
  </div>
 </div>
</div>
<div>
 <div>
  In multi-task learning, multiple related tasks are considered simultaneously, with the goal to improve the generalization performance by utilizing the intrinsic sharing of information across tasks. This paper presents a multi-task learning approach by modeling the task-feature relationships. Specifically, instead of assuming that similar tasks have similar weights on all the features, we start with the motivation that the tasks should be related in terms of subsets of features, which implies a co-cluster structure. We design a novel regularization term to capture this task-feature co-cluster structure. A proximal algorithm is adopted to solve the optimization problem. Convincing experimental results demonstrate the effectiveness of the proposed algorithm and justify the idea of exploiting the task-feature relationships.
 </div>
</div>

<div>
 In multi-task learning, multiple related tasks are considered simultaneously, with the goal to improve the generalization performance by utilizing the intrinsic sharing of information across tasks. This paper presents a multi-task learning approach by modeling the task-feature relationships. Specifically, instead of assuming that similar tasks have similar weights on all the features, we start with the motivation that the tasks should be related in terms of subsets of features, which implies a co-cluster structure. We design a novel regularization term to capture this task-feature co-cluster structure. A proximal algorithm is adopted to solve the optimization problem. Convincing experimental results demonstrate the effectiveness of the proposed algorithm and justify the idea of exploiting the task-feature relationships.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MTL_AAAI151.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MTL_AAAI151.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MTL_AAAI151.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Exploiting Task-Feature Co-Clusters in Multi-Task Learning
The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)
[u'Jianhui Chen', u'Linli Xu', u'Aiqing Huang', u'Enhong Chen']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6718/inertial-hidden-markov-models-modeling-change-multivariate-time-series
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Faced with the problem of characterizing systematic changes in multivariate time series in an unsupervised manner, we derive and test two methods of regularizing hidden Markov models for this task. Regularization on state transitions provide smooth transitioning among states, such that the sequences are split into broad, contiguous segments. Our methods are compared with a recent hierarchical Dirichlet process hidden Markov model (HDP-HMM) and a baseline standard hidden Markov model, of which the former suffers from poor performance on moderate-dimensional data and sensitivity to parameter settings, while the latter suffers from rapid state transitioning, over-segmentation and poor performance on a segmentation task involving human activity accelerometer data from the UCI Repository. The regularized methods developed here are able to perfectly characterize change of behavior the human activity data in roughly half of the real-data test cases, with accuracy of 94% and low variation of information. In contrast to the HDP-HMM, our methods provide simple, drop-in replacements for standard hidden Markov model update rules, allowing standard expectation maximization (EM) algorithms to be used for learning.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Faced with the problem of characterizing systematic changes in multivariate time series in an unsupervised manner, we derive and test two methods of regularizing hidden Markov models for this task. Regularization on state transitions provide smooth transitioning among states, such that the sequences are split into broad, contiguous segments. Our methods are compared with a recent hierarchical Dirichlet process hidden Markov model (HDP-HMM) and a baseline standard hidden Markov model, of which the former suffers from poor performance on moderate-dimensional data and sensitivity to parameter settings, while the latter suffers from rapid state transitioning, over-segmentation and poor performance on a segmentation task involving human activity accelerometer data from the UCI Repository. The regularized methods developed here are able to perfectly characterize change of behavior the human activity data in roughly half of the real-data test cases, with accuracy of 94% and low variation of information. In contrast to the HDP-HMM, our methods provide simple, drop-in replacements for standard hidden Markov model update rules, allowing standard expectation maximization (EM) algorithms to be used for learning.
  </div>
 </div>
</div>
<div>
 <div>
  Faced with the problem of characterizing systematic changes in multivariate time series in an unsupervised manner, we derive and test two methods of regularizing hidden Markov models for this task. Regularization on state transitions provide smooth transitioning among states, such that the sequences are split into broad, contiguous segments. Our methods are compared with a recent hierarchical Dirichlet process hidden Markov model (HDP-HMM) and a baseline standard hidden Markov model, of which the former suffers from poor performance on moderate-dimensional data and sensitivity to parameter settings, while the latter suffers from rapid state transitioning, over-segmentation and poor performance on a segmentation task involving human activity accelerometer data from the UCI Repository. The regularized methods developed here are able to perfectly characterize change of behavior the human activity data in roughly half of the real-data test cases, with accuracy of 94% and low variation of information. In contrast to the HDP-HMM, our methods provide simple, drop-in replacements for standard hidden Markov model update rules, allowing standard expectation maximization (EM) algorithms to be used for learning.
 </div>
</div>

<div>
 Faced with the problem of characterizing systematic changes in multivariate time series in an unsupervised manner, we derive and test two methods of regularizing hidden Markov models for this task. Regularization on state transitions provide smooth transitioning among states, such that the sequences are split into broad, contiguous segments. Our methods are compared with a recent hierarchical Dirichlet process hidden Markov model (HDP-HMM) and a baseline standard hidden Markov model, of which the former suffers from poor performance on moderate-dimensional data and sensitivity to parameter settings, while the latter suffers from rapid state transitioning, over-segmentation and poor performance on a segmentation task involving human activity accelerometer data from the UCI Repository. The regularized methods developed here are able to perfectly characterize change of behavior the human activity data in roughly half of the real-data test cases, with accuracy of 94% and low variation of information. In contrast to the HDP-HMM, our methods provide simple, drop-in replacements for standard hidden Markov model update rules, allowing standard expectation maximization (EM) algorithms to be used for learning.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI2015.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI2015.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI2015.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Inertial Hidden Markov Models: Modeling Change in Multivariate Time Series
AAAI 2015
[u'Saeed Amizadeh', u'Nikolay Laptev', u'George D Montanez']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6723/causal-inference-sparse-additive-models-application-online-advertising
found
<h6>
 Abstract
</h6>

<p class="leading">
 Advertising effectiveness measurement is a fundamental problem in online advertising. Various causal inference methods have been employed to measure the causal effects of ad treatments. However, existing methods mainly focus on linear logistic regression for univariate and binary treatments and are not well-suited for complex ad treatments of multi-dimensions, where each dimension could be discrete or continuous. In this paper we propose a novel two-stage causal inference framework for assessing the impact of complex ad treatments. In the first stage, we estimate the propensity parameter via a sparse additive model; in the second stage, a propensity-adjusted regression model is applied for measuring the treatment effect. Our approach is shown to provide an unbiased estimation of the ad effectiveness under regularity conditions. To demonstrate the efficacy of our approach, we apply it to a real online advertising campaign to evaluate the impact of three ad treatments: ad frequency, ad channel, and ad size. We show that the ad frequency usually has a treatment effect cap when ads are showing on mobile devices. In addition, the strategies for choosing best ad sizes are completely different for mobile ads and online ads.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI_882.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI_882.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/AAAI_882.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Causal Inference via Sparse Additive Models with Application to Online Advertising
AAAI 2015
[u'Pengyuan Wang', u'Dawei Yin', u'Jimmy Yang', u'Yi Chang', u'Wei Sun']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6731/shimmering-smartwatches-exploring-smartwatch-design-space
found
<h6>
 Abstract
</h6>

<p class="leading">
 We examine the nature of smartwatches and explore their associated user interface design space in this paper. Several smartwatches are using small graphical displays and as such are adopting similar forms. However, there are indications that other designs could be feasible. We discuss how smartwatches might use non-graphical displays and still offer smart capabilities. To demonstrate feasibility, we present two smartwatch prototypes and show how LED arrays can be used to dynamically support several functions needed by smartwatch applications. Finally, we discuss some tradeoffs associated with this approach and point to additional opportunities for investigating smartwatch designs.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper22.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper22.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper22.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Shimmering Smartwatches: Exploring the Smartwatch Design Space
TEI '15
[u'Cheng Xu', u'Kent Lyons']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6747/intonews-online-news-retrieval-using-closed-captions
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We present INTONEWS, a system to match online news articles with spoken news from a television newscasts represented by closed captions. We formalize the news matching problem as two independent tasks: closed captions segmentation and news retrieval. The system segments closed captions by using a windowing scheme: sliding or tumbling window. Next, it uses each segment to build a query by extracting representative terms. The query is used to retrieve previously indexed news articles from a search engine. To detect when a new article should be surfaced, the system compares the set of retrieved articles with the previously retrieved one. The intuition is that if the difference between these sets is large enough, it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user. In order to evaluate INTONEWS, we build a test collection using data coming from a second screen application and a major online news aggregator. The dataset is manually segmented and annotated by expert assessors, and used as our ground truth. It is freely available for download through the Webscope program. Our evaluation is based on a set of novel time-relevance metrics that take into account three different aspects of the problem at hand: precision, timeliness and coverage. We compare our algorithms against the best method previously proposed in literature for this problem. Experiments show the trade-offs involved among precision, timeliness and coverage of the airing news. Our best method is four times more accurate than the baseline.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We present INTONEWS, a system to match online news articles with spoken news from a television newscasts represented by closed captions. We formalize the news matching problem as two independent tasks: closed captions segmentation and news retrieval. The system segments closed captions by using a windowing scheme: sliding or tumbling window. Next, it uses each segment to build a query by extracting representative terms. The query is used to retrieve previously indexed news articles from a search engine. To detect when a new article should be surfaced, the system compares the set of retrieved articles with the previously retrieved one. The intuition is that if the difference between these sets is large enough, it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user. In order to evaluate INTONEWS, we build a test collection using data coming from a second screen application and a major online news aggregator. The dataset is manually segmented and annotated by expert assessors, and used as our ground truth. It is freely available for download through the Webscope program. Our evaluation is based on a set of novel time-relevance metrics that take into account three different aspects of the problem at hand: precision, timeliness and coverage. We compare our algorithms against the best method previously proposed in literature for this problem. Experiments show the trade-offs involved among precision, timeliness and coverage of the airing news. Our best method is four times more accurate than the baseline.
  </div>
 </div>
</div>
<div>
 <div>
  We present INTONEWS, a system to match online news articles with spoken news from a television newscasts represented by closed captions. We formalize the news matching problem as two independent tasks: closed captions segmentation and news retrieval. The system segments closed captions by using a windowing scheme: sliding or tumbling window. Next, it uses each segment to build a query by extracting representative terms. The query is used to retrieve previously indexed news articles from a search engine. To detect when a new article should be surfaced, the system compares the set of retrieved articles with the previously retrieved one. The intuition is that if the difference between these sets is large enough, it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user. In order to evaluate INTONEWS, we build a test collection using data coming from a second screen application and a major online news aggregator. The dataset is manually segmented and annotated by expert assessors, and used as our ground truth. It is freely available for download through the Webscope program. Our evaluation is based on a set of novel time-relevance metrics that take into account three different aspects of the problem at hand: precision, timeliness and coverage. We compare our algorithms against the best method previously proposed in literature for this problem. Experiments show the trade-offs involved among precision, timeliness and coverage of the airing news. Our best method is four times more accurate than the baseline.
 </div>
</div>

<div>
 We present INTONEWS, a system to match online news articles with spoken news from a television newscasts represented by closed captions. We formalize the news matching problem as two independent tasks: closed captions segmentation and news retrieval. The system segments closed captions by using a windowing scheme: sliding or tumbling window. Next, it uses each segment to build a query by extracting representative terms. The query is used to retrieve previously indexed news articles from a search engine. To detect when a new article should be surfaced, the system compares the set of retrieved articles with the previously retrieved one. The intuition is that if the difference between these sets is large enough, it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user. In order to evaluate INTONEWS, we build a test collection using data coming from a second screen application and a major online news aggregator. The dataset is manually segmented and annotated by expert assessors, and used as our ground truth. It is freely available for download through the Webscope program. Our evaluation is based on a set of novel time-relevance metrics that take into account three different aspects of the problem at hand: precision, timeliness and coverage. We compare our algorithms against the best method previously proposed in literature for this problem. Experiments show the trade-offs involved among precision, timeliness and coverage of the airing news. Our best method is four times more accurate than the baseline.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1-s2.0-S0306457314000703-main.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1-s2.0-S0306457314000703-main.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1-s2.0-S0306457314000703-main.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
INTONEWS: Online news retrieval using closed captions
Information Processing & Management
[u'Roi Blanco', u'Gianmarco De Francisci Morales', u'Fabrizio Silvestri']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6730/cross-domain-matching-squared-loss-mutual-information
found
<h6>
 Abstract
</h6>

<p class="leading">
 The goal of cross-domain matching (CDM) is to find correspondences between two sets of objects in different domains in an unsupervised way. CDM has various interesting applications, including photo album summarization where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system, and temporal alignment which aligns sequences such as videos that are potentially expressed using different features. In this paper, we propose an information theoretic CDM framework based on squared-loss mutual information (SMI). The proposed approach can directly handle nonlinearly related objects/sequences with different dimensions, with the ability that hyper-parameters can be objectively optimized by cross-validation. We apply the proposed method to several real-world problems including image matching, unpaired voice conversion, photo album summarization, cross-feature video and cross-domain video-to-mocap alignment, and Kinect-based action recognition, and experimentally demonstrate that the proposed method is a promising alternative to state-of-the-art CDM methods.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TPAMI_CDM.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TPAMI_CDM.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TPAMI_CDM.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Cross-Domain Matching with Squared-Loss Mutual Information
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
[u'Makoto Yamada', u'Yi Chang', u'Leonid Sigal', u'Michalis Raptis', u'Machiko Toyoda', u'Masashi Sugiyama']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6726/ups-and-downs-buzzes-life-cycle-modeling-temporal-pattern-discovery
found
<h6>
 Abstract
</h6>

<p class="leading">
 In social media analysis, one critical task is detecting burst of topics or buzz, which is reflected by extremely frequent mentions of certain key words in a short time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task because a buzz time-series usually exhibits sudden spikes andheavy tails, which fails most existing time-series models. To deal with buzz time-series sequences, we propose a novel time-series modeling approach which captures the rise and fade of temporal patterns via Product Life Cycle (PLC) models, a classical concept in economics. More specifically, we propose a mixture of PLC models to capture the multiple peaks in buzz time-series and furthermore develop a probabilistic graphical model (K-MPLC) to automatically discover inherent life cycle patterns within a collection of buzzes. Our experiment results show that our proposed method significantly outperforms existing state-of-the-art approaches on buzz clustering.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICDM2015_shortpaper.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICDM2015_shortpaper.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICDM2015_shortpaper.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Ups and Downs in Buzzes: Life Cycle Modeling for Temporal Pattern Discovery
ICDM 2014
[u'Yi Chang', u'Makoto Yamada', u'Antonio Ortega', u'Yan Liu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=8
found
 LINK 
https://labs.yahoo.com/publications/6729/modeling-adoptions-and-stages-diffusion-innovations
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We study the data mining problem of modeling adoptions and the stages of the diffusion of an innovation. For our aim we propose a stochastic model which decomposes a diffusion trace (sequence of adoptions) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages.

An empirical evaluation on synthetic and real-world adoption logs shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We study the data mining problem of modeling adoptions and the stages of the diffusion of an innovation. For our aim we propose a stochastic model which decomposes a diffusion trace (sequence of adoptions) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages.

An empirical evaluation on synthetic and real-world adoption logs shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future
  </div>
 </div>
</div>
<div>
 <div>
  We study the data mining problem of modeling adoptions and the stages of the diffusion of an innovation. For our aim we propose a stochastic model which decomposes a diffusion trace (sequence of adoptions) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages.

An empirical evaluation on synthetic and real-world adoption logs shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future
 </div>
</div>

<div>
 We study the data mining problem of modeling adoptions and the stages of the diffusion of an innovation. For our aim we propose a stochastic model which decomposes a diffusion trace (sequence of adoptions) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages.

An empirical evaluation on synthetic and real-world adoption logs shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper8.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper8.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper8.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Modeling Adoptions and the Stages of the Diffusion of Innovations
IEEE International Conference on Data Mining
[u'Yasir Mehmood', u'Nicola Barbieri', u'Francesco Bonchi']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6705/gsp-general-independent-click-through-rates
found
<h6>
 Abstract
</h6>

<p class="leading">
 The popular generalized second price (GSP) auction for sponsored search isbuilt upon a separable model of click-through-rates that decomposes the likelihood of a click into the product of a &quot;slot effect&quot; and an &quot;advertiser effect&quot; --- if the first slot is twice as good as the second for some bidder, then it is twice as good for everyone. Though appealing in its simplicity, this model is quite suspect in practice. A wide variety of factors including externalities and budgets have been studied that can and do cause it to be violated.In this paper we adopt a view of GSP as an iterated second price auction (see, e.g., Milgrom 2010) and study how the most basic violation of separability --- position dependent, arbitrary public click-through-rates that do not decompose---affects results from the foundational analysis of GSP [Varian 2007, Edelman et al. 2007]. For the two-slot setting we prove that for arbitrary click-through-rates, for arbitrary bidder values, an efficient pure-strategy equilibrium always exists; however, without separability there always exist values such that the VCG outcome and payments cannotbe realized by any bids, in equilibrium or otherwise. The separability assumption is therefore necessary in the two-slot case to match the payments of VCG but not for efficiency. We moreover show that without separability, generic existence of efficient equilibria is sensitive to the choice of tie-breaking rule, and when there are more than two slots, no (bid-independent) tie-breaking rule yields the positive result. In light of this we suggest alternative mechanisms that trade the simplicity of GSP for better equilibrium properties when there are three or more slots.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cw-wine14-long.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cw-wine14-long.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cw-wine14-long.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
GSP with General Independent Click-Through-Rates
10th Conference on Web and Internet Economics, WINE 2014
[u'Ruggiero Cavallo', u'Chris Wilkens']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6711/hidden-conditional-random-fields-distributed-user-embeddings-ad-targeting
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  <style type="text/css">
   <!--
p, li {white-space:pre-wrap;}
-->
  </style>
 </p>
 <p>
  Estimating a user's propensity to click on a display ad or purchase a particular item is a critical task in targeted advertising, a burgeoning online industry worth billions of dollars. Better and more accurate estimation methods result in improved online user experience, as only relevant and interesting ads are shown, and may also lead to large benefits for advertisers, as targeted users are more likely to click or make a purchase. In this paper we address this important problem, and propose an approach for improved estimation of ad click or conversion probability based on a sequence of user's online actions, modeled using Hidden Conditional Random Fields (HCRF) model. In addition, in order to address the sparsity issue at the input side of the HCRF model, we propose to learn distributed, low-dimensional representations of user actions through a directed skip-gram, a neural architecture suitable for sequential data. Experimental results on a real-world data set comprising thousands of user sessions collected at Yahoo servers clearly indicate the benefits and the potential of the proposed approach, which outperformed competing state-of-the-art algorithms and obtained significant improvements in terms of retrieval measures.
 </p>
</p>

<p>
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>
<p>
 Estimating a user's propensity to click on a display ad or purchase a particular item is a critical task in targeted advertising, a burgeoning online industry worth billions of dollars. Better and more accurate estimation methods result in improved online user experience, as only relevant and interesting ads are shown, and may also lead to large benefits for advertisers, as targeted users are more likely to click or make a purchase. In this paper we address this important problem, and propose an approach for improved estimation of ad click or conversion probability based on a sequence of user's online actions, modeled using Hidden Conditional Random Fields (HCRF) model. In addition, in order to address the sparsity issue at the input side of the HCRF model, we propose to learn distributed, low-dimensional representations of user actions through a directed skip-gram, a neural architecture suitable for sequential data. Experimental results on a real-world data set comprising thousands of user sessions collected at Yahoo servers clearly indicate the benefits and the potential of the proposed approach, which outperformed competing state-of-the-art algorithms and obtained significant improvements in terms of retrieval measures.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014icdm.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014icdm.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014icdm.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Hidden Conditional Random Fields with Distributed User Embeddings for Ad Targeting
IEEE International Conference on Data Mining (ICDM)
[u'Nemanja Djuric', u'Vladan Radosavljevic', u'Mihajlo Grbovic', u'Narayan Bhamidipati']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6728/are-you-still-tuning-hyperparameters-parameter-free-model-selection-and-learning
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm estimates over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function.
 </p>
</p>

<p>
 Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm estimates over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_workshop.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_workshop.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_workshop.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Are You Still Tuning Hyperparameters? Parameter-free Model Selection and Learning
Workshop &quot;Modern Nonparametrics 3: Automating the Learning Pipeline&quot; at NIPS 2014
[u'Francesco Orabona']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6724/large-scale-canonical-correlation-analysis-iterative-least-squares
found
<h6>
 Abstract
</h6>

<p class="leading">
 Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA , a iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fast_cca2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fast_cca2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fast_cca2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Large Scale Canonical Correlation Analysis with Iterative Least Squares
NIPS
[u'Dean Foster', u'Yichao Lu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6710/scalable-nonlinear-learning-adaptive-polynomial-expansions
found
<h6>
 Abstract
</h6>

<p class="leading">
 Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its
computation/prediction tradeoff ability compares very favorably against strong baselines.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ABHLT-141.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ABHLT-141.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ABHLT-141.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Scalable Nonlinear Learning with Adaptive Polynomial Expansions
Neural Information Processing Systems (NIPS)
[u'Alina Beygelzimer', u'Alekh Agarwal', u'Daniel Hsu', u'John Langford', u'Matus Telgarsky']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6974/simultaneous-model-selection-and-optimization-through-parameter-free-stochastic
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.
 </p>
</p>

<p>
 Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_camera_ready2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_camera_ready2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_camera_ready2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning
Advances in Neural Information Processing Systems (NIPS 2015)
[u'Francesco Orabona']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6706/actively-secure-private-function-evaluation
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We propose the first general framework for designing actively secure private function evaluation (PFE), not based on universal circuits. Our framework is naturally divided into pre-processing and online stages and can be instantiated using any generic actively secure multiparty computation (MPC) protocol. Our framework helps address the main open questions about efficiency of actively secure PFE. On the theoretical side, our framework yields the first actively secure PFE with linear complexity in the circuit size. On the practical side, we obtain the first actively secure PFE for arithmetic circuits with $O(g \cdot \log g)$ complexity where $g$ is the circuit size. The best previous construction (of practical interest) is based on an arithmetic universal circuit and has complexity $O(g^5)$. We also introduce the first linear Zero-Knowledge proof of correctness of &quot;extended permutation&quot; of ciphertexts (a generalization of ZK proof of correct shuffles) which maybe of independent interest.
 </p>
</p>

<p>
 We propose the first general framework for designing actively secure private function evaluation (PFE), not based on universal circuits. Our framework is naturally divided into pre-processing and online stages and can be instantiated using any generic actively secure multiparty computation (MPC) protocol. Our framework helps address the main open questions about efficiency of actively secure PFE. On the theoretical side, our framework yields the first actively secure PFE with linear complexity in the circuit size. On the practical side, we obtain the first actively secure PFE for arithmetic circuits with $O(g \cdot \log g)$ complexity where $g$ is the circuit size. The best previous construction (of practical interest) is based on an arithmetic universal circuit and has complexity $O(g^5)$. We also introduce the first linear Zero-Knowledge proof of correctness of &quot;extended permutation&quot; of ciphertexts (a generalization of ZK proof of correct shuffles) which maybe of independent interest.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main4.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main4.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main4.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Actively Secure Private Function Evaluation
Advances in Cryptology--ASIACRYPT 2014
[u'Payman Mohassel', u'Saeed Sadeghian', u'Nigel Smart']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6811/how-big-data-changing-user-engagement
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the online world, user engagement refers to the quality of the user experience that emphasizes the phenomena associated with wanting to use an application longer and frequently. This talk looks at the role of Big Data in measuring user engagement. It does so through two case studies on using absence time, within sessions and across sessions.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
How Big Data is Changing User Engagement

[u'Mounia Lalmas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6810/measuring-user-engagement-do-do-not-do-and-we-do-not-know
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the online world, user engagement refers to the quality of the user experience that emphasises the phenomena associated with wanting to use an application longer and frequently. User engagement is a multifaceted, complex phenomenon; this gives rise to a number of measurement approaches. Common ways to evaluate user engagement include self-report measures, e.g., questionnaires; physiological methods, e.g. cursor and eye tracking; and web analytics, e.g., number of site visits, click depth. These methods represent various trade-off in terms of the setting (laboratory versus in the wild), object of measurement (user behaviour, affect or cognition) and scale of data collected. This talk will present various efforts aiming at combining approaches to measure engagement. A particular focus will be what these measures individually and combined can tell us and not tell about user engagement. The talk will use examples of studies on news sites, social media, and native advertising.

[slideshare id=41493766&doc=berlin-141113005607-conversion-gate01]
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Measuring User Engagement: The Do, The Do Not Do, and The We Do Not Know

[u'Mounia Lalmas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=9
found
 LINK 
https://labs.yahoo.com/publications/6810/measuring-user-engagement-do-do-not-do-and-we-do-not-know
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the online world, user engagement refers to the quality of the user experience that emphasises the phenomena associated with wanting to use an application longer and frequently. User engagement is a multifaceted, complex phenomenon; this gives rise to a number of measurement approaches. Common ways to evaluate user engagement include self-report measures, e.g., questionnaires; physiological methods, e.g. cursor and eye tracking; and web analytics, e.g., number of site visits, click depth. These methods represent various trade-off in terms of the setting (laboratory versus in the wild), object of measurement (user behaviour, affect or cognition) and scale of data collected. This talk will present various efforts aiming at combining approaches to measure engagement. A particular focus will be what these measures individually and combined can tell us and not tell about user engagement. The talk will use examples of studies on news sites, social media, and native advertising.

[slideshare id=41493766&doc=berlin-141113005607-conversion-gate01]
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Measuring User Engagement: The Do, The Do Not Do, and The We Do Not Know

[u'Mounia Lalmas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5665/if-it-funny-it-mean-understanding-social-perceptions-yelp-online-reviews
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
  </div>
 </div>
</div>
<div>
 <div>
  Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
 </div>
</div>

<div>
 Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
If It Is Funny, It Is Mean Understanding Social Perceptions of Yelp Online Reviews
ACM GROUP 2014
[u'Saeideh Bakhshi', u'Partha Kanuparthy', u'David Ayman Shamma']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6719/influence-indirect-ties-social-network-dynamics
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    While direct social ties have been intensely studied in the context of computer-mediated social networks, indirect ties (e.g., friends of friends) have seen less attention. Yet in real life, we often rely on friends of our friends for recommendations (of doctors, schools, or babysitters), for introduction to a new job opportunity, and for many other occasional needs. In this work we empirically study the predictive power of indirect ties in two dynamic processes in social networks: new link formation and information diffusion. We not only verify the predictive power of indirect ties in new link formation but also show that this power is effective over longer social distance. Moreover, we show that the strength of an indirect tie positively correlates to the speed of forming a new link between the two end users of the indirect tie. Finally, we show that the strength of indirect ties can serve as a predictor for diffusion paths in social networks.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   While direct social ties have been intensely studied in the context of computer-mediated social networks, indirect ties (e.g., friends of friends) have seen less attention. Yet in real life, we often rely on friends of our friends for recommendations (of doctors, schools, or babysitters), for introduction to a new job opportunity, and for many other occasional needs. In this work we empirically study the predictive power of indirect ties in two dynamic processes in social networks: new link formation and information diffusion. We not only verify the predictive power of indirect ties in new link formation but also show that this power is effective over longer social distance. Moreover, we show that the strength of an indirect tie positively correlates to the speed of forming a new link between the two end users of the indirect tie. Finally, we show that the strength of indirect ties can serve as a predictor for diffusion paths in social networks.
  </div>
 </div>
</div>
<div>
 <div>
  While direct social ties have been intensely studied in the context of computer-mediated social networks, indirect ties (e.g., friends of friends) have seen less attention. Yet in real life, we often rely on friends of our friends for recommendations (of doctors, schools, or babysitters), for introduction to a new job opportunity, and for many other occasional needs. In this work we empirically study the predictive power of indirect ties in two dynamic processes in social networks: new link formation and information diffusion. We not only verify the predictive power of indirect ties in new link formation but also show that this power is effective over longer social distance. Moreover, we show that the strength of an indirect tie positively correlates to the speed of forming a new link between the two end users of the indirect tie. Finally, we show that the strength of indirect ties can serve as a predictor for diffusion paths in social networks.
 </div>
</div>

<div>
 While direct social ties have been intensely studied in the context of computer-mediated social networks, indirect ties (e.g., friends of friends) have seen less attention. Yet in real life, we often rely on friends of our friends for recommendations (of doctors, schools, or babysitters), for introduction to a new job opportunity, and for many other occasional needs. In this work we empirically study the predictive power of indirect ties in two dynamic processes in social networks: new link formation and information diffusion. We not only verify the predictive power of indirect ties in new link formation but also show that this power is effective over longer social distance. Moreover, we show that the strength of an indirect tie positively correlates to the speed of forming a new link between the two end users of the indirect tie. Finally, we show that the strength of indirect ties can serve as a predictor for diffusion paths in social networks.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/InfluenceOfIndirectTies_SocInfo2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/InfluenceOfIndirectTies_SocInfo2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/InfluenceOfIndirectTies_SocInfo2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Influence of Indirect Ties on Social Network Dynamics
6th International Conference on Social Informatics
[u'Nicolas Kourtellis', u'Xiangzuo', u'Jeremy Blackburn', u'John Skvoretz', u'Adriana Iamnitchi']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6973/fluxflow-visual-analysis-anomalous-information-spreading-social-media
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present FluxFlow, an interactive visual analysis system for revealing and analyzing anomalous information spreading in social media. Everyday, millions of messages are created, commented, and shared by people on social media websites, such as Twitter and Facebook. This provides valuable data for researchers and practitioners in many application domains, such as marketing, to inform decision-making. Distilling valuable social signals from the huge crowd's messages, however, is challenging, due to the heterogeneous and dynamic crowd behaviors. The challenge is rooted in data analysts' capability of discerning the anomalous information behaviors, such as the spreading of rumors or misinformation, from the rest that are more conventional patterns, such as popular topics and newsworthy events, in a timely fashion. FluxFlow incorporates advanced machine learning algorithms to detect anomalies, and offers a set of novel visualization designs for presenting the detected threads for deeper analysis. We evaluated FluxFlow with real datasets containing the Twitter feeds captured during significant events such as Hurricane Sandy. Through quantitative measurements of the algorithmic performance and qualitative interviews with domain experts, the results show that the back-end anomaly detection model is effective in identifying anomalous retweeting threads, and its front-end interactive visualizations are intuitive and useful for analysts to discover insights in data and comprehend the underlying analytical model.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fluxflow.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fluxflow.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fluxflow.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
#FluxFlow: Visual Analysis of Anomalous Information Spreading on Social Media
IEEE Transactions on Visualization and Computer Graphics (Proceedings of VAST 2014)
[u'Jian Zhao', u'Nan Cao', u'Zhen Wen', u'Yale Song', u'Yu Ru Lin', u'Christopher Collins']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6725/feasibility-predicting-news-popularity-cold-start
found
<h6>
 Abstract
</h6>

<p class="leading">
 We perform a study on cold-start news popularity prediction using a collection of 13,319 news articles obtained from Yahoo News. We characterise the online popularity of news articles by two different metrics and try to predict them using machine learning techniques. Contrary to a prior work on the same topic, our findings indicate that predicting the news popularity at cold start is a difficult task and the previously published results may be superficial.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper5.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper5.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper5.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On the Feasibility of Predicting News Popularity at Cold Start
In Proceedings of the 6th International Conference on Social Informatics (SocInfo)
[u'Ioannis Arapakis', u'B. Barla Cambazoglu', u'Mounia Lalmas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6690/if-it-funny-it-mean-understanding-social-perceptions-yelp-online-reviews
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
  </div>
 </div>
</div>
<div>
 <div>
  Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
 </div>
</div>

<div>
 Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review communities have their own dynamics behind the social interactions therein. In this work, we study the Yelp review votes of useful, funny, and/or cool to understand these social perceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the communitys perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230K Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand- ing of perceptions of social signals and have implications for design of recommendation systems.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
If It Is Funny, It Is Mean Understanding Social Perceptions of Yelp Online Reviews
ACM GROUP 2014
[u'Saeideh Bakhshi', u'Partha Kanuparthy', u'David Ayman Shamma']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6761/placing-task-large-scale-geo-estimation-challenge-social-media-videos-and-images
found
<h6>
 Abstract
</h6>

<p class="leading">
 The Placing Task is a yearly challenge offered by the MediaEval Multimedia Benchmarking Initiative that requires participants to develop algorithms that automatically predict the geo-location of social media videos and images. We introduce a recent development of a new standardized web-scale geo-tagged dataset for Placing Task 2014, which contains 5.5 million photos and 35,000 videos. This standardized benchmark with a large persistent dataset allows the research community to easily evaluate new algorithms and to analyze their performance with respect to the state-of-the-art approaches. We discuss the characteristics of this year's Placing Task along with the description of the new dataset components and how they were collected.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/geomm15i-choi.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/geomm15i-choi.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/geomm15i-choi.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Placing Task: a large-scale geo-estimation challenge for social-media videos and images
ACM International Workshop on Geotagging and Its Applications in Multimedia
[u'Bart Thomee', u'Jaeyoung Choi', u'Gerald Friedland', u'Liangliang Cao', u'Karl Ni', u'Damian Borth', u'Benjamin Elizalde', u'Luke Gottlieb', u'Carmen Carrano', u'Robert Pearce', u'Douglas Poland']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6809/geography-online-news-engagement
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Geographical processes might well impact online engagement in big countries like the USA. Upon a random sample of 200K news articles and corresponding 41M comments posted on the Yahoo News in that country, we show that nearby individuals tend to comment and engage with similar news articles more than distant individuals do. Interestingly, at state level, topics one reads about are associated with specific socio-economic conditions and personality traits.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Geographical processes might well impact online engagement in big countries like the USA. Upon a random sample of 200K news articles and corresponding 41M comments posted on the Yahoo News in that country, we show that nearby individuals tend to comment and engage with similar news articles more than distant individuals do. Interestingly, at state level, topics one reads about are associated with specific socio-economic conditions and personality traits.
  </div>
 </div>
</div>
<div>
 <div>
  Geographical processes might well impact online engagement in big countries like the USA. Upon a random sample of 200K news articles and corresponding 41M comments posted on the Yahoo News in that country, we show that nearby individuals tend to comment and engage with similar news articles more than distant individuals do. Interestingly, at state level, topics one reads about are associated with specific socio-economic conditions and personality traits.
 </div>
</div>

<div>
 Geographical processes might well impact online engagement in big countries like the USA. Upon a random sample of 200K news articles and corresponding 41M comments posted on the Yahoo News in that country, we show that nearby individuals tend to comment and engage with similar news articles more than distant individuals do. Interestingly, at state level, topics one reads about are associated with specific socio-economic conditions and personality traits.
</div>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Geography of Online News Engagement

[u'Amin Mantrach']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6702/focused-crawling-structured-data
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    The Web is rapidly transforming from a pure document collection to the largest connected public data space. Semantic annotations of web pages make it notably easier to extract and reuse data and are increasingly used by both search engines and social media sites to provide better search experiences through rich snippets, faceted search, task completion, etc. In our work, we study the novel problem of crawling structured data embedded inside HTML pages. We describe Anthelion, the first focused crawler addressing this task. We propose new methods of focused crawling specifically designed for collecting data-rich pages with greater efficiency. In particular, we propose a novel combination of online learning and bandit-based explore/exploit approaches to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages. We show that these techniques significantly outperform state-of-the-art approaches for focused crawling, measured as the ratio of relevant pages and non-relevant pages collected within a given budget.
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   The Web is rapidly transforming from a pure document collection to the largest connected public data space. Semantic annotations of web pages make it notably easier to extract and reuse data and are increasingly used by both search engines and social media sites to provide better search experiences through rich snippets, faceted search, task completion, etc. In our work, we study the novel problem of crawling structured data embedded inside HTML pages. We describe Anthelion, the first focused crawler addressing this task. We propose new methods of focused crawling specifically designed for collecting data-rich pages with greater efficiency. In particular, we propose a novel combination of online learning and bandit-based explore/exploit approaches to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages. We show that these techniques significantly outperform state-of-the-art approaches for focused crawling, measured as the ratio of relevant pages and non-relevant pages collected within a given budget.
  </div>
 </div>
</div>
<div>
 <div>
  The Web is rapidly transforming from a pure document collection to the largest connected public data space. Semantic annotations of web pages make it notably easier to extract and reuse data and are increasingly used by both search engines and social media sites to provide better search experiences through rich snippets, faceted search, task completion, etc. In our work, we study the novel problem of crawling structured data embedded inside HTML pages. We describe Anthelion, the first focused crawler addressing this task. We propose new methods of focused crawling specifically designed for collecting data-rich pages with greater efficiency. In particular, we propose a novel combination of online learning and bandit-based explore/exploit approaches to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages. We show that these techniques significantly outperform state-of-the-art approaches for focused crawling, measured as the ratio of relevant pages and non-relevant pages collected within a given budget.
 </div>
</div>

<div>
 The Web is rapidly transforming from a pure document collection to the largest connected public data space. Semantic annotations of web pages make it notably easier to extract and reuse data and are increasingly used by both search engines and social media sites to provide better search experiences through rich snippets, faceted search, task completion, etc. In our work, we study the novel problem of crawling structured data embedded inside HTML pages. We describe Anthelion, the first focused crawler addressing this task. We propose new methods of focused crawling specifically designed for collecting data-rich pages with greater efficiency. In particular, we propose a novel combination of online learning and bandit-based explore/exploit approaches to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages. We show that these techniques significantly outperform state-of-the-art approaches for focused crawling, measured as the ratio of relevant pages and non-relevant pages collected within a given budget.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anthelion.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anthelion.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anthelion.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Focused Crawling for Structured Data
CIKM 2014
[u'Robert Meusel', u'Peter Mika', u'Roi Blanco']
Web Search and Data Mining
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6712/building-decision-trees-large-scale-data-applications-line-advertising
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
  </div>
 </div>
</div>
<div>
 <div>
  Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
 </div>
</div>

<div>
 Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On Building Decision Trees from Large-scale Data in Applications of On-line Advertising
CIKM 2014
[u'Durga Deepthi Singh', u'Ravi Kant']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=10
found
 LINK 
https://labs.yahoo.com/publications/6701/time-aware-rank-aggregation-microblog-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 We tackle the problem of searching microblog posts and frame it as a rank aggregation problem where we merge result lists generated by separate rankers so as to produce a final ranking to be returned to the user. We propose a rank aggregation method, TimeRA, that is able to infer the rank scores of documents via latent factor modeling. It is time-aware and rewards posts that are published in or near a burst of posts that are ranked highly in many of the lists being aggregated. Our experimental results show that it significantly outperforms state-of-the-art rank aggregation and time-sensitive microblog search algorithms.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm-2014-liang.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm-2014-liang.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm-2014-liang.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Time-aware Rank Aggregation for Microblog Search
CIKM 2014: 23rd ACM Conference on Information and Knowledge Management
[u'Edgar Meij', u'Shangsong Liang', u'Zhaochun Ren', u'Wouter Weerkamp', u'Maarten De Rijke']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6687/whos-time-it-anyway-investigating-accuracy-camera-timestamps
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  People take photos all over the world at all times of day; each photo depicting a place and a moment worth capturing. In the context of multimedia analysis and social computing, accurate location and time information about where and when these photos were taken is of importance for understanding event semantics, image content and many other purposes. While location information associated with photos is known to be relatively accurate, time is not. From a sample of 10 million public Flickr photos, we observe that 37% of the photos differ more than an hour between their camera timestamps and GPS timestamps with respect to local time at the locations where the photos were taken. Erroneous time information may adversely influence the correctness of any kind of temporal analysis that relies on camera timestamps, as well as research and real-world applications that require accurate knowledge of when and where photos were captured. In light of our observations we propose a simple yet effective metadata-only technique for improving the accuracy of camera timestamps.
 </p>
</p>

<p>
 People take photos all over the world at all times of day; each photo depicting a place and a moment worth capturing. In the context of multimedia analysis and social computing, accurate location and time information about where and when these photos were taken is of importance for understanding event semantics, image content and many other purposes. While location information associated with photos is known to be relatively accurate, time is not. From a sample of 10 million public Flickr photos, we observe that 37% of the photos differ more than an hour between their camera timestamps and GPS timestamps with respect to local time at the locations where the photos were taken. Erroneous time information may adversely influence the correctness of any kind of temporal analysis that relies on camera timestamps, as well as research and real-world applications that require accurate knowledge of when and where photos were captured. In light of our observations we propose a simple yet effective metadata-only technique for improving the accuracy of camera timestamps.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/time-correction.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/time-correction.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/time-correction.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Who's time is it anyway?: investigating the accuracy of camera timestamps
ACM International Conference on Multimedia
[u'Bart Thomee', u'David Ayman Shamma', u'Jose G Moreno']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6712/building-decision-trees-large-scale-data-applications-line-advertising
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
  </div>
 </div>
</div>
<div>
 <div>
  Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
 </div>
</div>

<div>
 Decision trees have been used for several decades as simple and effective solutions to supervised learning problems. Their success extends to tasks across a variety of areas. Yet, data collected today through web-domains such as online advertising presents many new challenges: sheer size, the prevalence of high-arity categorical features, unknown feature-values, cold starts, sparse training instances, and imbalance in the class labels. We argue that decision trees remain an ideal choice for applications of online advertising as they naturally construct higher-order conjunctive features; we then contribute two ideas to improve tree-building accordingly. First, to handle high-arity categorical features, we introduce a method to cluster feature-values based on their output responses. The result is more data-dense trees with relatively small branching factors. Second, we employ cross-validation as a principled approach to derive splitting and stopping criteria: thereby we identify splits that gen- eralize well, and also curb overfitting. Evaluated on three distinct probability-estimation tasks in on-line advertising, our method, CCDT, shows significant improvements in the accuracy of prediction.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/csdt.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On Building Decision Trees from Large-scale Data in Applications of On-line Advertising
CIKM 2014
[u'Durga Deepthi Singh', u'Ravi Kant']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6714/how-many-folders-do-you-really-need-classifying-email-handful-categories
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 Email classification is still a mostly manual task. Consequently, most Web mail users never define a single folder. Recently however, automatic classification offering the same categories to all users has started to appear in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather than previous (unsuccessful) personalized approaches because of the change in the nature of consumer email traffic, which is now dominated by (non-spam) machine-generated email. We propose here a novel approach for (1) automatically distinguishing between personal and machine-generated email and (2) classifying messages into latent categories, without requiring users to have defined any folder. We report how we have discovered that a set of 6 &quot;latent&quot; categories (one for human- and the others for machine-generated messages) can explain a significant portion of email traffic. We describe in details the steps involved in building a Web-scale email categorization system, from the collection of ground-truth labels, the selection of features to the training of models. Experimental evaluation was performed on more than 500 billion messages received during a period of six months by users of Yahoo mail service, who elected to be part of such research studies. Our system achieved precision and recall rates close to 90% and the latent categories we discovered were shown to cover 70% of both email traffic and email search queries. We believe that these results pave the way for a change of approach in the Web mail industry, and could support the invention of new large-scale email discovery paradigms that had not been possible before.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_km0481.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_km0481.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_km0481.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
How Many Folders Do You Really Need? Classifying Email into a Handful of Categories
CIKM 2014: 23rd ACM Conference on Information and Knowledge Management
[u'Mihajlo Grbovic', u'Guy Halawi', u'Zohar Karnin', u'Yoelle Maarek']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6716/improving-term-weighting-community-question-answering-search-using-syntactic
found
<h6>
 Abstract
</h6>

<p class="leading">
 Query term weighting is a fundamental task in information retrieval and most popular term weighting schemes are primarily based on statistical analysis of term occurrences within the document collection.

In this work we study how term weighting may benefit from syntactic analysis of the corpus.

Focusing on community question answering (CQA) sites, we take into account the syntactic function of the terms within CQA texts as an important factor affecting their relative importance for retrieval. We analyze a large log of web queries that landed on the Yahoo Answers site, showing a strong deviation between the tendencies of different document words to appear in a landing (click-through) query given their syntactic function. To this end, we propose a novel term weighting method that makes use of the syntactic information available for each query term occurrence in the document, on top of term occurrence statistics. The relative importance of each feature is learned via a learning to rank algorithm that utilizes a click-through query log. We examine the new weighting scheme using manual evaluation based on editorial data and using automatic evaluation over the query log. Our experimental results show consistent improvement in retrieval when syntactic information is taken into account.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ir01492.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ir01492.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ir01492.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Improving Term Weighting for Community Question Answering Search Using Syntactic Analysis
CIKM 2014
[u'David Carmel', u'Avihai Mejer', u'Yuval Pinter', u'Idan Szpektor']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6972/evasion-and-counter-evasion-study-malicious-websites-detection
found
<h6>
 Abstract
</h6>

<p class="leading">
 Malicious websites are a major cyber attack vector, and effective detection of them is an important cyber defense task. The main defense paradigm in this regard is that the defender uses some kind of machine learning algorithms to train a detection model, which is then used to classify websites in question. Unlike other settings, the following issue is inherent to the problem of malicious websites detection: the attacker essentially has access to the same data that the defender uses to train its detection models. This 'symmetry' can be exploited by the attacker, at least in principle, to evade the defender's detection models. In this paper, we present a framework for characterizing the evasion and counter-evasion interactions between the attacker and the defender, where the attacker attempts to evade the defender's detection models by taking advantage of this symmetry. Within this framework, we show that an adaptive attacker can make malicious websites evade powerful detection models, but proactive training can be an effective counter-evasion defense mechanism. The framework is geared toward the popular detection model of decision tree, but can be adapted to accommodate other classifiers.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cns.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cns.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cns.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
An Evasion and Counter-evasion Study in Malicious Websites Detection
IEEE Conference on Communications and Network Security
[u'Li Xu', u'Zhenxin Zhan', u'Shouhuai Xu', u'Keyin Ye']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6812/web-scale-semantic-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 Most web search engine users are increasingly expecting direct and contextually relevant answers to their information needs rather than mere links to documents. In order to arrive at such answers we need to tackle several issues, including (but not limited to) entity linking, entity retrieval, entity reconciliation, intent classification, and personalization, all without losing sight of efficiency. In this talk I will give some background on how such an end-to-end pipeline for semantic search is being implemented and improved at Yahoo.

[slideshare id=41711465&doc=20141031ssa-webscalesemanticsearch-141118105213-conversion-gate02]
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Web-scale Semantic Search

[u'Edgar Meij']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6691/user-interests-imbalance-exploration-social-recommendation-fitness-adaptation
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Recent years have witnessed an increasing interest in how to incorporate social network information into recommendation algorithms to enhance the user experience. In this paper, we find the phenomenon that users in the contexts of recommendation system and social network do not share the same interest space. Based on this finding, we proposed the social regulatory factor regression model (SRFRM) which could connect different interest spaces in different contexts together in an unified latent factor model. Specifically, different from the traditional social based latent factor models with strong limitation that all sides share the same feature space, the proposed method leverages the regulatory factor number on both sides to meet the fact that users and items or users in different contexts may not share the same interest space. It works by incorporating two linear transformation matrices into the matrix co-factorization framework that matrix factorization of user ratings is regularized by that of social trust network. We study a large subsets of data from epinions.com and douban.com respectively. The experimental results indicate that users in different contexts have different interest spaces and our model achieves a higher performance for both error metrics and ranking metrics compared with related state-of-the-art methods.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Recent years have witnessed an increasing interest in how to incorporate social network information into recommendation algorithms to enhance the user experience. In this paper, we find the phenomenon that users in the contexts of recommendation system and social network do not share the same interest space. Based on this finding, we proposed the social regulatory factor regression model (SRFRM) which could connect different interest spaces in different contexts together in an unified latent factor model. Specifically, different from the traditional social based latent factor models with strong limitation that all sides share the same feature space, the proposed method leverages the regulatory factor number on both sides to meet the fact that users and items or users in different contexts may not share the same interest space. It works by incorporating two linear transformation matrices into the matrix co-factorization framework that matrix factorization of user ratings is regularized by that of social trust network. We study a large subsets of data from epinions.com and douban.com respectively. The experimental results indicate that users in different contexts have different interest spaces and our model achieves a higher performance for both error metrics and ranking metrics compared with related state-of-the-art methods.
  </div>
 </div>
</div>
<div>
 <div>
  Recent years have witnessed an increasing interest in how to incorporate social network information into recommendation algorithms to enhance the user experience. In this paper, we find the phenomenon that users in the contexts of recommendation system and social network do not share the same interest space. Based on this finding, we proposed the social regulatory factor regression model (SRFRM) which could connect different interest spaces in different contexts together in an unified latent factor model. Specifically, different from the traditional social based latent factor models with strong limitation that all sides share the same feature space, the proposed method leverages the regulatory factor number on both sides to meet the fact that users and items or users in different contexts may not share the same interest space. It works by incorporating two linear transformation matrices into the matrix co-factorization framework that matrix factorization of user ratings is regularized by that of social trust network. We study a large subsets of data from epinions.com and douban.com respectively. The experimental results indicate that users in different contexts have different interest spaces and our model achieves a higher performance for both error metrics and ranking metrics compared with related state-of-the-art methods.
 </div>
</div>

<div>
 Recent years have witnessed an increasing interest in how to incorporate social network information into recommendation algorithms to enhance the user experience. In this paper, we find the phenomenon that users in the contexts of recommendation system and social network do not share the same interest space. Based on this finding, we proposed the social regulatory factor regression model (SRFRM) which could connect different interest spaces in different contexts together in an unified latent factor model. Specifically, different from the traditional social based latent factor models with strong limitation that all sides share the same feature space, the proposed method leverages the regulatory factor number on both sides to meet the fact that users and items or users in different contexts may not share the same interest space. It works by incorporating two linear transformation matrices into the matrix co-factorization framework that matrix factorization of user ratings is regularized by that of social trust network. We study a large subsets of data from epinions.com and douban.com respectively. The experimental results indicate that users in different contexts have different interest spaces and our model achieves a higher performance for both error metrics and ranking metrics compared with related state-of-the-art methods.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/final_20141110.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/final_20141110.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/final_20141110.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
User Interests Imbalance Exploration in Social Recommendation: A Fitness Adaptation
Proceedings of the 22nd ACM international conference on Conference on information & knowledge management
[u'Xuetao Ding', u'Tianchun Wang', u'Xiaoming Jin', u'Xiaojun Ye']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6715/visual-similarity-based-interactive-product-recommendation-online-shopping
found
<h6>
 Abstract
</h6>

<p class="leading">
 With the rapid development of e-commerce and explosivegrowth of online shopping market, the problem of how tooptimize the process of guiding the user to the huge amountof online products has been urgent. Existing recommendersystems use information from users' profiles (demographicfiltering), similar neighbors (collaborative filtering), andtextual description (content-based model) to makerecommendations, which easily generate irrelevantsuggestions to users due to the ignorance of users' intentionsand the visual similarity among products. In this paper, weproposed an interactive product recommendation method,which considers not only the product diversity but also thevisual similarity, to interactively capture a user's realintention and refine the product recommendation resultbased on the user's real product interests. Our algorithm isexperimentally evaluated under a real-world user log, and
shown to significantly improve recommendation accuracyover the traditional approaches.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/15699111691.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/15699111691.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/15699111691.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On Visual Similarity Based Interactive Product Recommendation for Online Shopping
IEEE International Conference of Image Processing 2014
[u'Jia Li', u'Jenhao Hsiao']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6713/distributed-adaptive-model-rules-mining-big-data-streams
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.
  </div>
 </div>
</div>
<div>
 <div>
  Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.
 </div>
</div>

<div>
 Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/amrules.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/amrules.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/amrules.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distributed Adaptive Model Rules for Mining Big Data Streams
BigData 14: 2014 IEEE International Conference on Big Data, Washington, 2014
[u'Gianmarco De Francisci Morales', u'Anh Thu Vu', u'Joao Gama', u'Albert Bifet']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=11
found
 LINK 
https://labs.yahoo.com/publications/5666/knowledge-graph-and-corpus-driven-segmentation-and-answer-inference-telegraphic
found
<h6>
 Abstract
</h6>

<p class="leading">
 Much recent work focuses on formal interpretation of natural question utterances, withthe goal of executing the resulting structured queries on knowledge graphs (KGs) suchas Freebase. Here we address two limitations of this approach when applied to open-domain, entity-oriented Web queries. First,Web queries are rarely well-formed questions.They are telegraphic, with missing verbs,prepositions, clauses, case and phrase clues.Second, the KG is always incomplete, unableto directly answer many queries. We proposea novel technique to segment a telegraphicquery and assign a coarse-grained purpose toeach segment: a base entity e1 , a relation typer, a target entity type t2 , and contextual wordss. The query seeks entity e2  t2 wherer(e1 , e2 ) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpuswhere mentions of entities have been linked tothe KG. We do not trust the best or any specific query segmentation. Instead, evidence infavor of candidate answer entities e2 are aggregated acrossseveral segmentations. Extensive experimentson the ClueWeb corpus and parts of Freebaseas our KG, using over a thousand telegraphicqueries adapted from TREC, INEX, and WebQuestions, show the efficacy of our approach.For one benchmark, MAP improves from 0.20.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.290.36 to0.54. Code and data will be made available.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries
EMNLP 2014
[u'Uma Sawant', u'Mandar Joshi', u'Soumen Chakrabarti']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6990/distributed-heuristic-forward-search-multi-agent-planning
found
<h6>
 Abstract
</h6>

<p class="leading">
 This paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal. Two main approaches have recently been proposed to solve this type of problem -- one is based on reduction to distributed constraint satisfaction, and the other on partial-order planning techniques. In classical single-agent planning, constraint-based and partial-order planning techniques are currently dominated by heuristic forward search. The question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacy-preserving classical multi-agent planning. Our work provides a positive answer to this question in the form of a general approach to distributed state-space search in which each agent performs only the part of the state expansion relevant to it. The resulting algorithms are simple and efficient -- outperforming previous algorithms by orders of magnitude -- while offering similar flexibility to that of forward-search algorithms for single-agent planning. Furthermore, one particular variant of our general approach yields a distributed version of the A* algorithm that is the first cost-optimal distributed algorithm for privacy-preserving planning.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Nissim_Brafman2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Nissim_Brafman2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Nissim_Brafman2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distributed Heuristic Forward Search for Multi-agent Planning
Journal of Artificial Intelligence Research
[u'Ronen Brafman']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6704/beyond-clicks-dwell-time-personalization
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  Many internet companies, such as Yahoo, Facebook, Google andTwitter, rely on content recommendation systems to deliver themost relevant content items to individual users through personalization. Delivering such personalized user experiences is believed toincrease the long term engagement of users. While there has beena lot of progress in designing effective personalized recommendersystems, by exploiting user interests and historical interaction datathrough implicit (item click) or explicit (item rating) feedback, directly optimizing for users satisfaction with the system remains challenging. In this paper, we explore the idea of using item-level dwell time as a proxy to quantify how likely a content item is relevant to a particular user. We describe a novel method to compute accurate dwell time based on client-side and server-side logging and demonstrate how to normalize dwell time across different devices and contexts. In addition, we describe our experiments in incorporating dwell time into state-of-the-art learning to rank techniques and collaborative filtering models that obtain competitive performances in both offline and online settings.
 </div>
</p>

<div>
 Many internet companies, such as Yahoo, Facebook, Google andTwitter, rely on content recommendation systems to deliver themost relevant content items to individual users through personalization. Delivering such personalized user experiences is believed toincrease the long term engagement of users. While there has beena lot of progress in designing effective personalized recommendersystems, by exploiting user interests and historical interaction datathrough implicit (item click) or explicit (item rating) feedback, directly optimizing for users satisfaction with the system remains challenging. In this paper, we explore the idea of using item-level dwell time as a proxy to quantify how likely a content item is relevant to a particular user. We describe a novel method to compute accurate dwell time based on client-side and server-side logging and demonstrate how to normalize dwell time across different devices and contexts. In addition, we describe our experiments in incorporating dwell time into state-of-the-art learning to rank techniques and collaborative filtering models that obtain competitive performances in both offline and online settings.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/recsys2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/recsys2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/recsys2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Beyond Clicks: Dwell Time for Personalization
ACM RecSys 2014
[u'Xing Yi', u'Liangjie Hong', u'Erheng Zhong', u'Nathan Liu', u'Suju Rajan']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6709/loupe-handheld-near-eye-display
found
<h6>
 Abstract
</h6>

<p class="leading">
 Loupe is a novel interactive device with a near-eye virtual display similar to head-up display glasses that retains a handheld form factor. We present our hardware implementation and discuss our user interface that leverages Loupe's unique combination of properties. In particular, we present our input capabilities, spatial metaphor, opportunities for using the round aspect of Loupe, and our use of focal depth. We demonstrate how those capabilities come together in an example application designed to allow quick access to information feeds.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper4.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper4.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper4.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Loupe: A Handheld Near-Eye Display
UIST
[u'Kent Lyons', u'Seung Wook Kim', u'Shigeyuki Seko', u'David H Nguyen', u'Audrey Dejardins', u'Melodie Vidal', u'David Dobbelstein', u'Jeremy Rubin']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6689/recommending-tumblr-blogs-follow-inductive-matrix-completion
found
<h6>
 Abstract
</h6>

<p class="leading">
 In microblogging sites, recommending blogs (users) to follow is one of the core tasks for enhancing user experience. In this paper, we propose a novel inductive matrix completion based blog recommendation method to effectively utilize multiple rich sources of evidence such as the social network and the content as well as the activity data from users and blogs. Experiments on a large-scale real-world dataset from Tumblr show the effectiveness of the proposed blog recommendation method.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014RecSys_ShinCetintasLee.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014RecSys_ShinCetintasLee.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014RecSys_ShinCetintasLee.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Recommending Tumblr Blogs to Follow with Inductive Matrix Completion
Proceedings of the 8th ACM Conference on Recommender systems
[u'Suleyman Cetintas', u'Kuang-chih Lee', u'Donghyuk Shin']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6707/item-cold-start-recommendations-learning-local-collective-embeddings
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Recommender systems suggest to users items that they might like (e.g., news articles, songs, movies) and, in doing so, they help users deal with information overload and enjoy a personalized experience. One of the main problems of these systems is the item cold-start, i.e., when a new item is introduced in the system and no past information is available, then no effective recommendations can be produced. The item cold-start is a very common problem in practice: modern online platforms have hundreds of new items published every day. To address this problem, we propose to learn Local Collective Embeddings: a matrix factorization that exploits items properties and past user preferences while enforcing the manifold structure exhibited by the collective embeddings. We present a learning algorithm based on multiplicative update rules that are efficient and easy to implement. The experimental results on two item cold-start use cases: news recommendation and email recipient recommendation, demonstrate the effectiveness of this approach and show that it significantly outperforms six state-of-the- art methods for item cold-start.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Recommender systems suggest to users items that they might like (e.g., news articles, songs, movies) and, in doing so, they help users deal with information overload and enjoy a personalized experience. One of the main problems of these systems is the item cold-start, i.e., when a new item is introduced in the system and no past information is available, then no effective recommendations can be produced. The item cold-start is a very common problem in practice: modern online platforms have hundreds of new items published every day. To address this problem, we propose to learn Local Collective Embeddings: a matrix factorization that exploits items properties and past user preferences while enforcing the manifold structure exhibited by the collective embeddings. We present a learning algorithm based on multiplicative update rules that are efficient and easy to implement. The experimental results on two item cold-start use cases: news recommendation and email recipient recommendation, demonstrate the effectiveness of this approach and show that it significantly outperforms six state-of-the- art methods for item cold-start.
  </div>
 </div>
</div>
<div>
 <div>
  Recommender systems suggest to users items that they might like (e.g., news articles, songs, movies) and, in doing so, they help users deal with information overload and enjoy a personalized experience. One of the main problems of these systems is the item cold-start, i.e., when a new item is introduced in the system and no past information is available, then no effective recommendations can be produced. The item cold-start is a very common problem in practice: modern online platforms have hundreds of new items published every day. To address this problem, we propose to learn Local Collective Embeddings: a matrix factorization that exploits items properties and past user preferences while enforcing the manifold structure exhibited by the collective embeddings. We present a learning algorithm based on multiplicative update rules that are efficient and easy to implement. The experimental results on two item cold-start use cases: news recommendation and email recipient recommendation, demonstrate the effectiveness of this approach and show that it significantly outperforms six state-of-the- art methods for item cold-start.
 </div>
</div>

<div>
 Recommender systems suggest to users items that they might like (e.g., news articles, songs, movies) and, in doing so, they help users deal with information overload and enjoy a personalized experience. One of the main problems of these systems is the item cold-start, i.e., when a new item is introduced in the system and no past information is available, then no effective recommendations can be produced. The item cold-start is a very common problem in practice: modern online platforms have hundreds of new items published every day. To address this problem, we propose to learn Local Collective Embeddings: a matrix factorization that exploits items properties and past user preferences while enforcing the manifold structure exhibited by the collective embeddings. We present a learning algorithm based on multiplicative update rules that are efficient and easy to implement. The experimental results on two item cold-start use cases: news recommendation and email recipient recommendation, demonstrate the effectiveness of this approach and show that it significantly outperforms six state-of-the- art methods for item cold-start.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p89.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p89.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p89.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Item Cold-Start Recommendations: Learning Local Collective Embeddings
RecSys 2014
[u'Amin Mantrach', u'Martin Saveski']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6762/benchmark-research-catalyst-charting-progress-geo-prediction-social-multimedia
found
<h6>
 Abstract
</h6>

<p class="leading">
 Benchmarks have the power to bring research communities together to focus on specific research challenges. They drive research forward by making it easier to systematically compare and contrast new solutions, and evaluate their performance with respect to the existing state of the art. In this chapter, we present a retrospective on the Placing Task, a yearly challenge offered by the MediaEval Multimedia Benchmark. The Placing Task, launched in 2010, is a benchmarking task that requires participants to develop algorithms that automatically predict the geo-location of social multimedia (videos and images). This chapter covers the editions of the Placing Task offered in 2010--2013, and also presents an outlook onto 2014. We present the formulation of the task and the task data set for each year, tracing the design decisions that were made by the organizers, and how each year built on the previous year. Finally, we provide a summary of future directions and challenges for multimodal geo-location, and concluding remarks on how benchmarking has catalyzed research progress in the research area of geo-location prediction for social multimedia.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Book-chapter-Placing.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Book-chapter-Placing.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Book-chapter-Placing.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The benchmark as a research catalyst: charting the progress of geo-prediction for social multimedia
Multimodal Location Estimation of Videos and Images
[u'Bart Thomee', u'Michele Trevisiol', u'Martha Larson', u'Pascal Kelm', u'Adam Rae', u'Claudia Hauff', u'Jaeyoung Choi', u'Olivier Van Laere', u'Steven Schockaert', u'Gareth Jones', u'Pavel Serdyukov', u'Vanessa Murdock', u'Gerald Friedland']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6963/situ-study-mobile-phone-notifications
found
<h6>
 Abstract
</h6>

<p class="leading">
 Notifications on mobile phones alert users about new messages, emails, social network updates, and other events. However, little is understood about the nature and effect of such notifications on the daily lives of mobile users. We report from a one-week, in-situ study involving 15 mobile phones users, where we collected real-world notifications through a smartphone logging application alongside subjective perceptions of those notifications through an online diary. We found that our participants had to deal with 63.5 notifications on average per day, mostly from messengers and email. Whether the phone is in silent mode or not, notifications were typically viewed within minutes. Social pressure in personal communication was amongst the main reasons given. While an increasing number of notifications was associated with an increase in negative emotions, receiving more messages and social network updates also made our participants feel more connected with others. Our findings imply that avoiding interruptions from notifications may be viable for professional communication, while in personal communication, approaches should focus on managing expectations.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MHCI14-Mobint.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MHCI14-Mobint.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MHCI14-Mobint.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
An In-Situ Study of Mobile Phone Notifications
Mobile HCI 2014
[u'Martin Pielot', u'Karen Church', u'Rodrigo De Oliveira']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6703/reviewcollage-mobile-interface-direct-comparison-using-online-reviews
found
<h6>
 Abstract
</h6>

<p class="leading">
 Review comments posted in online websites can help the user decide a product to purchase or place to visit. They can also be useful to closely compare a couple of candidate entities. However, the user may have to read different webpages back and forth for comparison, and this is not desirable particularly when she is using a mobile device. We present ReviewCollage, a mobile interface that aggregates information about two reviewed entities in a one-page view. ReviewCollage uses attribute-value pairs, known to be effective for review text summarization, and highlights the similarities and differences between the entities. Our user study confirms that ReviewCollage can support the user to compare two entities and make a decision within a couple of minutes, at least as quickly as existing summarization interfaces. It also reveals that ReviewCollage could be most useful when two entities are very similar.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MobileHCI2014-ReviewCollage-Final.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MobileHCI2014-ReviewCollage-Final.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MobileHCI2014-ReviewCollage-Final.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
ReviewCollage: A Mobile Interface for Direct Comparison Using Online Reviews
Proceedings of the 16th international conference on Human-computer interaction with mobile devices and services
[u'Haojian Jin', u'Tetsuya Sakai', u'Koji Yatani']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6695/do-we-need-annotation-experts-case-study-celiac-disease-classification
found
<h6>
 Abstract
</h6>

<p class="leading">
 Inference of clinically relevant findings from the visual appearance of images has become an essential part of processing pipelines for many problems in medical imaging. Typically, a sufficient amount labeled training data is assumed to be available, provided by domain experts. However, acquisition of this data is usually a time-consuming and expensive endeavor. In this work, we ask the question if, for certain problems, expert knowledge is actually required. In fact, we investigate the impact of letting non-expert volunteers annotate a database of endoscopy images which are then used to assess the absence/presence of celiac disease. Contrary to previous approaches, we are not interested in algorithms that can handle the label noise. Instead, we present compelling empirical evidence that label noise can be compensated by a sufficiently large corpus of training data, labeled by the non-experts.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/86740455-2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/86740455-2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/86740455-2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Do We Need Annotation Experts?  A Case Study in Celiac Disease Classification
Medical Image Computing and Computer Assisted Intervention, MICCAI 2014
[u'Nikhil Rasiwasia', u'Roland Kwitt', u'Sebastian Hegenbart', u'Andreas V Ecsei', u'Andreas Uhl']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=12
found
 LINK 
https://labs.yahoo.com/publications/6684/be-certain-how-mining-uncertain-data
found
<h6>
 Abstract
</h6>

<p class="leading">
 The purpose of this technical note is to introduce the problems of similarity detection and summarization in uncertain data. We provide the essential arguments that make the problems relevant to the data-mining and machine-learning community, stating major issues and summarizing our contributions in the field. Further challenges and directions of research are also issued.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/PDF-proof.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/PDF-proof.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/PDF-proof.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Be Certain of How-to Before Mining Uncertain Data
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD 14) (Nectar Track)
[u'Francesco Gullo', u'Giovanni Ponti', u'Andrea Tagarelli']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6694/architecture-innovation-tracking-face-face-interactions-ubicomp-technologies
found
<h6>
 Abstract
</h6>

<p class="leading">
 The layouts of the buildings we live in shape our everyday lives. In office environments, building spaces affect employees' communication, which is crucial for productivity and innovation. However, accurate measurement of how spatial layouts affect interactions is a major challenge and traditional techniques may not give an objective view. We measure the impact of building spaces on social interactions using wearable sensing devices. We study a single organization that moved between two different buildings, affording a unique opportunity to examine how space alone can affect interactions. The analysis is based on two large scale deployments of wireless sensing technologies: short-range, lightweight RFID tags capable of detecting face-to-face interactions. We analyze the traces to study the impact of the building change on social behavior, which represents a first example of using ubiquitous sensing technology to study how the physical design of two workplaces combines with organizational structure to shape contact patterns.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1406.6829.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1406.6829.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1406.6829.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Architecture of Innovation: Tracking Face-to-Face Interactions with UbiComp Technologies
ACM Ubicomp
[u'Daniele Quercia', u'Chloe Brown', u'Christos Efstratiou', u'Ilias Leontiadis', u'Cecilia Mascolo', u'James Scott', u'Peter Key']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6693/understanding-within-content-engagement-through-pattern-analysis-mouse-gestures
found
<h6>
 Abstract
</h6>

<p class="leading">
 The availability of large volumes of interaction data andscalable data mining techniques have made possible to study the online behaviour for millions of Web users. Part of theefforts have focused on understanding how users interactand engage with web content. However, the measurementof within-content engagement remains a difficult and unsolved task. This is because of the lack of standardised, well-validated methods for measuring engagement, especially inan online context. To address this gap, we perform a controlled user study where we observe how users respond to online news in the presence or lack of interest. We collectmouse tracking data, which are known to correlate with visual attention, and examine how cursor behaviour can inform user engagement measures. The proposed method does not use any pre-determined concepts to characterise the cursor patterns. We, rather, follow an unsupervised approachand use a large set of features engineered from our data toextract the cursor patterns. Our findings support the connection between gaze and cursor behaviour but also, and more importantly, reveal other dependencies, such as the correlation between cursor activity and experienced affect. Finally, we demonstrate the value of our method by predicting the outcome of online news reading experiences.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mousecikm2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mousecikm2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mousecikm2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Understanding Within-Content Engagement through Pattern Analysis of Mouse Gestures
23rd International Conference on Information and Knowledge Management (CIKM), Shanghai, China, November 3-7, 2014.
[u'Ioannis Arapakis', u'Mounia Lalmas', u'George Valkanas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6692/readers-preference-and-behavior-wikipedia
found
<h6>
 Abstract
</h6>

<p class="leading">
 Wikipedia is a collaboratively-edited online encyclopaedia that relies on thousands of editors to both contribute articles and maintain their quality. Over the last years, research has extensively investigated this group of users while another group of Wikipedia users, the readers, their preferences and their behavior have not been much studied. This paper makes this group and its activities visible and valuable to Wikipedias editor community. We carried out a study on two datasets covering a 13-months period to obtain insights on users' preferences and reading behavior in Wikipedia. We show that the most read articles do not necessarily correspond to those frequently edited, suggesting some degree of non-alignment between user reading preferences and author editing preferences. We also identified that popular and often edited articles are read according to four main patterns, and that how an article is read may change over time. We illustrate how this information can provide valuable insights toWikipedias editor community.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wiki.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wiki.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wiki.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Readers Preference and Behavior on Wikipedia
25th ACM Conference on Hypertext and Social Media, 1-4 September 2014, Santiago, Chile
[u'Janette Lehmann', u'Mounia Lalmas', u'Claudia Muller Birn', u'David Laniado', u'Andreas Kaltenbrunner']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6686/automatic-discovery-global-and-local-equivalence-relationships-labeled-geo-spatial
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We propose a novel algorithmic framework to automatically detect which labels refer to the same concept in labeled spatial data. People often use different words and synonyms when referring to the same concept or location.Furthermore these words and their usage vary across culture, language, and place. Our method analyzes the patterns in the spatial distribution of labels to discover equivalence relationships. We evaluate our proposed technique on a large collection of geo-referenced Flickr photos using a semi-automatically constructed ground truth from an existing ontology. Our approach is able to classify equivalent tags with a high accuracy (AUC of $0.85$), as well as providing the geographic extent where the relationship holds.
 </p>
</p>

<p>
 We propose a novel algorithmic framework to automatically detect which labels refer to the same concept in labeled spatial data. People often use different words and synonyms when referring to the same concept or location.Furthermore these words and their usage vary across culture, language, and place. Our method analyzes the patterns in the spatial distribution of labels to discover equivalence relationships. We evaluate our proposed technique on a large collection of geo-referenced Flickr photos using a semi-automatically constructed ground truth from an existing ontology. Our approach is able to classify equivalent tags with a high accuracy (AUC of $0.85$), as well as providing the geographic extent where the relationship holds.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tag-canonicalization.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tag-canonicalization.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/tag-canonicalization.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Automatic discovery of global and local equivalence relationships in labeled geo-spatial data
ACM International Conference on Hypertext and Social Media
[u'Bart Thomee', u'Gianmarco De Francisci Morales']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6676/shortest-path-happiness-recommending-beautiful-quiet-and-happy-routes-city
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  When providing directions to a place, web and mobile mapping services are all able to suggest the shortest route. The goal of this work is to automatically suggest routes that are not only short but also emotionally pleasant. To quantify the extent to which urban locations are pleasant, we use data from a crowd-sourcing platform that shows two street scenes in London (out of hundreds), and a user votes on which one looks more beautiful, quiet, and happy. We consider votes from more than 3.3K individuals and translate them into quantitative measures of location perceptions. We arrange those locations into a graph upon which we learn pleasant routes. Based on a quantitative validation, we find that, compared to the shortest routes, the recommended ones add just a few extra walking minutes and are indeed perceived to be more beautiful, quiet, and happy. To test the generality of our approach, we consider Flickr metadata of more than 3.7M pictures in London and 1.3M in Boston, compute proxies for the crowdsourced beauty dimension (the one for which we have collected the most votes), and evaluate those proxies with 30 participants in London and 54 in Boston. These participants have not only rated our recommendations but have also carefully motivated their choices, providing insights for future work.
 </div>
</p>

<div dir="ltr">
 When providing directions to a place, web and mobile mapping services are all able to suggest the shortest route. The goal of this work is to automatically suggest routes that are not only short but also emotionally pleasant. To quantify the extent to which urban locations are pleasant, we use data from a crowd-sourcing platform that shows two street scenes in London (out of hundreds), and a user votes on which one looks more beautiful, quiet, and happy. We consider votes from more than 3.3K individuals and translate them into quantitative measures of location perceptions. We arrange those locations into a graph upon which we learn pleasant routes. Based on a quantitative validation, we find that, compared to the shortest routes, the recommended ones add just a few extra walking minutes and are indeed perceived to be more beautiful, quiet, and happy. To test the generality of our approach, we consider Flickr metadata of more than 3.7M pictures in London and 1.3M in Boston, compute proxies for the crowdsourced beauty dimension (the one for which we have collected the most votes), and evaluate those proxies with 30 participants in London and 54 in Boston. These participants have not only rated our recommendations but have also carefully motivated their choices, providing insights for future work.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ht14.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ht14.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ht14.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Shortest Path to Happiness: Recommending Beautiful, Quiet, and Happy Routes in the City
Hypertext
[u'Daniele Quercia', u'Luca Maria Aiello', u'Rossano Schifanella']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6664/identifying-and-labeling-search-tasks-query-based-hawkes-processes
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.
  </div>
 </div>
</div>
<div>
 <div>
  We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.
 </div>
</div>

<div>
 We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/hbdeng_53085989e1e4f.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/hbdeng_53085989e1e4f.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/hbdeng_53085989e1e4f.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Identifying and Labeling Search Tasks via Query-based Hawkes Processes
The 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
[u'Hongbo Deng', u'Anlei Dong', u'Yi Chang', u'Liangda Li', u'Hongyuan Zha']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6969/optimal-recommendations-under-attraction-aversion-and-social-influence
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  Peoples interests are dynamically evolving, often affected by external factors such as trends promoted by the media or adopted by their friends. In this work, we model interest evolution through dynamic interest cascades: we consider a scenario where a users interests may be affected by (a) the interests of other users in her social circle, as well as (b) suggestions she receives from a recommender system. In the latter case, we model user reactions through either attraction or aversion towards past suggestions. We study this interest evolution process, and the utility accrued by recommendations, as a function of the systems recommendation strategy. We show that, in steady state, the optimal strategy can be computed as the solution of a semi-definite program (SDP). Using datasets of user ratings, we provide evidence for the existence of aversion and attraction in real-life data, and show that our optimal strategy can lead to significantly improved recommendations over systems that ignore aversion and attraction.
 </div>
</p>

<div dir="ltr">
 Peoples interests are dynamically evolving, often affected by external factors such as trends promoted by the media or adopted by their friends. In this work, we model interest evolution through dynamic interest cascades: we consider a scenario where a users interests may be affected by (a) the interests of other users in her social circle, as well as (b) suggestions she receives from a recommender system. In the latter case, we model user reactions through either attraction or aversion towards past suggestions. We study this interest evolution process, and the utility accrued by recommendations, as a function of the systems recommendation strategy. We show that, in steady state, the optimal strategy can be computed as the solution of a semi-definite program (SDP). Using datasets of user ratings, we provide evidence for the existence of aversion and attraction in real-life data, and show that our optimal strategy can lead to significantly improved recommendations over systems that ignore aversion and attraction.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd1361-lu1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd1361-lu1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd1361-lu1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Optimal Recommendations under Attraction, Aversion, and Social Influence
Knowledge Discovery and Data Mining (KDD)
[u'Wei Lu', u'Stratis Ioannidis', u'Smriti Bhagat', u'Laks V.s. Lakshmanan']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6675/provable-deterministic-leverage-score-sampling
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We explain theoretically a curious empirical phenomenon: Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We explain theoretically a curious empirical phenomenon: Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.
  </div>
 </div>
</div>
<div>
 <div>
  We explain theoretically a curious empirical phenomenon: Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.
 </div>
</div>

<div>
 We explain theoretically a curious empirical phenomenon: Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd_boutsidis1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd_boutsidis1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/kdd_boutsidis1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Provable Deterministic Leverage Score Sampling
20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
[u'Christos Boutsidis', u'Dimitris Papailiopoulos', u'Anastasios Kyrillidis']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6661/large-scale-2014-world-cup-outcome-analysis-based-tumblr-posts
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 With the 2014 FIFA World Cup kicking o on June 12th, billions of fans across the world have turned their attention toward host country Brazil to root for their teams. Soccer (or football, if you prefer) fans are loud; you need only remember the last World Cup's infamous vuvuzelas for a demonstration. But fans are not only loud in stadiums. They also make their voices heard across social media. And though you may assume these fans are just blowing their vuvuzelas into the social abyss, if you listen closely, you will discover a treasure trove of data, including an answer to the most important question of all: &quot;Who will win?&quot;. In this paper we use Tumblr posts collected during 4 months prior to the start of the World Cup to predict the outcome of every game. We describe the prediction algorithm as well as the analysis of the performance results, including comparison of the predictions of several competing methods.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/radosavljevic2014sportskdd.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/radosavljevic2014sportskdd.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/radosavljevic2014sportskdd.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Large-scale 2014 World Cup outcome analysis based on Tumblr posts
Workshop on Large-Scale Sports Analytics at ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SportsKDD), New York City, USA, 2014.
[u'Vladan Radosavljevic', u'Mihajlo Grbovic', u'Nemanja Djuric', u'Narayan Bhamidipati']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=13
found
 LINK 
https://labs.yahoo.com/publications/6679/unified-framework-evaluating-online-user-treatment-effectiveness-advertising
found
<h6>
 Abstract
</h6>

<p class="leading">
 The measurement of ad effectiveness is one of the central problems of online advertising. Typically the performance is measured by investigating the proportion of people who converted or performed other success actions after they saw the ads. These metrics commonly overestimate campaign effectiveness since they do not account for users who would have performed actions even if the campaign did not happen. Conventional metrics also fail to answer the following questions that are important to advertisers: 1) Which users convert because they see the ad and which users would have converted even if they do not see the ad? 2) What is the cumulative effect of multiple advertising strategies on performance? 3) How does a campaign affect the size of the potential audience pool?

In this paper we propose a general methodology for assessing campaign performance that addresses all of these questions. Our method does not require randomized experiments or additional ads to be shown. We develop a unified causal modeling framework that establishes a causal relationship between seeing an ad and performing an action, which is based on propensity methodology embedded in a parallel computation algorithm. We derive a novel robust rank test for model validation. We also provide innovative interpretations of the estimation results by the causal inference, addressing `smart cheating' of online ads (i.e. targeting the users who are likely to convert even without any ad exposure, which does not add value to the advertisers). The three components (model, validation, and interpretation) complete a unified solution to ad effectiveness measurement. The framework is applied to three online campaigns involving millions of unique users. Results from real online campaigns show that this methodology is robust to online data sparseness, high dimensionality and biases from user features.

This paper focuses on measuring the effectiveness of online ads, but the framework is readily applicable to measure the effectiveness of other kinds of treatments on various user metrics, for example the impact of different strategies on user engagement metrics.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/proposal_21_acmformat_unblind_UEO1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/proposal_21_acmformat_unblind_UEO1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/proposal_21_acmformat_unblind_UEO1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Unified Framework for Evaluating Online User Treatment Effectiveness, with Advertising Applications
The Second User Engagement Optimization Workshop
[u'Pengyuan Wang', u'Jimmy Yang', u'Marsha Meytlis', u'Fei Yu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6653/learning-features-and-parts-fine-grained-recognition
found
<h6>
 Abstract
</h6>

<p class="leading">
 This paper addresses the problem of fine-grained recognition:recognizing subordinate categories such as birdspecies or car models. We focus on the two major challenges:learning expressive appearance descriptors and localizingdiscriminative parts. To this end we propose a newobject representation that detects important parts and describesfine-grained appearances. The part detectors arelearned in a fully unsupervised manner, based on the insightthat images with similar poses can be automaticallydiscovered for fine-grained classes in the same domain. Theappearance descriptors are learned using deep convolutionalneural networks. Our approach requires only imagelevel class labels, without any use of part annotations orsegmentation masks. We demonstrate competitive results onpar with best results achieved by methods using part-levelannotations, while outperforming existing approaches withcomparable supervision by a large margin.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICPR14_2078_FI.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICPR14_2078_FI.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ICPR14_2078_FI.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Features and Parts for Fine-Grained Recognition
22nd International Conference on Pattern Recognition
[u'Jia Li', u'Jonathan Krause', u'Timit Gebru', u'Deng Jia', u'Feifei']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6682/who-follow-and-why-link-prediction-explanations
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p style="text-align:left;">
  User recommender systems are a key component in any on-line socialnetworking platform: they help the users growing their networkfaster, thus driving engagement and loyalty.
 </p>
 <p style="text-align:left;">
  In this paper we study link prediction with explanations foruser recommendation in social networks. For this problem we propose
  <strong>
   WTFW
  </strong>
  (&quot;Who to Follow and Why&quot;), a stochastic topic model forlink prediction over directed and nodes-attributed graphs. Ourmodel not only predicts links, but for each predicted link itdecides whether it is a &quot;topical&quot; or a &quot;social&quot; link, anddepending on this decision it produces a different type ofexplanation.
 </p>
 <p style="text-align:left;">
  A topical link is recommended between a user interested in a topicand a user authoritative in that topic: the explanation in this caseis a set of binary features describing the topic responsible of thelink creation. A social link is recommended between users whichshare a large social neighborhood: in this case the explanation isthe set of neighbors which are more likely to be responsible for thelink creation.
 </p>
 Our experimental assessment onreal-world data confirms the accuracy of WTFW in thelink prediction and the quality of the associated explanations.
</p>

<p style="text-align:left;">
 User recommender systems are a key component in any on-line socialnetworking platform: they help the users growing their networkfaster, thus driving engagement and loyalty.
</p>

<p style="text-align:left;">
 In this paper we study link prediction with explanations foruser recommendation in social networks. For this problem we propose
 <strong>
  WTFW
 </strong>
 (&quot;Who to Follow and Why&quot;), a stochastic topic model forlink prediction over directed and nodes-attributed graphs. Ourmodel not only predicts links, but for each predicted link itdecides whether it is a &quot;topical&quot; or a &quot;social&quot; link, anddepending on this decision it produces a different type ofexplanation.
</p>

<strong>
 WTFW
</strong>

<p style="text-align:left;">
 A topical link is recommended between a user interested in a topicand a user authoritative in that topic: the explanation in this caseis a set of binary features describing the topic responsible of thelink creation. A social link is recommended between users whichshare a large social neighborhood: in this case the explanation isthe set of neighbors which are more likely to be responsible for thelink creation.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/KDD2014_cr.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/KDD2014_cr.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/KDD2014_cr.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Who to Follow and Why: Link Prediction with Explanations
20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
[u'Nicola Barbieri', u'Francesco Bonchi', u'Giuseppe Manco']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6683/core-decomposition-uncertain-graphs
found
<h6>
 Abstract
</h6>

<p class="leading">
 Core decomposition has proven to be a useful primitive for a wide range of graph analyses. One of its most appealing features is that, unlike other notions of dense subgraphs, it can be computed linearly in the size of the input graph.In this paper we provide an analogous tool for uncertain graphs, i.e., graphs whose edges are assigned a probability of existence. The fact that core decomposition can be computed efficiently in deterministic graphs does not guarantee efficiency in uncertain graphs, where even the simplest graph operations may become computationally intensive. Here we show that core decomposition of uncertain graphs can be carried out efficiently as well. We extensively evaluate our definitions and methods on a number of real-world datasets and applications, such as influence maximization and task-driven team formation.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0554-Bonchi.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0554-Bonchi.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0554-Bonchi.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Core Decomposition of Uncertain Graphs
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 14)
[u'Francesco Bonchi', u'Francesco Gullo', u'Andreas Kaltenbrunner', u'Yana Volkovich']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6949/crowdsourced-time-sync-video-tagging-using-temporal-and-personalized-topic
found
<h6>
 Abstract
</h6>

<p class="leading">
 Time-sync video tagging aims to automatically generate tags for each video shot. It can improve the users experience in previewing a videos timeline structure compared to traditional schemes that tag an entire video clip. In this paper, we propose a new application which extracts time-sync video tags by automatically exploiting crowdsourced comments from video websites such as Nico Nico Douga, where videos are commented on by online crowd users in a time-sync manner. The challenge of the proposed application is that users with bias interact with one another frequently and bring noise into the data, while the comments are too sparse to compensate for the noise. Previous techniques are unable to handle this task well as they consider video semantics independently, which may overfit the sparse comments in each shot and thus fail to provide accurate modeling. To resolve these issues, we propose a novel temporal and personalized topic model that jointly considers temporal dependencies between video semantics, users interaction in commenting, and users preferences as prior knowledge. Our proposed model shares knowledge across video shots via users to enrich the short comments, and peels off user interaction and user bias to solve the noisy-comment problem. Log-likelihood analyses and user studies on large datasets show that the proposed model outperforms several state-of-the-art baselines in video tagging quality. Case studies also demonstrate our models capability of extracting tags from the crowdsourced short and noisy comments.
 <div id="xunlei_com_thunder_helper_plugin_d462f475-c18e-46be-bd10-327458d045bd">
 </div>
</p>

<div id="xunlei_com_thunder_helper_plugin_d462f475-c18e-46be-bd10-327458d045bd">
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0203-wuA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0203-wuA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/frp0203-wuA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Crowdsourced Time-sync Video Tagging Using Temporal and Personalized Topic Modeling
Crowdsourced Time-sync Video Tagging using Temporal and Personalized Topic Modeling
[u'Bin Wu', u'Erheng Zhong', u'Ben Tan', u'Andrew Horner', u'Qiang Yang']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6688/random-forests-very-fast-decision-trees-gpu-mining-evolving-big-data-streams
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div>
  <div>
   <div>
    Random Forest is a classical ensemble method used to improve the performance of single tree classifiers. It is able to obtain superior performance by increasing the diversity of the single classifiers. However, in the more challenging context of evolving data streams, the classifier also has to be adaptive and work under very strict constraints of space and time. Furthermore, the computational load of using a large number of classifiers can make its application extremely expensive. In this work, we present a method for building Random Forests that use Very Fast Decision Trees for data streams on GPUs. We show how this method can benefit from the massive parallel architecture of GPUs, which are becoming an efficient hardware alternative to large clusters of computers. Moreover, our algorithm minimizes the communication between CPU and GPU by building the trees directly inside the GPU. We run an empirical evaluation and compare our method to two well know machine learning frameworks, VFML and MOA. Random Forests on the GPU are at least 300x faster while maintaining a similar accuracy.
   </div>
  </div>
 </div>
</p>

<div>
 <div>
  <div>
   Random Forest is a classical ensemble method used to improve the performance of single tree classifiers. It is able to obtain superior performance by increasing the diversity of the single classifiers. However, in the more challenging context of evolving data streams, the classifier also has to be adaptive and work under very strict constraints of space and time. Furthermore, the computational load of using a large number of classifiers can make its application extremely expensive. In this work, we present a method for building Random Forests that use Very Fast Decision Trees for data streams on GPUs. We show how this method can benefit from the massive parallel architecture of GPUs, which are becoming an efficient hardware alternative to large clusters of computers. Moreover, our algorithm minimizes the communication between CPU and GPU by building the trees directly inside the GPU. We run an empirical evaluation and compare our method to two well know machine learning frameworks, VFML and MOA. Random Forests on the GPU are at least 300x faster while maintaining a similar accuracy.
  </div>
 </div>
</div>
<div>
 <div>
  Random Forest is a classical ensemble method used to improve the performance of single tree classifiers. It is able to obtain superior performance by increasing the diversity of the single classifiers. However, in the more challenging context of evolving data streams, the classifier also has to be adaptive and work under very strict constraints of space and time. Furthermore, the computational load of using a large number of classifiers can make its application extremely expensive. In this work, we present a method for building Random Forests that use Very Fast Decision Trees for data streams on GPUs. We show how this method can benefit from the massive parallel architecture of GPUs, which are becoming an efficient hardware alternative to large clusters of computers. Moreover, our algorithm minimizes the communication between CPU and GPU by building the trees directly inside the GPU. We run an empirical evaluation and compare our method to two well know machine learning frameworks, VFML and MOA. Random Forests on the GPU are at least 300x faster while maintaining a similar accuracy.
 </div>
</div>

<div>
 Random Forest is a classical ensemble method used to improve the performance of single tree classifiers. It is able to obtain superior performance by increasing the diversity of the single classifiers. However, in the more challenging context of evolving data streams, the classifier also has to be adaptive and work under very strict constraints of space and time. Furthermore, the computational load of using a large number of classifiers can make its application extremely expensive. In this work, we present a method for building Random Forests that use Very Fast Decision Trees for data streams on GPUs. We show how this method can benefit from the massive parallel architecture of GPUs, which are becoming an efficient hardware alternative to large clusters of computers. Moreover, our algorithm minimizes the communication between CPU and GPU by building the trees directly inside the GPU. We run an empirical evaluation and compare our method to two well know machine learning frameworks, VFML and MOA. Random Forests on the GPU are at least 300x faster while maintaining a similar accuracy.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ECAI-550.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ECAI-550.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ECAI-550.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Random Forests of Very Fast Decision Trees on GPU for Mining Evolving Big Data Streams
ECAI 2014
[u'Diego Marron', u'Gianmarco De Francisci Morales', u'Albert Bifet']
Machine Learning and Data Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6956/flexor-flexible-garbling-xor-gates-beats-free-xor
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Most implementations of Yao's garbled circuit approach for 2-party secure computation use the {\em free-XOR} optimization of Kolesnikov \& Schneider (ICALP 2008). We introduce an alternative technique called {\em flexible-XOR} (fleXOR) that generalizes free-XOR and offers several advantages. First, fleXOR can be instantiated under a weaker hardness assumption on the underlying cipher/hash function (related-key security only, compared to related-key and circular security required for free-XOR) while maintaining most of the performance improvements that free-XOR offers. Alternatively, even though XOR gates are not always ``free'' in our approach, we show that the other (non-XOR) gates can be optimized more heavily than what is possible when using free-XOR. For many circuits of cryptographic interest, this can yield a significantly (over 30\%) smaller garbled circuit than any other known techniques (including free-XOR) or their combinations.
 </p>
</p>

<p>
 Most implementations of Yao's garbled circuit approach for 2-party secure computation use the {\em free-XOR} optimization of Kolesnikov \& Schneider (ICALP 2008). We introduce an alternative technique called {\em flexible-XOR} (fleXOR) that generalizes free-XOR and offers several advantages. First, fleXOR can be instantiated under a weaker hardness assumption on the underlying cipher/hash function (related-key security only, compared to related-key and circular security required for free-XOR) while maintaining most of the performance improvements that free-XOR offers. Alternatively, even though XOR gates are not always ``free'' in our approach, we show that the other (non-XOR) gates can be optimized more heavily than what is possible when using free-XOR. For many circuits of cryptographic interest, this can yield a significantly (over 30\%) smaller garbled circuit than any other known techniques (including free-XOR) or their combinations.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/460.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/460.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/460.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
FleXOR: Flexible garbling for XOR gates that beats free-XOR
Advances in Cryptology--CRYPTO 2014
[u'Vladimir Kolesnikov', u'Payman Mohassel', u'Mike Rosulek']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6662/delivering-guaranteed-display-ads-under-reach-and-frequency-requirements
found
<h6>
 Abstract
</h6>

<p class="leading">
 We propose a novel idea in the allocation and serving of online advertising. We show that by using predetermined fixed-length streams of ads (which we call patterns) to serve advertising, we can incorporate a variety of interesting features into the ad allocation optimization problem. In particular, our formulation optimizes for representativeness as well as user-level diversity and pacing of ads, under reach and frequency requirements. We show how the problem can be solved efficiently using a column generation scheme in which only a small set of best patterns are kept in the optimization problem. Our numerical tests suggest that with parallelization of the pattern generation process, the algorithm has a promising run time and memory usage.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014AAAI__HojjatTurnerCetintasYang.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014AAAI__HojjatTurnerCetintasYang.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/2014AAAI__HojjatTurnerCetintasYang.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Delivering Guaranteed Display Ads Under Reach and Frequency Requirements
AAAI Conference on Artificial Intelligence (AAAI-14)
[u'Suleyman Cetintas', u'Jimmy Yang', u'Ali Hojjat', u'John Turner']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6644/non-linear-label-ranking-large-scale-prediction-long-term-user-interests
found
<h6>
 Abstract
</h6>

<p class="leading">
 We consider the problem of personalization of online services from the viewpoint of display ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertiser revenue. We propose to reformulate this problem as a label ranking task, and introduce a novel label ranking approach, capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefits of the proposed ranking model in a web-scale setting of targeted advertising.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014aaai1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014aaai1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2014aaai1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests
AAAI Conference on Artificial Intelligence (AAAI-14)
[u'Nemanja Djuric', u'Mihajlo Grbovic', u'Vladan Radosavljevic', u'Narayan Bhamidipati', u'Slobodan Vucetic']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6681/min-d-occur-ensuring-future-occurrences-streaming-sets
found
<h6>
 Abstract
</h6>

<p class="leading">
 Given a set of n elements and a corresponding stream of its subsets, we consider the problem of selecting k elements that should appear in at least d such subsets arriving in the &quot;near&quot; future with high probability. For this min-d-occur problem, we present an algorithm that provides a solution with the success probability of at least 1 - O(kd log n / D + 1/n), where D is a known constant. Our empirical observations on two streaming data sets show that this algorithm achieves high precision and recall values. We further present a sliding window adaptation of the proposed algorithm to provide a continuous selection of these elements. In contrast to the existing work on predicting trends based on potential increase in popularity, our work focuses on a setting with provable guarantees.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cr.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cr.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cr.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Min-d-Occur: Ensuring Future Occurrences in Streaming Sets
The Conference on Uncertainty in Artificial Intelligence (UAI)
[u'Vidit Jain', u'Sainyam Galhotra']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=14
found
 LINK 
https://labs.yahoo.com/publications/6945/verifying-atomicity-data-independence
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present a technique for automatically verifying atomicity ofcomposed concurrent operations. The main observation behind ourapproach is that many composed concurrent operations which occurin practice are data-independent. That is, the control-flow ofthe composed operation does not depend on specific input values.While verifying data-independence is undecidable in the generalcase, we provide succint sufficient conditions that can be used toestablish a composed operation as data-independent. We show thatfor the common case of concurrent maps, data-independence reducesthe hard problem of verifying linearizability to a verificationproblem that can be solved efficiently with a bounded number ofkeys and values.

We implemented our approach in a tool called VINE and evaluatedit on all composed operations from 57 real-world applications(112 composed operations). We show that many composed operations(49 out of 112) are data-independent, and automatically verify30 of them as linearizable and the rest 19 as having violations oflinearizability that could be repaired and then subsequently automaticallyverified. Moreover, we show that the remaining 63 operationsare not linearizable, thus indicating that data independencedoes not limit the expressiveness of writing realistic linearizablecomposed operations.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/issta14.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/issta14.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/issta14.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Verifying Atomicity via Data Independence
ISSTA
[u'Ohad Shacham', u'Eran Yahav', u'Guy Golan Gueta', u'Alex Aiken', u'Nathan Bronson', u'Mooly Sagiv', u'Martin Vechev']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6652/pursuit-good-possible-world-extracting-representative-instances-uncertain-graphs
found
<h6>
 Abstract
</h6>

<p class="leading">
 Data in several applications can be represented as an uncertain graph, whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Even approximate processing based on sampling is usually extremely expensive since it requires a vast number of samples to achieve reasonable quality guarantees. To overcome these problems, we propose algorithms for creating deterministic representative instances of uncertain graphs that maintain the underlying graph properties. Specifically, our algorithms aim at preserving the expected vertex degrees because they capture well the graph topology. Conventional processing techniques can then be applied on these instances to closely approximate the result on the uncertain graph. We experimentally demonstrate, with real and synthetic uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several properties such as shortest path distance, clustering coefficient and betweenness centrality.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/RepInst.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/RepInst.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/RepInst.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
The Pursuit of a Good Possible World: Extracting Representative Instances of Uncertain Graphs
ACM SIGMOD International Conference on Management of Data (SIGMOD 14)
[u'Francesco Gullo', u'Francesco Bonchi', u'Panos Parchas', u'Dimitris Papadias']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6767/fast-and-unconditionally-secure-anonymous-channel
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this paper we focus on sender-anonymous channels (a.k.aDining Cryptographers networks) and present a constructionrequiring a very low (constant) number of rounds of interaction while tolerating actively malicious behavior by someof the participants (up to less than half of them). Our construction is unconditionally secure (meaning that no boundsare placed on the computational power of the adversary),makes black-box use of a verifiable secret sharing (VSS) protocol, and is based on a special-purpose secure multipartycomputation protocol implementing the method of &quot;throwing darts&quot;; its round complexity is essentially equal to thatof the VSS protocol.

In addition, since broadcast cannot be simulated in a point-to-point network when a third or more of the participantsare corrupt, it is impossible to construct VSS (and, moregenerally, any other basic multiparty protocol) in this setting without using a &quot;physical broadcast channel,&quot; and arecent line of research has sought to minimize the use of thisexpensive resource. Our anonymous channel protocol's reduction to VSS is broadcast-round-preserving, thus makingthe fewest (known to date) calls to the broadcast channelwhile running in an overall constant number of rounds.

Finally, anonymous channels play an important role in thesetup phase of an authentication technique known as pseudosignatures, which then may be used to simulate authenticated Byzantine agreement protocols in the information-theoretic setting. Plugging in our anonymous channel translates into a fast (and broadcast-effcient) pseudosignatureconstruction.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anon_channel1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anon_channel1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/anon_channel1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Fast and Unconditionally Secure Anonymous Channel
The 33th Annual ACM Symposium on Principles of Distributed Computing -- PODC 2014
[u'Juan A. Garay', u'Clint Givens', u'Rafail Ostrovsky', u'Pavel Raykov']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6780/semi-streaming-algorithms-annotated-graph-streams
found
<h6>
 Abstract
</h6>

<p class="leading">
 Considerable effort has been devoted to the development of streaming algorithms for analyzing massive graphs. Unfortunately, many results have been negative, establishing that a wide variety of problems require $\Omega(n^2)$ space to solve. One of the few bright spots has been the development of semi-streaming algorithms for a handful of graph problems -- these algorithms use space $O(n\cdot\text{polylog}(n))$.

In the annotated data streaming model of Chakrabarti et al., a computationally limited client wants to compute some property of a massive input, but lacks the resources to store even a small fraction of the input, and hence cannot perform the desired computation locally. The client therefore accesses a powerful but untrusted service provider, who not only performs the requested computation, but also proves that the answer is correct.

We put forth the notion of semi-streaming algorithms for annotated graph streams (semi-streaming annotation schemes for short). These are protocols in which both the client's space usage and the length of the proof are $O(n \cdot \text{polylog}(n))$. We give evidence that semi-streaming annotation schemes represent a substantially more robust solution concept than does the standard semi-streaming model. On the positive side, we give semi-streaming annotation schemes for two dynamic graph problems that are intractable in the standard model: (exactly) counting triangles, and (exactly) computing maximum matchings. The former scheme answers a question of Cormode. On the negative side, we identify for the first time two natural graph problems (connectivity and bipartiteness in a certain edge update model) that can be solved in the standard semi-streaming model, but cannot be solved by annotation schemes of &quot;sub-semi-streaming&quot; cost. That is, these problems are just as hard in the annotations model as they are in the standard model.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/graphstreams2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/graphstreams2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/graphstreams2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Semi-Streaming Algorithms for Annotated Graph Streams

[u'Justin Thaler']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6678/impact-response-latency-user-behavior-web-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 Traditionally, the efficiency and effectiveness of search systems have both been of great interest to the information retrieval community. However, an in-depth analysis on the interplay between the response latency of web search systems and users' search experience has been missing so far. In order to fill this gap, we conduct two separate studies aiming to reveal how response latency affects the user behavior in web search. First, we conduct a controlled user study trying to understand how users perceive the response latency of a search system and how sensitive they are to increasing delays in response. This study reveals that, when artificial delays are introduced into the response, the users of a fast search system are more likely to notice these delays than the users of a slow search system. The introduced delays become noticeable by the users once they exceed a certain threshold value. Second, we perform an analysis using a large-scale query log obtained from Yahoo web search to observe the potential impact of increasing response latency on the click behavior of users. This analysis demonstrates that latency has an impact on the click behavior of users to some extent. In particular, given two content-wise identical search result pages, we show that the users are more likely to perform clicks on the result page that is served with lower latency.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp482-arapakis.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp482-arapakis.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp482-arapakis.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Impact of Response Latency on User Behavior in Web Search
The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'14)
[u'Ioannis Arapakis', u'Xiao Bai', u'B. Barla Cambazoglu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6663/two-dimensional-click-model-query-auto-completion
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <p>
   Query auto-completion (QAC) facilitates faster user query input by predicting users intended queries. Most QAC algorithms take a learning-based approach to incorporate various signals for query relevance prediction. However, such models are trained on simulated user inputs from query log data. The lack of real user interaction data in the QAC process prevents them from further improving the QAC performance.In this work, for the first time we have collected a high-resolution QAC query log that records every keystroke in a QAC session. Based on this data, we discover two types of user behavior, namely the horizontal skipping bias and vertical position bias which are crucial for relevance prediction in QAC. In order to better explain them, we propose a novel two-dimensional click model for modeling the QAC process with emphasis on these behaviors.Extensive experiments on our QAC data set from both PC and mobile devices demonstrate that our proposed model can accurately explain the users behaviors in interacting with a QAC system, and the resulting relevance model significantly improves the QAC performance over existing click models. Furthermore, the learned knowledge about the skipping behavior can be effectively incorporated into existing learning-based QAC models to further improve their performance.
  </p>
 </div>
</p>

<div title="Page 1">
 <p>
  Query auto-completion (QAC) facilitates faster user query input by predicting users intended queries. Most QAC algorithms take a learning-based approach to incorporate various signals for query relevance prediction. However, such models are trained on simulated user inputs from query log data. The lack of real user interaction data in the QAC process prevents them from further improving the QAC performance.In this work, for the first time we have collected a high-resolution QAC query log that records every keystroke in a QAC session. Based on this data, we discover two types of user behavior, namely the horizontal skipping bias and vertical position bias which are crucial for relevance prediction in QAC. In order to better explain them, we propose a novel two-dimensional click model for modeling the QAC process with emphasis on these behaviors.Extensive experiments on our QAC data set from both PC and mobile devices demonstrate that our proposed model can accurately explain the users behaviors in interacting with a QAC system, and the resulting relevance model significantly improves the QAC performance over existing click models. Furthermore, the learned knowledge about the skipping behavior can be effectively incorporated into existing learning-based QAC models to further improve their performance.
 </p>
</div>
<p>
 Query auto-completion (QAC) facilitates faster user query input by predicting users intended queries. Most QAC algorithms take a learning-based approach to incorporate various signals for query relevance prediction. However, such models are trained on simulated user inputs from query log data. The lack of real user interaction data in the QAC process prevents them from further improving the QAC performance.In this work, for the first time we have collected a high-resolution QAC query log that records every keystroke in a QAC session. Based on this data, we discover two types of user behavior, namely the horizontal skipping bias and vertical position bias which are crucial for relevance prediction in QAC. In order to better explain them, we propose a novel two-dimensional click model for modeling the QAC process with emphasis on these behaviors.Extensive experiments on our QAC data set from both PC and mobile devices demonstrate that our proposed model can accurately explain the users behaviors in interacting with a QAC system, and the resulting relevance model significantly improves the QAC performance over existing click models. Furthermore, the learned knowledge about the skipping behavior can be effectively incorporated into existing learning-based QAC models to further improve their performance.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SIGIR2014_submission_0.99.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SIGIR2014_submission_0.99.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SIGIR2014_submission_0.99.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Two-Dimensional Click Model for Query Auto-Completion
The 37th Annual International ACM SIGIR CONFERENCE
[u'Anlei Dong', u'Hongbo Deng', u'Yi Chang', u'Yanen Li', u'Hongning Wang', u'Chengxiang Zhai']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6744/privacy-social-mechanism-maintaining-inconsistency-between-identities
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  This paper explain how the theory of social representations [Moscovici'71] justifies the creation of a mechanism which will help individuals maintain inconsistent identities. This mechanism has the properties described by Emergent Privacy.
 </p>
</p>

<p>
 This paper explain how the theory of social representations [Moscovici'71] justifies the creation of a mechanism which will help individuals maintain inconsistent identities. This mechanism has the properties described by Emergent Privacy.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="/mobstor/publication_attachments/2014Privacy as  Social Machanism .pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="/mobstor/publication_attachments/2014Privacy as  Social Machanism .pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="/mobstor/publication_attachments/2014Privacy as  Social Machanism .pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Privacy as a Social Mechanism for Maintaining Inconsistency Between Identities
Papers on Social Representations
[u'Ran Wolff', u'Smadar Ben Asher']
None of the above
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6670/probabilistic-modeling-joint-context-distributional-similarity
found
<h6>
 Abstract
</h6>

<p class="leading">
 Most traditional distributional similaritymodels fail to capture syntagmatic patternsthat group together multiple word featureswithin the same joint context. In this workwe introduce a novel generic distributionalsimilarity scheme under which the powerof probabilistic models can be leveragedto effectively model joint contexts. Basedon this scheme, we implement a concretemodel which utilizes probabilistic n-gramlanguage models.Our evaluations suggest that this model is particularly well-suited for measuring similarity for verbs, which are known to exhibit richer syntagmatic patterns, while maintaining comparable or better performance with respect to competitive baselines for nouns. Following this, we propose our scheme as a framework for future semantic similarity models leveraging the substantial body of work that exists in probabilistic language modeling.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/joint_context_conll144.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/joint_context_conll144.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/joint_context_conll144.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Probabilistic Modeling of Joint-context in Distributional Similarity
CONLL
[u'Idan Szpektor', u'Oren Melamud', u'Ido Dagan', u'Jacob Goldberger', u'Deniz Yuret']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6666/mychannel-exploring-city-based-multimedia-news-presentations-living-room-tv
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We see the television as a primary device to connect viewers with the information and people that matter most in their lives. Televisions, as central places where the family gathers, provide a unique location to elevate news and social updates that can connect family and friends across a distance. Through creating the MyChannel service, a TV- based personalized news program, we have explored the types of content that work best in this format. We have also gained a detailed understanding of how television content can inspire feelings of connection and communication with friends and family at a distance through an 8-day in-home field evaluation. We describe the system and findings from our studies and close with a discussion on the future of personalized television news.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We see the television as a primary device to connect viewers with the information and people that matter most in their lives. Televisions, as central places where the family gathers, provide a unique location to elevate news and social updates that can connect family and friends across a distance. Through creating the MyChannel service, a TV- based personalized news program, we have explored the types of content that work best in this format. We have also gained a detailed understanding of how television content can inspire feelings of connection and communication with friends and family at a distance through an 8-day in-home field evaluation. We describe the system and findings from our studies and close with a discussion on the future of personalized television news.
  </div>
 </div>
</div>
<div>
 <div>
  We see the television as a primary device to connect viewers with the information and people that matter most in their lives. Televisions, as central places where the family gathers, provide a unique location to elevate news and social updates that can connect family and friends across a distance. Through creating the MyChannel service, a TV- based personalized news program, we have explored the types of content that work best in this format. We have also gained a detailed understanding of how television content can inspire feelings of connection and communication with friends and family at a distance through an 8-day in-home field evaluation. We describe the system and findings from our studies and close with a discussion on the future of personalized television news.
 </div>
</div>

<div>
 We see the television as a primary device to connect viewers with the information and people that matter most in their lives. Televisions, as central places where the family gathers, provide a unique location to elevate news and social updates that can connect family and friends across a distance. Through creating the MyChannel service, a TV- based personalized news program, we have explored the types of content that work best in this format. We have also gained a detailed understanding of how television content can inspire feelings of connection and communication with friends and family at a distance through an 8-day in-home field evaluation. We describe the system and findings from our studies and close with a discussion on the future of personalized television news.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p71-bentley.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p71-bentley.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p71-bentley.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
MyChannel: Exploring City-Based Multimedia News Presentations on the Living Room TV
The 2014 ACM international conference on Interactive experiences for TV and online video (ACM TVx)
[u'Frank Bentley', u'Karolina Buchner', u'Jofish Kaye']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6667/learning-optimal-seeds-diffusion-based-salient-object-detection
found
<h6>
 Abstract
</h6>

<p class="leading">
 In diffusion-based saliency detection, an image is partitioned into superpixels and mapped to a graph, with superpixels as nodes and edge strengths proportional to superpixel similarity. Saliency information is then propagated over the graph using a diffusion process, whose equilibrium state yields the object saliency map. The optimal solution is the product of a propagation matrix and a saliency seed vector that contains a prior saliency assessment. This is obtained from either a bottom-up saliency detector or some heuristics. In this work, we propose a method to learn optimal seeds for object saliency. Two types of features are computed per superpixel: the bottom-up saliency of the superpixel region and a set of mid-level vision features informative of how likely the superpixel is to belong to an object. The combination of features that best discriminates between object and background saliency is then learned, using a large-margin formulation of the discriminant saliency principle. The propagation of the resulting saliency seeds, using a diffusion process, is finally shown to outperform the state of the art on a number of salient object detection datasets.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/CVPR2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/CVPR2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/CVPR2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Optimal Seeds for Diffusion-based Salient Object Detection
CVPR 2014 Columbus, OH
[u'Vijay Mahadevan', u'Song Lu', u'Nuno Vasconcelos']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=15
found
 LINK 
https://labs.yahoo.com/publications/6668/6-seconds-sound-and-vision-creativity-micro-videos
found
<h6>
 Abstract
</h6>

<p class="leading">
 The notion of creativity, as opposed to related concepts such as beauty or interestingness, has not been studied from the perspective of automatic analysis of multimedia content. Meanwhile, short online videos shared on social media platforms, or micro-videos, have arisen as a new medium for creative expression. In this paper we study creative micro-videos in an effort to understand the features that make a video creative, and to address the problem of automatic detection of creative content. Defining creative videos as those that are novel and have aesthetic value, we conduct a crowdsourcing experiment to create a dataset of 4,000 micro-videos labelled as creative and non-creative. We propose a set of computational features that we map to the components of our definition of creativity, and conduct an analysis to determine which of these features correlate most with creative video. Finally, we evaluate a supervised approach to automatically detect creative video, with promising results, showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
6 Seconds of Sound and Vision: Creativity in Micro-Videos
CVPR 2014, IEEE International Conference on Computer Vision and Pattern Recognition
[u'Miriam Redi', u"Neil O'hare", u'Michele Trevisiol', u'Alejandro Jaimes', u'Rossano Schifanella']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6654/co-localization-real-world-images
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this paper, we tackle the problem of co-localization inreal-world images. Co-localization is the problem of simultaneouslylocalizing (with bounding boxes) objects of thesame class across a set of distinct images. Although similarproblems such as co-segmentation and weakly supervisedlocalization have been previously studied, we focus on beingable to perform co-localization in real-world settings,which are typically characterized by large amounts of intraclassvariation, inter-class diversity, and annotation noise.To address these issues, we present a joint image-box formulationfor solving the co-localization problem, and showhow it can be relaxed to a convex quadratic program whichcan be efficiently solved. We perform an extensive evaluationof our method compared to previous state-of-the artapproaches on the challenging PASCAL VOC 2007 andObject Discovery datasets. In addition, we also present alarge-scale study of co-localization on ImageNet, involvingground-truth annotations for 3,624 classes and approximately1 million images.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/camera_readyTang.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/camera_readyTang.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/camera_readyTang.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Co-localization in Real-World Images
The Twenty-Seventh IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
[u'Jia Li', u'Kevin Tang', u'Armand Joulin', u'Feifei']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6987/locally-optimized-product-quantization-approximate-nearest-neighbor-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate nearest neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to provide non-exhaustive search, i.e. inverted lists or a multi-index, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Local optimization is over rotation and space decomposition; interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/lopq.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/lopq.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/lopq.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Locally Optimized Product Quantization for Approximate Nearest Neighbor Search
International Conference on Computer Vision and Pattern Recognition (CVPR) 2014
[u'Yannis Kalantidis', u'Yannis Avrithis']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6685/reading-source-code-social-ties
found
<h6>
 Abstract
</h6>

<p class="leading">
 Though online social network research has exploded during the past years, not much thought has been given to the exploration of the nature of social links. Online interactions have been interpreted as indicative of one social process or another (e.g., status exchange or trust), often with little systematic justification regarding the relation between observed data and theoretical concept. Our research aims to breach this gap in computational social science by proposing an unsupervised, parameter-free method to discover, with high accuracy, the fundamental domains of interaction occurring in social networks. By applying this method on two online datasets different by scope and type of interaction (aNobii and Flickr) we observe the spontaneous emergence of three domains of interaction representing the exchange of status, knowledge and social support. By finding significant relations between the domains of interaction and classic social network analysis issues (e.g., tie strength, dyadic interaction over time) we show how the network of interactions induced by the extracted domains can be used as a starting point for more nuanced analysis of online social data that may one day incorporate the normative grammar of social interaction. Our methods finds applications in online social media services ranging from recommendation to visual link summarization.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/websci14.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/websci14.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/websci14.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Reading the Source Code of Social Ties
WebSci
[u'Luca Maria Aiello', u'Rossano Schifanella', u'Bogdan State']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6942/parallel-peeling-algorithms
found
<h6>
 Abstract
</h6>

<p class="leading">
 The analysis of several algorithms and data structures can be framed as a peeling process on a random hypergraph: vertices with degree less than k are removed until there are no vertices of degree less than k left. The remaining hypergraph is known as the k-core. In this paper, we analyze parallel peeling processes, where in each round, all vertices of degree less than k are removed. It is known that, below a specific edge density threshold, the k-core is empty with high probability. We show that, with high probability, below this threshold, only (log log n)/log(k-1)(r-1) + O(1) rounds of peeling are needed to obtain the empty k-core for r-uniform hypergraphs. Interestingly, we show that above this threshold, Omega(log n) rounds of peeling are required to find the non-empty k-core. Since most algorithms and data structures aim to peel to an empty k-core, this asymmetry appears fortunate. We verify the theoretical results both with simulation and with a parallel implementation using graphical processing units (GPUs). Our implementation provides insights into how to structure parallel peeling algorithms for efficiency in practice.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SPAACRfull1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SPAACRfull1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SPAACRfull1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Parallel Peeling Algorithms
26th ACM Symposium on Parallelism in Algorithms and Architectures
[u'Jiayang Jiang', u'Michael Mitzenmacher', u'Justin Thaler']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6937/triad-distributed-shared-nothing-rdf-engine-based-asynchronous-message-passing
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We investigate a new approach to the design of distributed, shared-nothing RDF engines. Our engine, coined TriAD, combines join- ahead pruning via a novel form of RDF graph summarization with a locality-based, horizontal partitioning of RDF triples into a grid-like, distributed index structure. The multi-threaded and distributed execution of joins in TriAD is facilitated by an asynchronous Message Passing protocol which allows us to run multiple join operators along a query plan in a fully parallel, asynchronous fashion. We believe that our architecture provides a so far unique approach to join-ahead pruning in a distributed environment, as the more classical form of sideways information passing would not permit for executing distributed joins in an asynchronous way. Our experiments over the LUBM, BTC and WSDTS benchmarks demonstrate that TriAD consistently outperforms centralized RDF engines by up to two orders of magnitude, while gaining a factor of more than three compared to the currently fastest, distributed engines. To our knowledge, we are thus able to report the so far fastest query response times for the above benchmarks using a mid-range server and regular Ethernet setup.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We investigate a new approach to the design of distributed, shared-nothing RDF engines. Our engine, coined TriAD, combines join- ahead pruning via a novel form of RDF graph summarization with a locality-based, horizontal partitioning of RDF triples into a grid-like, distributed index structure. The multi-threaded and distributed execution of joins in TriAD is facilitated by an asynchronous Message Passing protocol which allows us to run multiple join operators along a query plan in a fully parallel, asynchronous fashion. We believe that our architecture provides a so far unique approach to join-ahead pruning in a distributed environment, as the more classical form of sideways information passing would not permit for executing distributed joins in an asynchronous way. Our experiments over the LUBM, BTC and WSDTS benchmarks demonstrate that TriAD consistently outperforms centralized RDF engines by up to two orders of magnitude, while gaining a factor of more than three compared to the currently fastest, distributed engines. To our knowledge, we are thus able to report the so far fastest query response times for the above benchmarks using a mid-range server and regular Ethernet setup.
  </div>
 </div>
</div>
<div>
 <div>
  We investigate a new approach to the design of distributed, shared-nothing RDF engines. Our engine, coined TriAD, combines join- ahead pruning via a novel form of RDF graph summarization with a locality-based, horizontal partitioning of RDF triples into a grid-like, distributed index structure. The multi-threaded and distributed execution of joins in TriAD is facilitated by an asynchronous Message Passing protocol which allows us to run multiple join operators along a query plan in a fully parallel, asynchronous fashion. We believe that our architecture provides a so far unique approach to join-ahead pruning in a distributed environment, as the more classical form of sideways information passing would not permit for executing distributed joins in an asynchronous way. Our experiments over the LUBM, BTC and WSDTS benchmarks demonstrate that TriAD consistently outperforms centralized RDF engines by up to two orders of magnitude, while gaining a factor of more than three compared to the currently fastest, distributed engines. To our knowledge, we are thus able to report the so far fastest query response times for the above benchmarks using a mid-range server and regular Ethernet setup.
 </div>
</div>

<div>
 We investigate a new approach to the design of distributed, shared-nothing RDF engines. Our engine, coined TriAD, combines join- ahead pruning via a novel form of RDF graph summarization with a locality-based, horizontal partitioning of RDF triples into a grid-like, distributed index structure. The multi-threaded and distributed execution of joins in TriAD is facilitated by an asynchronous Message Passing protocol which allows us to run multiple join operators along a query plan in a fully parallel, asynchronous fashion. We believe that our architecture provides a so far unique approach to join-ahead pruning in a distributed environment, as the more classical form of sideways information passing would not permit for executing distributed joins in an asynchronous way. Our experiments over the LUBM, BTC and WSDTS benchmarks demonstrate that TriAD consistently outperforms centralized RDF engines by up to two orders of magnitude, while gaining a factor of more than three compared to the currently fastest, distributed engines. To our knowledge, we are thus able to report the so far fastest query response times for the above benchmarks using a mid-range server and regular Ethernet setup.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/triad-sigmod2014.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/triad-sigmod2014.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/triad-sigmod2014.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
TriAD: A Distributed Shared-Nothing RDF Engine based on Asynchronous Message Passing
ACM SIGMOD International Conference on Management of Data (SIGMOD 2014)
[u'Sairam Gurajada', u'Stephan Seufert', u'Iris Miliaraki', u'Martin Theobald']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6647/reducing-dueling-bandits-cardinal-bandits
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form &quot;A is preferred to B'' (as opposed to cardinal feedback like &quot;A has value 2.5''), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. In addition we are the first to provide an almost optimal regret bound in terms of the second order terms such as the differences between the values of the arms.We present three such algorithms: Doubler, MultiSbm and Sparring. For Doubler and MultiSbm we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of Sparring which empirically outperforms the other two as well as previous algorithms in our experiments.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/dueling_bandits.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/dueling_bandits.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/dueling_bandits.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Reducing Dueling Bandits to Cardinal Bandits
icml 2014
[u'Zohar Karnin', u'Nir Ailon']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6648/hard-margin-active-linear-regression
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We consider the fundamental problem of linear regression in which the designer can actively choose observations. This model naturally captures various experiment design settings in medical experiments, ad placement problems, and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give efficient algorithms that attain near-optimal bounds.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We consider the fundamental problem of linear regression in which the designer can actively choose observations. This model naturally captures various experiment design settings in medical experiments, ad placement problems, and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give efficient algorithms that attain near-optimal bounds.
  </div>
 </div>
</div>
<div>
 <div>
  We consider the fundamental problem of linear regression in which the designer can actively choose observations. This model naturally captures various experiment design settings in medical experiments, ad placement problems, and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give efficient algorithms that attain near-optimal bounds.
 </div>
</div>

<div>
 We consider the fundamental problem of linear regression in which the designer can actively choose observations. This model naturally captures various experiment design settings in medical experiments, ad placement problems, and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give efficient algorithms that attain near-optimal bounds.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/activeRegression1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/activeRegression1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/activeRegression1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Hard-Margin Active Linear Regression
icml 2014
[u'Zohar Karnin']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6968/learning-mixtures-linear-classifiers
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a mirroring trick, that discovers the subspace spanned by the classifiers parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency
 </div>
</p>

<div dir="ltr">
 We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a mirroring trick, that discovers the subspace spanned by the classifiers parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SpectralMirror.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SpectralMirror.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/SpectralMirror.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Mixtures of Linear Classifiers
International Conference on Machine Learning (ICML)
[u'Yuekai Sun', u'Stratis Ioannidis', u'Andrea Montanari']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6650/pythia-diagnosing-performance-problems-wide-area-providers
found
<h6>
 Abstract
</h6>

<p class="leading">
 Performance problem diagnosis is a critical part of network operations in ISPs. Service providers typically deploy monitoring nodes at several vantage points in their network, to record end-to-end measurements of network performance. Network operators use these measurements offline; for example, to troubleshoot customer complaints. In this work, we leverage such monitoring infrastructure deployments in ISPs to build a system for near real-time performance problem detection and root cause diagnosis. Our system works with wide area interdomain monitoring, unlike approaches that require data sources from network devices (SNMP, Netflow, router logs, table dumps, etc.). Operators can input operational and domain knowledge of performance problems to the system to add diagnosis functionality. We have deployed the system on existing monitoring infrastructure in the US, diagnosing over 300 inter-domain paths. We study the extent and nature of performance problems that manifest in edge and core networks on the Internet.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/atc14-final183.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/atc14-final183.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/atc14-final183.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Pythia: Diagnosing Performance Problems in Wide Area Providers
USENIX ATC
[u'Partha Kanuparthy', u'Constantine Dovrolis']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=16
found
 LINK 
https://labs.yahoo.com/publications/6970/privacy-tradeoffs-predictive-analytics
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  Online services routinely mine user data to predict user preferences, make recommendations, and place targeted ads. Recent research has demonstrated that several private user attributes (such as political affiliation, sexual orientation, and gender) can be inferred from such data. Can a privacy-conscious user benefit from personalization while simultaneously protecting her private attributes? We study this question in the context of a rating prediction service based on matrix factorization. We construct a protocol of interactions between the service and users that has remarkable optimality properties: it is privacy-preserving, in that no inference algorithm can succeed in inferring a user's private attribute with a probability better than random guessing; it has maximal accuracy, in that no other privacy-preserving protocol improves rating prediction; and, finally, it involves a minimal disclosure, as the prediction accuracy strictly decreases when the service reveals less information. We extensively evaluate our protocol using several rating datasets, demonstrating that it successfully blocks the inference of gender, age and political affiliation, while incurring less than 5% decrease in the accuracy of rating prediction.
 </div>
</p>

<div dir="ltr">
 Online services routinely mine user data to predict user preferences, make recommendations, and place targeted ads. Recent research has demonstrated that several private user attributes (such as political affiliation, sexual orientation, and gender) can be inferred from such data. Can a privacy-conscious user benefit from personalization while simultaneously protecting her private attributes? We study this question in the context of a rating prediction service based on matrix factorization. We construct a protocol of interactions between the service and users that has remarkable optimality properties: it is privacy-preserving, in that no inference algorithm can succeed in inferring a user's private attribute with a probability better than random guessing; it has maximal accuracy, in that no other privacy-preserving protocol improves rating prediction; and, finally, it involves a minimal disclosure, as the prediction accuracy strictly decreases when the service reveals less information. We extensively evaluate our protocol using several rating datasets, demonstrating that it successfully blocks the inference of gender, age and political affiliation, while incurring less than 5% decrease in the accuracy of rating prediction.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ObfuscationSIGMETRICS_CR.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ObfuscationSIGMETRICS_CR.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ObfuscationSIGMETRICS_CR.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Privacy Tradeoffs in Predictive Analytics
International Conference on Measurements and Modeling of Computer Systems (SIGMETRICS)
[u'Stratis Ioannidis', u'Andrea Montanari', u'Udi Weinsberg', u'Smriti Bhagat', u'Nadia Fawaz', u'Nina Taft.']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5662/volumetric-spanners-efficient-exploration-basis-learning
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    <p>
     Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.
    </p>
    <p>
     We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.
    </p>
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   <p>
    Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.
   </p>
   <p>
    We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.
  </p>
  <p>
   We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.
  </p>
 </div>
</div>

<div>
 <p>
  Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.
 </p>
 <p>
  We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.
 </p>
</div>

<p>
 Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.
</p>

<p>
 We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/vspanner.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/vspanner.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/vspanner.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Volumetric Spanners: an Efficient Exploration Basis for Learning
COLT 2014
[u'Zohar Karnin']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6674/reconciling-transactional-and-non-transactional-operations-distributed-key-value
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
  </div>
 </div>
</div>
<div>
 <div>
  NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
 </div>
</div>

<div>
 NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Reconciling Transactional and Non-Transactional Operations in Distributed Key-Value Stores
Systor 2014
[u'Edward Bortnikov', u'Eshcar Hillel', u'Artyom Sharov']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6671/user-engagement-online-news-under-scope-sentiment-interest-affect-and-gaze
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Online content providers, such as news portals and social media platforms, constantly seek new ways to attract large shares of online attention by keeping their users engaged. A common challenge is to identify which aspects of online interaction influence user engagement the most. In this article, through an analysis of a news article collection obtained from Yahoo News US, we demonstrate that news articles exhibit considerable variation in terms of the sentimentality and polarity of their content, depending on factors such as news provider and genre. Moreover, through a laboratory study, we observe the effect of sentimentality and polarity of news and comments on a set of subjective and objective measures of engagement. In particular, we show that attention, affect, and gaze differ across news of varying interestingness. As part of our study, we also explore methods that exploit the sentiments expressed in user comments to reorder the lists of comments displayed in news pages. Our results indicate that user engagement can be anticipated if we account for the sentimentality and polarity of the content as well as other factors that drive attention and inspire human curiosity.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Online content providers, such as news portals and social media platforms, constantly seek new ways to attract large shares of online attention by keeping their users engaged. A common challenge is to identify which aspects of online interaction influence user engagement the most. In this article, through an analysis of a news article collection obtained from Yahoo News US, we demonstrate that news articles exhibit considerable variation in terms of the sentimentality and polarity of their content, depending on factors such as news provider and genre. Moreover, through a laboratory study, we observe the effect of sentimentality and polarity of news and comments on a set of subjective and objective measures of engagement. In particular, we show that attention, affect, and gaze differ across news of varying interestingness. As part of our study, we also explore methods that exploit the sentiments expressed in user comments to reorder the lists of comments displayed in news pages. Our results indicate that user engagement can be anticipated if we account for the sentimentality and polarity of the content as well as other factors that drive attention and inspire human curiosity.
  </div>
 </div>
</div>
<div>
 <div>
  Online content providers, such as news portals and social media platforms, constantly seek new ways to attract large shares of online attention by keeping their users engaged. A common challenge is to identify which aspects of online interaction influence user engagement the most. In this article, through an analysis of a news article collection obtained from Yahoo News US, we demonstrate that news articles exhibit considerable variation in terms of the sentimentality and polarity of their content, depending on factors such as news provider and genre. Moreover, through a laboratory study, we observe the effect of sentimentality and polarity of news and comments on a set of subjective and objective measures of engagement. In particular, we show that attention, affect, and gaze differ across news of varying interestingness. As part of our study, we also explore methods that exploit the sentiments expressed in user comments to reorder the lists of comments displayed in news pages. Our results indicate that user engagement can be anticipated if we account for the sentimentality and polarity of the content as well as other factors that drive attention and inspire human curiosity.
 </div>
</div>

<div>
 Online content providers, such as news portals and social media platforms, constantly seek new ways to attract large shares of online attention by keeping their users engaged. A common challenge is to identify which aspects of online interaction influence user engagement the most. In this article, through an analysis of a news article collection obtained from Yahoo News US, we demonstrate that news articles exhibit considerable variation in terms of the sentimentality and polarity of their content, depending on factors such as news provider and genre. Moreover, through a laboratory study, we observe the effect of sentimentality and polarity of news and comments on a set of subjective and objective measures of engagement. In particular, we show that attention, affect, and gaze differ across news of varying interestingness. As part of our study, we also explore methods that exploit the sentiments expressed in user comments to reorder the lists of comments displayed in news pages. Our results indicate that user engagement can be anticipated if we account for the sentimentality and polarity of the content as well as other factors that drive attention and inspire human curiosity.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/jasist.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/jasist.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/jasist.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
User Engagement in Online News: Under the Scope of Sentiment, Interest, Affect, and Gaze
Journal of the Association for Information Science and Technology
[u'Ioannis Arapakis', u'Mounia Lalmas', u'B. Barla Cambazoglu', u'Mari Carmen Marcos', u'Joemon M Jose']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6781/signaling-schemes-revenue-maximization
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Signaling is an important topic in the study of asymmetric information in economic settings. In particular, the transparency of information available to a seller in an auction setting is a question of major interest. We introduce the study of signaling when conducting a second price auction of a probabilistic good whose actual instantiation is known to the auctioneer but not to the bidders. This framework can be used to model impressions selling in display advertising. We establish several results within this framework. First, we study the problem of computing a signaling scheme that maximizes the auctioneers revenue in a Bayesian setting. We show that this problem is polynomially solvable for some interesting special cases, but computationally hard in general. Second, we establish a tight bound on the minimum number of signals required to implement an optimal signaling scheme. Finally, we show that at least half of the maximum social welfare can be preserved within such a scheme.
 </p>
</p>

<p>
 Signaling is an important topic in the study of asymmetric information in economic settings. In particular, the transparency of information available to a seller in an auction setting is a question of major interest. We introduce the study of signaling when conducting a second price auction of a probabilistic good whose actual instantiation is known to the auctioneer but not to the bidders. This framework can be used to model impressions selling in display advertising. We establish several results within this framework. First, we study the problem of computing a signaling scheme that maximizes the auctioneers revenue in a Bayesian setting. We show that this problem is polynomially solvable for some interesting special cases, but computationally hard in general. Second, we establish a tight bound on the minimum number of signals required to implement an optimal signaling scheme. Finally, we show that at least half of the maximum social welfare can be preserved within such a scheme.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Signaling-Schemes-for-Revenue-Maximization.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Signaling-Schemes-for-Revenue-Maximization.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Signaling-Schemes-for-Revenue-Maximization.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Signaling Schemes for Revenue Maximization
ACM Transactions on Economics and Computation (ACM TEAC)
[u'Iftah Gamzu', u'Yuval Emek', u'Michal Feldman', u'Renato Paes Leme', u'Moshe Tennenholtz']
Advertising Science
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6660/verta-facing-multilingual-experience-linguistically-based-mt-evaluation
found
<h6>
 Abstract
</h6>

<p class="leading">
 There are several MT metrics used to evaluate translation into Spanish, although most of them use partial or little linguistic information. In this paper we present the multilingual capability of VERTa, an automatic MT metric that combines linguistic information at lexical, morphological, syntactic and semantic level. In the experiments conducted we aim at identifying those linguistic features that prove the most effective to evaluate adequacy in Spanish segments. This linguistic information is tested both as independent modules (to observe what each type of feature provides) and in a combinatory fashion (where different kinds of information interact with each other). This allows us to extract the optimal combination. In addition we compare these linguistic features to those used in previous versions of VERTa aimed at evaluating adequacy for English segments. Finally, experiments show that VERTa can be easily adapted to other languages than English and that its collaborative approach correlates better with human judgements on adequacy than other well-known metrics.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1032_Paper.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1032_Paper.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/1032_Paper.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
VERTa: Facing a Multilingual Experience of a Linguistically-based MT Evaluation
Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&amp;amp;#39;14), Reykjavik, Iceland ISBN-978-2-9517408-8-4
[u'Jordi Atserias Batalla', u'Elisabet Comelles', u'Victoria Arranz', u'Irene Castellon', u'Jordi Sese']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6768/complexity-universally-composable-commitments
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  Motivated by applications to secure multiparty computation, we study the complexity of realizing universally composable (UC) commitments. Several recent works obtain practical UC commitment protocols in the common reference string (CRS) model under the DDH assumption. These protocols have two main disadvantages. First, even when applied to long messages, they can only achieve a small constant rate (namely, the communication complexity is larger than the length of the message by a large constant factor). Second, they require computationally expensive public-key operations for each block of each message being committed. Our main positive result is a UC commitment protocol that simultaneously avoids both of these limitations. It achieves an optimal rate of 1 (strictly speaking, 1 - o(1)) by making only few calls to an ideal oblivious transfer (OT) oracle and additionally making a black-box use of a (computationally inexpensive) PRG. By plugging in known efficient protocols for UC-secure OT, we get rate-1, computationally efficient UC commitment protocols under a variety of setup assumptions (including the CRS model) and under a variety of standard cryptographic assumptions (including DDH). We are not aware of any previous UC commitment protocols that achieve an optimal asymptotic rate. A corollary of our technique is a rate-1 construction  for UC commitment length extension, that is, a UC commitment protocol for a long message using a single ideal commitment for a short message. The extension protocol additionally requires the use of a semi-honest (stand-alone) OT protocol. This raises a natural question: can we achieve UC commitment length extension while using only inexpensive PRG operations as is the case for stand-alone commitments and UC OT? We answer this question in the negative, showing that the existence of a semi-honest OT protocol is necessary (and sufficient) for UC commitment length extension. This shows, quite surprisingly, that UC commitments are qualitatively different from both stand-alone commitments and UC OT.
 </p>
</p>

<p>
 Motivated by applications to secure multiparty computation, we study the complexity of realizing universally composable (UC) commitments. Several recent works obtain practical UC commitment protocols in the common reference string (CRS) model under the DDH assumption. These protocols have two main disadvantages. First, even when applied to long messages, they can only achieve a small constant rate (namely, the communication complexity is larger than the length of the message by a large constant factor). Second, they require computationally expensive public-key operations for each block of each message being committed. Our main positive result is a UC commitment protocol that simultaneously avoids both of these limitations. It achieves an optimal rate of 1 (strictly speaking, 1 - o(1)) by making only few calls to an ideal oblivious transfer (OT) oracle and additionally making a black-box use of a (computationally inexpensive) PRG. By plugging in known efficient protocols for UC-secure OT, we get rate-1, computationally efficient UC commitment protocols under a variety of setup assumptions (including the CRS model) and under a variety of standard cryptographic assumptions (including DDH). We are not aware of any previous UC commitment protocols that achieve an optimal asymptotic rate. A corollary of our technique is a rate-1 construction  for UC commitment length extension, that is, a UC commitment protocol for a long message using a single ideal commitment for a short message. The extension protocol additionally requires the use of a semi-honest (stand-alone) OT protocol. This raises a natural question: can we achieve UC commitment length extension while using only inexpensive PRG operations as is the case for stand-alone commitments and UC OT? We answer this question in the negative, showing that the existence of a semi-honest OT protocol is necessary (and sufficient) for UC commitment length extension. This shows, quite surprisingly, that UC commitments are qualitatively different from both stand-alone commitments and UC OT.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MAIN.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MAIN.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/MAIN.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
On the Complexity of Universally Composable Commitments
The 33rd Annual International Conference on the Theory and Applications of Cryptographic Techniques (Eurocrypt 2014)
[u'Juan A. Garay', u'Yuval Ishai', u'Ranjit Kumaresan', u'Hoeteck Wee']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6955/do-not-crawl-dust-different-urls-similar-text
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We consider the problem of dust: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering dust; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines dust effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from information about dust to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We consider the problem of dust: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering dust; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines dust effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from information about dust to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.
  </div>
 </div>
</div>
<div>
 <div>
  We consider the problem of dust: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering dust; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines dust effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from information about dust to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.
 </div>
</div>

<div>
 We consider the problem of dust: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering dust; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines dust effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from information about dust to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper1941.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper1941.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper1941.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Do Not Crawl in the DUST: Different URLs with Similar Text
16th International World Wide Web Conference (WWW2007)
[u'Ziv Bar Yossef', u'Idit Keidar', u'Uri Schonfeld']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6957/non-interactive-secure-computation-based-cut-and-choose
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  In recent years, secure two-party computation (2PC) has been demonstrated to be feasible in practice. However, all efficient general-computation 2PC protocols require multiple rounds of interaction between the two players. This property restricts 2PC to be only relevant to scenarios where both players can be simultaneously online, and where communication latency is not an issue. This work considers the model of 2PC with a {\em single} round of interaction, called \emph{Non-Interactive Secure Computation (NISC)}. In addition to the non-interaction property, we also consider a flavor of NISC that allows reusing the first message for many different 2PC invocations, possibly with different players acting as the player who sends the second message, similar to a public-key encryption where a single public-key can be used to encrypt many different messages. We present a NISC protocol that is based on the cut-and-choose paradigm of Lindell and Pinkas (Eurocrypt 2007). This protocol achieves concrete efficiency similar to that of best multi-round 2PC protocols based on the cut-and-choose paradigm. The protocol requires only $t$ garbled circuits for achieving cheating probability of $2^{-t}$, similar to the recent result of Lindell (Crypto 2013), but only needs a single round of interaction. To validate the efficiency of our protocol, we provide a prototype implementation of it and show experiments that confirm its competitiveness with that of the best multi-round 2PC protocols. This is the \emph{first} prototype implementation of an efficient NISC protocol. In addition to our NISC protocol, we introduce a new encoding technique that significantly reduces communication in the NISC setting. We further show how our NISC protocol can be improved in the multi-round setting, resulting in a highly efficient constant-round 2PC that is also suitable for pipelined implementation.
 </p>
</p>

<p>
 In recent years, secure two-party computation (2PC) has been demonstrated to be feasible in practice. However, all efficient general-computation 2PC protocols require multiple rounds of interaction between the two players. This property restricts 2PC to be only relevant to scenarios where both players can be simultaneously online, and where communication latency is not an issue. This work considers the model of 2PC with a {\em single} round of interaction, called \emph{Non-Interactive Secure Computation (NISC)}. In addition to the non-interaction property, we also consider a flavor of NISC that allows reusing the first message for many different 2PC invocations, possibly with different players acting as the player who sends the second message, similar to a public-key encryption where a single public-key can be used to encrypt many different messages. We present a NISC protocol that is based on the cut-and-choose paradigm of Lindell and Pinkas (Eurocrypt 2007). This protocol achieves concrete efficiency similar to that of best multi-round 2PC protocols based on the cut-and-choose paradigm. The protocol requires only $t$ garbled circuits for achieving cheating probability of $2^{-t}$, similar to the recent result of Lindell (Crypto 2013), but only needs a single round of interaction. To validate the efficiency of our protocol, we provide a prototype implementation of it and show experiments that confirm its competitiveness with that of the best multi-round 2PC protocols. This is the \emph{first} prototype implementation of an efficient NISC protocol. In addition to our NISC protocol, we introduce a new encoding technique that significantly reduces communication in the NISC setting. We further show how our NISC protocol can be improved in the multi-round setting, resulting in a highly efficient constant-round 2PC that is also suitable for pipelined implementation.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ec_main.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ec_main.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/ec_main.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Non-Interactive Secure Computation Based on Cut-and-Choose
Advances in Cryptology--EUROCRYPT (EUROCRYPT 2014)
[u'Arash Afshar', u'Benny Pinkas', u'Payman Mohassel', u'Ben Riva']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6640/faces-engage-us-photos-faces-attract-more-likes-and-comments-instagram
found
<h6>
 Abstract
</h6>

<p class="leading">
 Photos are becoming prominent means of communication online. Despite photos pervasive presence in social media and online world, we know little about how people interact and engage with their content. Understanding how photo content might signify engagement, can impact both science and design, influencing production and distribution. One common type of photo content that is shared on social media, is the photos of people. From studies of offline behavior, we know that human faces are powerful channels of non-verbal communication. In this paper, we study this behavioral phenomena online. We ask how presence of a face, its age and gender might impact social engagement on the photo. We use a corpus of 1 million Instagram images and organize our study around two social engagement feedback factors, likes and comments. Our results show that photos with faces are 38% more likely to receive likes and 32% more likely to receive comments, even after controlling for social network reach and activity. We find, however, that the number of faces, their age and gender do not have an effect. This work presents the first results on how photos with human faces relate to engagement on large scale image sharing communities. In addition to contributing to the research around online user behavior, our findings offer a new line of future work using visual analysis.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/faces-bakhshi.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/faces-bakhshi.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/faces-bakhshi.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Faces Engage Us: Photos with Faces Attract More Likes and Comments on Instagram
CHI
[u'Saeideh Bakhshi', u'David Ayman Shamma', u'Eric Gilbert']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=17
found
 LINK 
https://labs.yahoo.com/publications/6669/interaction-techniques-co-located-collaborative-tv
found
<h6>
 Abstract
</h6>

<p class="leading">
 We propose a number of interaction techniques allowing TV viewers to use their mobile phones to view and share content with others in the room, thus supporting local social interaction. Based on a preliminary evaluation, we provide guidelines for designing interactions to support co-located collaborative TV viewing.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p1819-buchner.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p1819-buchner.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p1819-buchner.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Interaction Techniques for Co-located Collaborative TV
CHI
[u'Karolina Buchner', u'Roman Lissermann', u'Lars Erik Holmquist']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6638/money-talks-tracking-personal-finances
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  How do people keep track of their money? In this paper we present a preliminary scoping study of how 14 individuals in the San Francisco Bay Area earn, save, spend and understand money and their personal and family finances. We describe the practices we developed for exploring the sensitive topic of money, and then discuss three sets of findings. The first is the emotional component of the relationship people have with their finances. Second, we discuss the tools and processes people used to keep track of their financial situation. Finally we discuss how people account for the unknown and unpredictable nature of the future through their financial decisions. We conclude by discussing the future of studies of money and finance in HCI, and reflect on the opportunities for improving tools to aid people in managing and planning their finances.
 </div>
</p>

<div title="Page 1">
 How do people keep track of their money? In this paper we present a preliminary scoping study of how 14 individuals in the San Francisco Bay Area earn, save, spend and understand money and their personal and family finances. We describe the practices we developed for exploring the sensitive topic of money, and then discuss three sets of findings. The first is the emotional component of the relationship people have with their finances. Second, we discuss the tools and processes people used to keep track of their financial situation. Finally we discuss how people account for the unknown and unpredictable nature of the future through their financial decisions. We conclude by discussing the future of studies of money and finance in HCI, and reflect on the opportunities for improving tools to aid people in managing and planning their finances.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Money-Talks-Tracking-Personal-Finances.-Kaye-et-al-Proc-CHI-20161.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Money-Talks-Tracking-Personal-Finances.-Kaye-et-al-Proc-CHI-20161.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Money-Talks-Tracking-Personal-Finances.-Kaye-et-al-Proc-CHI-20161.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Money Talks: Tracking Personal Finances
Proc. CHI 2014
[u'Jofish Kaye', u'David Ayman Shamma', u'Mary Mccuistion', u'Rebecca Gulotta']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6649/influence-maximization-viral-product-design
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Product design and viral marketing are two popular concepts in the marketing literature that, although following different paths, aim at the same goal: maximizing the adoption of a new product. While the effect of the social network is nowadays kept in great consideration in any marketing- related activity, the interplay between product design and social influence is surprisingly still largely unexplored.

In this paper we move a first step in this direction and study the problem of designing the features of a novel product such that its adoption, fueled by peer influence and word-of-mouth effect, is maximized. We model the viral process of product adoption on the basis of social influence and the features of the product, and devise an improved iterative scaling procedure to learn the parameters that maximize the likelihood of our novel feature-aware propagation model. In order to design an effective algorithm for our problem, we study the property of the underlying propagation model. In particular we show that the expected spread, i.e., the objective function to maximize, is monotone and submodular when we fix the features of the product and seek for the set of users to target in the viral marketing campaign. Instead, when we fix the set of users and try to find the optimal features for the product, then the expected spread is neither submodular nor monotone (as it is the case, in general, for product design). Therefore, we develop an algorithm based on an alternating optimization between selecting the features of the product, and the set of users to target in the campaign. Our experimental evaluation on real-world data from the domain of social music consumption (LastFM) and social movie consumption (Flixster) confirms the effectiveness of the proposed framework in integrating product design in viral marketing.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Product design and viral marketing are two popular concepts in the marketing literature that, although following different paths, aim at the same goal: maximizing the adoption of a new product. While the effect of the social network is nowadays kept in great consideration in any marketing- related activity, the interplay between product design and social influence is surprisingly still largely unexplored.

In this paper we move a first step in this direction and study the problem of designing the features of a novel product such that its adoption, fueled by peer influence and word-of-mouth effect, is maximized. We model the viral process of product adoption on the basis of social influence and the features of the product, and devise an improved iterative scaling procedure to learn the parameters that maximize the likelihood of our novel feature-aware propagation model. In order to design an effective algorithm for our problem, we study the property of the underlying propagation model. In particular we show that the expected spread, i.e., the objective function to maximize, is monotone and submodular when we fix the features of the product and seek for the set of users to target in the viral marketing campaign. Instead, when we fix the set of users and try to find the optimal features for the product, then the expected spread is neither submodular nor monotone (as it is the case, in general, for product design). Therefore, we develop an algorithm based on an alternating optimization between selecting the features of the product, and the set of users to target in the campaign. Our experimental evaluation on real-world data from the domain of social music consumption (LastFM) and social movie consumption (Flixster) confirms the effectiveness of the proposed framework in integrating product design in viral marketing.
  </div>
 </div>
</div>
<div>
 <div>
  Product design and viral marketing are two popular concepts in the marketing literature that, although following different paths, aim at the same goal: maximizing the adoption of a new product. While the effect of the social network is nowadays kept in great consideration in any marketing- related activity, the interplay between product design and social influence is surprisingly still largely unexplored.

In this paper we move a first step in this direction and study the problem of designing the features of a novel product such that its adoption, fueled by peer influence and word-of-mouth effect, is maximized. We model the viral process of product adoption on the basis of social influence and the features of the product, and devise an improved iterative scaling procedure to learn the parameters that maximize the likelihood of our novel feature-aware propagation model. In order to design an effective algorithm for our problem, we study the property of the underlying propagation model. In particular we show that the expected spread, i.e., the objective function to maximize, is monotone and submodular when we fix the features of the product and seek for the set of users to target in the viral marketing campaign. Instead, when we fix the set of users and try to find the optimal features for the product, then the expected spread is neither submodular nor monotone (as it is the case, in general, for product design). Therefore, we develop an algorithm based on an alternating optimization between selecting the features of the product, and the set of users to target in the campaign. Our experimental evaluation on real-world data from the domain of social music consumption (LastFM) and social movie consumption (Flixster) confirms the effectiveness of the proposed framework in integrating product design in viral marketing.
 </div>
</div>

<div>
 Product design and viral marketing are two popular concepts in the marketing literature that, although following different paths, aim at the same goal: maximizing the adoption of a new product. While the effect of the social network is nowadays kept in great consideration in any marketing- related activity, the interplay between product design and social influence is surprisingly still largely unexplored.

In this paper we move a first step in this direction and study the problem of designing the features of a novel product such that its adoption, fueled by peer influence and word-of-mouth effect, is maximized. We model the viral process of product adoption on the basis of social influence and the features of the product, and devise an improved iterative scaling procedure to learn the parameters that maximize the likelihood of our novel feature-aware propagation model. In order to design an effective algorithm for our problem, we study the property of the underlying propagation model. In particular we show that the expected spread, i.e., the objective function to maximize, is monotone and submodular when we fix the features of the product and seek for the set of users to target in the viral marketing campaign. Instead, when we fix the set of users and try to find the optimal features for the product, then the expected spread is neither submodular nor monotone (as it is the case, in general, for product design). Therefore, we develop an algorithm based on an alternating optimization between selecting the features of the product, and the set of users to target in the campaign. Our experimental evaluation on real-world data from the domain of social music consumption (LastFM) and social movie consumption (Flixster) confirms the effectiveness of the proposed framework in integrating product design in viral marketing.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/how-to-design-SDM141.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/how-to-design-SDM141.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/how-to-design-SDM141.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Influence Maximization with Viral Product Design
SIAM International Conference on Data Mining (SDM)
[u'Nicola Barbieri', u'Francesco Bonchi']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6783/influence-maximization-viral-product-design-sdm-2014
found
<h6>
 Abstract
</h6>

<p class="leading">
 <a href="http://www.slideshare.net/NicolaBarbieri/influence-maximization-with-viral-product-design" rel="nofollow" target="_blank">
  Influence maximization with viral product design
 </a>
 Presentation at Siam International Conference on Data Mining - Philadelphia 24 April 2014.

 

[slideshare id=33895226&doc=sdm2014barbieri-140424072755-phpapp02]
</p>

<a href="http://www.slideshare.net/NicolaBarbieri/influence-maximization-with-viral-product-design" rel="nofollow" target="_blank">
 Influence maximization with viral product design
</a>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Influence Maximization with Viral Product Design - SDM 2014

[u'Nicola Barbieri', u'Francesco Bonchi']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6658/large-scale-multi-label-learning-incomplete-label-assignments
found
<h6>
 Abstract
</h6>

<p class="leading">
 Multi-label learning deals with the classification problems where each instance can be assigned with multiplelabels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly,that the label sets for training instances are fully labeled without any missing labels. However, in manyreal-world multi-label datasets, the label assignmentsfor training instances can be incomplete. Some ground-truth labels can be missed by the labeler from the labelset. This problem is especially typical when the number of instances is very large, and the labeling cost is veryhigh, which makes it almost impossible to get a fully-labeled training set. In this paper, we study the problemof large-scale multi-label learning with incomplete label assignments. We propose an approach, called Mpu,based upon positive and unlabeled stochastic gradientdescent and stacked models.Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable withlinear time complexities over the size of the data. Extensive experiments on two real-worldmulti-label datasets show that our Mpu model consistently outperform other commonly-used baselines.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Large-Scale Multi-Label Learning with Incomplete Label Assignments
SIAM International Conference on Data Mining
[u'Jia Li', u'Xiangnan Kong', u'Zhaoming Wu', u'Ruofei Zhang', u'Philip S Yu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6635/cluster-canonical-correlation-analysis
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this paper we present cluster canonical correlation analysis (cluster-CCA) for joint dimensionality reduction of two sets of data points. Unlike the standard pairwise correspondence between the data points, in our problem each set is partitioned into multiple clusters or classes, where the class labels define correspondences between the sets. Cluster-CCA is able to learn discriminant low dimensional representations that maximize the correlation between the two sets while segregating the different classes on the learned space. Furthermore, we present a kernel extension, kernel cluster canonical correlation analysis (cluster-KCCA) that extends cluster-CCA to account for non-linear relationships. Cluster-(K)CCA is shown to be computationally efficient, the complexity being similar to standard (K)CCA. By means of experimental evaluation on benchmark datasets, cluster-(K)CCA is shown to achieve state of the art performance for cross-modal retrieval tasks.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/120.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/120.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/120.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Cluster Canonical Correlation Analysis
International Conference on Artificial Intelligence and Statistics
[u'Nikhil Rasiwasia', u'Vijay Mahadevan', u'Gaurav Aggarwal', u'D Mahajan']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6965/linear-time-training-nonlinear-low-dimensional-embeddings
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the O(N log(N)) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours runtime.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the O(N log(N)) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours runtime.
  </div>
 </div>
</div>
<div>
 <div>
  Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the O(N log(N)) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours runtime.
 </div>
</div>

<div>
 Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the O(N log(N)) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours runtime.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/aistats14b.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/aistats14b.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/aistats14b.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Linear-time Training of Nonlinear Low-Dimensional Embeddings
Proc. of the 17th Int. Workshop on Artificial Intelligence and Statistics (AISTATS 2014)
[u'Max Vladymyrov', u'Miguel . Carreira Perpin']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6645/reconciling-transactional-and-non-transactional-operations-distributed-key-value
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
  </div>
 </div>
</div>
<div>
 <div>
  NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
 </div>
</div>

<div>
 NoSQL databases were initially designed to provide extreme scalability and availability for Internet applications, often at the expense of data consistency. The recent generation of Web-scale databases fills this gap, by offering transaction support. However, transaction processing implies a significant performance overhead on online applications that only require atomic reads and writes. The state-of-the-art solutions are either static separation of the data accessed by transaction-enabled and native applications, or complete transactification of the latter, which are both inadequate.

We present a scalable transaction processor, Mediator, that enjoys the best of both worlds. It preserves the latencies of atomic reads and writes, without compromising data safety. We introduce a lightweight synchronization protocol that enables conflict resolution between transactions and native operations that share data in a distributed database. We evaluate Mediators implementation on top of the HBase key-value store on a large-scale testbed, and show that it substantially outperforms the traditional approach on a vast majority of mixed workloads. In particular, Mediator achieves a significantly larger throughput for all workloads in which the fraction of native operations exceeds 50%.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mediator1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Reconciling Transactional and Non-Transactional Operations in Distributed Key-Value Stores
Systor 2014
[u'Edward Bortnikov', u'Eshcar Hillel', u'Artyom Sharov']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6642/hierarchical-dirichlet-model-taxonomy-expansion-search-engines
found
<h6>
 Abstract
</h6>

<p class="leading">
 Emerging trends and products pose a challenge to modern search engines since they must adapt to the constantly changing needs and interests of users. For example, vertical search engines, such as Amazon, eBay, Walmart, Yelp and Yahoo Local, provide business category hierarchies forpeople to navigate through millions of business listings. The category information also provides important ranking features that can be used to improve search experience. However, category hierarchies are often manually crafted by some human experts and they are far from complete. Manually constructed category hierarchies cannot handle the everchanging and sometimes long-tail user information needs.

In this paper, we study the problem of how to expand an existing category hierarchy for a search/navigation system to accommodate the information needs of users more comprehensively. We propose a general framework for this task, which has three steps: 1) detecting meaningful missing categories; 2) modeling the category hierarchy using a hierarchical Dirichlet model and predicting the optimal tree structure according to the model; 3) reorganizing the corpus using the complete category structure, i.e., associating each webpage with the relevant categories from the complete category hierarchy.Experimental results demonstrate that our proposed framework generates a high-quality category hierarchy and significantly boosts the retrieval performance.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Hierarchical Dirichlet Model for Taxonomy Expansion for Search Engines
WWW 2014
[u'Changsung Kang', u'Yi Chang', u'Jingjing Wang', u'Jiawei Han']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6673/short-text-categorization-using-diffusion-wavelets
found
<h6>
 Abstract
</h6>

<p class="leading">
 Usual text document representations such as tf-idf do not work well in classification tasks for short-text documents and across diverse data domains. Optimizing different representations for different data domains is infeasible in a practical setting on the Internet. Mining such representations from the data in an unsupervised manner is desirable. In this paper, we study a representation based on the multi-scale harmonic analysis of term-term co-occurrence graph. This representation is not only sparse, but also leads to the discovery of semantically coherent topics in data. In our experiments on user-generated short documents e.g., newsgroup messages, user comments, and meta-data, we found this representation to outperform other representations across different choice of classifiers. Similar improvements were also observed for data sets in Chinese and Portuguese languages.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14Poster.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14Poster.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14Poster.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Short-Text Categorization using Diffusion Wavelets
WWW 2014
[u'Vidit Jain', u'Jay Mahadeokar']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=18
found
 LINK 
https://labs.yahoo.com/publications/6642/hierarchical-dirichlet-model-taxonomy-expansion-search-engines
found
<h6>
 Abstract
</h6>

<p class="leading">
 Emerging trends and products pose a challenge to modern search engines since they must adapt to the constantly changing needs and interests of users. For example, vertical search engines, such as Amazon, eBay, Walmart, Yelp and Yahoo Local, provide business category hierarchies forpeople to navigate through millions of business listings. The category information also provides important ranking features that can be used to improve search experience. However, category hierarchies are often manually crafted by some human experts and they are far from complete. Manually constructed category hierarchies cannot handle the everchanging and sometimes long-tail user information needs.

In this paper, we study the problem of how to expand an existing category hierarchy for a search/navigation system to accommodate the information needs of users more comprehensively. We propose a general framework for this task, which has three steps: 1) detecting meaningful missing categories; 2) modeling the category hierarchy using a hierarchical Dirichlet model and predicting the optimal tree structure according to the model; 3) reorganizing the corpus using the complete category structure, i.e., associating each webpage with the relevant categories from the complete category hierarchy.Experimental results demonstrate that our proposed framework generates a high-quality category hierarchy and significantly boosts the retrieval performance.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/WWW14_Local.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Hierarchical Dirichlet Model for Taxonomy Expansion for Search Engines
WWW 2014
[u'Changsung Kang', u'Yi Chang', u'Jingjing Wang', u'Jiawei Han']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5664/time-based-collective-factorization-topic-discovery-and-monitoring-news
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
  </div>
 </div>
</div>
<div>
 <div>
  Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
 </div>
</div>

<div>
 Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Time-based Collective Factorization for Topic Discovery and Monitoring in News.
WWW 2014
[u'Amin Mantrach', u'Alejandro Jaimes', u'Carmen Vaca', u'Marco Saerens']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6938/quite-mess-my-cookie-jar-leveraging-machine-learning-protect-web-authentication
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  Browser-based defenses have recently been advocated as an effective mechanism to protect web applications against thethreats of session hijacking, fixation, and related attacks.In existing approaches, all such defenses ultimately rely onclient-side heuristics to automatically detect cookies containing session information, to then protect them againsttheft or otherwise unintended use. While clearly crucial tothe effectiveness of the resulting defense mechanisms, theseheuristics have not, as yet, undergone any rigorous assessment of their adequacy. In this paper, we conduct the firstsuch formal assessment, based on a gold set of cookies we collect from 70 popular websites of the Alexa ranking. To obtain the gold set, we devise a semi-automatic procedure thatdraws on a novel notion ofauthentication token, which we introduce to capture multiple web authentication schemes. Wetest existing browser-based defenses in the literature againstour gold set, unveiling several pitfalls both in the heuristics adopted and in the methods used to assess them. Wethen propose a new detection method based onsupervisedlearning, where our gold set is used to train a binary classifier, and report on experimental evidence that our methodoutperforms existing proposals. Interestingly, the resultingclassication, together with our hands-on experience in theconstruction of the gold set, provides new insight on howweb authentication is implemented in practice.
 </div>
</p>

<div dir="ltr">
 Browser-based defenses have recently been advocated as an effective mechanism to protect web applications against thethreats of session hijacking, fixation, and related attacks.In existing approaches, all such defenses ultimately rely onclient-side heuristics to automatically detect cookies containing session information, to then protect them againsttheft or otherwise unintended use. While clearly crucial tothe effectiveness of the resulting defense mechanisms, theseheuristics have not, as yet, undergone any rigorous assessment of their adequacy. In this paper, we conduct the firstsuch formal assessment, based on a gold set of cookies we collect from 70 popular websites of the Alexa ranking. To obtain the gold set, we devise a semi-automatic procedure thatdraws on a novel notion ofauthentication token, which we introduce to capture multiple web authentication schemes. Wetest existing browser-based defenses in the literature againstour gold set, unveiling several pitfalls both in the heuristics adopted and in the methods used to assess them. Wethen propose a new detection method based onsupervisedlearning, where our gold set is used to train a binary classifier, and report on experimental evidence that our methodoutperforms existing proposals. Interestingly, the resultingclassication, together with our hands-on experience in theconstruction of the gold set, provides new insight on howweb authentication is implemented in practice.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2014-tolomei-cookie.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2014-tolomei-cookie.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/www2014-tolomei-cookie.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Quite a Mess in My Cookie Jar! Leveraging Machine Learning to Protect Web Authentication
WWW 2014
[u'Stefano Calzavara', u'Gabriele Tolomei', u'Michele Bugliesi', u'Salvatore Orlando']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6629/generating-ad-targeting-rules-using-sparse-principal-component-analysis
found
<h6>
 Abstract
</h6>

<p class="leading">
 <style type="text/css">
  <!--
p, li {white-space:pre-wrap;}
-->
 </style>
 Determining the right audience for an advertising campaign is a well-established problem of central importance to many Internet companies. Two distinct targeting approaches exist, the model-based approach, which leverages machine learning, and the rule-based approach, which relies on manual generation of targeting rules. Common rules include identifying users that had interactions (website visits, emails received, etc.) with the companies related to the advertiser, or search queries related to their product. We consider a problem of discovering such rules from data using Constrained Sparse PCA. The constraints are put in place to account for cases when evidence in data suggests a relation that is not appropriate for advertising. Experiments on real-world data indicate the potential of the proposed approach.
</p>

<style type="text/css">
 <!--
p, li {white-space:pre-wrap;}
-->
</style>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic_www.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic_www.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/grbovic_www.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Generating Ad Targeting Rules using Sparse Principal Component Analysis with Constraints
23rd International World Wide Web Conference, WWW 14
[u'Mihajlo Grbovic', u'Slobodan Vucetic']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6948/checking-linearizability-encapsulated-extended-operations
found
<h6>
 Abstract
</h6>

<p class="leading">
 Linearizable objects (data-structures) provide operations that appear to execute atomically. Modern mainstream languages provide many linearizable data-structures, simplifying concurrent programming. In practice, however, programmers often find a need to execute a sequence of operations (on linearizable objects) that executes atomically and writeextended operationsfor this purpose. Such extended operations are a common source of atomicity bugs.

This paper focuses on the problem of verifying that a set of extension operations (to a linearizable library) are themselves linearizable. We present several reduction theorems that simplify this verification problem enabling more efficient verification.

We first introduce the notion of anencapsulated extension: this is an extension that (a) does not introduce new shared state (beyond the shared state in the base linearizable library), and (b) accesses or modifies the shared state only through the base operations. We show that encapsulated extensions are widely prevalent in real applications.

We show that linearizability of encapsulated extended operations can be verified by considering only histories with one occurrence of an extended operation, interleaved with atomic occurrences of base and extended operations. As a consequence, this verification needs to consider only histories with two threads, whereas general linearizability verification requires considering histories with an unbounded number of threads.

We show that when the operations satisfy certain properties, each extended operation can be verified independently of the others, enabling further reductions.

We have implemented a simple static analysis algorithm that conservatively verifies linearizabilty of encapsulated extensions of Java concurrent maps. We present empirical results illustrating the benefits of the reduction theorems.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/84100311.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/84100311.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/84100311.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Checking Linearizability of Encapsulated Extended Operations
ESOP 2014
[u'Oren Zomer', u'Guy Golan Gueta', u'G. Ramalingam', u'Mooly Sagiv']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6721/time-based-collective-factorization-topic-discovery-and-monitoring-news
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
  </div>
 </div>
</div>
<div>
 <div>
  Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
 </div>
</div>

<div>
 Discovering and tracking topic shifts in news constitutes a new challenge for applications nowadays. Topics evolve, emerge and fade, making it more difficult for the journal- ist  or the press consumer  to decrypt the news. For in- stance, the current Syrian chemical crisis has been the start- ing point of the UN Russian initiative and also the revival of the US France alliance. A topical mapping representing how the topics evolve in time would be helpful to contex- tualize information. As far as we know, few topic tracking systems can provide such temporal topic connections. In this paper, we introduce a novel framework inspired from Collective Factorization for online topic discovery able to connect topics between different time-slots. The framework learns jointly the topics evolution and their time dependen- cies. It offers the user the ability to control, through one unique hyper-parameter, the tradeoff between the past ac- cumulated knowledge and the current observed data. We show, on semi-synthetic datasets and on Yahoo News arti- cles, that our method is competitive with state-of-the-art techniques while providing a simple way to monitor topics evolution (including emerging and disappearing topics).
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
A Time-based Collective Factorization for Topic Discovery and Monitoring in News.
WWW 2014
[u'Amin Mantrach', u'Alejandro Jaimes', u'Carmen Vaca', u'Marco Saerens']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6720/random-walks-based-modularity-application-semi-supervised-learning
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Although criticized for some of its limitations, modularity remains a standard measure for analyzing social networks. Quantifying the statistical surprise in the arrangement of the edges of the network has led to simple and powerful algorithms. However, relying solely on the distribution of edges instead of more complex structures such as paths lim- its the extent of modularity. Indeed, recent studies have shown restrictions of optimizing modularity, for instance its resolution limit. We introduce here a novel, formal and well- defined modularity measure based on random walks. We show how this modularity can be computed from paths in- duced by the graph instead of the traditionally used edges. We argue that by computing modularity on paths instead of edges, more informative features can be extracted from the network. We verify this hypothesis on a semi-supervised classification procedure of the nodes in the network, where we show that, under the same settings, the features of the random walk modularity help to classify better than the features of the usual modularity. Additionally, the proposed approach outperforms the classical label propagation proce- dure on two data sets of labeled social networks.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Although criticized for some of its limitations, modularity remains a standard measure for analyzing social networks. Quantifying the statistical surprise in the arrangement of the edges of the network has led to simple and powerful algorithms. However, relying solely on the distribution of edges instead of more complex structures such as paths lim- its the extent of modularity. Indeed, recent studies have shown restrictions of optimizing modularity, for instance its resolution limit. We introduce here a novel, formal and well- defined modularity measure based on random walks. We show how this modularity can be computed from paths in- duced by the graph instead of the traditionally used edges. We argue that by computing modularity on paths instead of edges, more informative features can be extracted from the network. We verify this hypothesis on a semi-supervised classification procedure of the nodes in the network, where we show that, under the same settings, the features of the random walk modularity help to classify better than the features of the usual modularity. Additionally, the proposed approach outperforms the classical label propagation proce- dure on two data sets of labeled social networks.
  </div>
 </div>
</div>
<div>
 <div>
  Although criticized for some of its limitations, modularity remains a standard measure for analyzing social networks. Quantifying the statistical surprise in the arrangement of the edges of the network has led to simple and powerful algorithms. However, relying solely on the distribution of edges instead of more complex structures such as paths lim- its the extent of modularity. Indeed, recent studies have shown restrictions of optimizing modularity, for instance its resolution limit. We introduce here a novel, formal and well- defined modularity measure based on random walks. We show how this modularity can be computed from paths in- duced by the graph instead of the traditionally used edges. We argue that by computing modularity on paths instead of edges, more informative features can be extracted from the network. We verify this hypothesis on a semi-supervised classification procedure of the nodes in the network, where we show that, under the same settings, the features of the random walk modularity help to classify better than the features of the usual modularity. Additionally, the proposed approach outperforms the classical label propagation proce- dure on two data sets of labeled social networks.
 </div>
</div>

<div>
 Although criticized for some of its limitations, modularity remains a standard measure for analyzing social networks. Quantifying the statistical surprise in the arrangement of the edges of the network has led to simple and powerful algorithms. However, relying solely on the distribution of edges instead of more complex structures such as paths lim- its the extent of modularity. Indeed, recent studies have shown restrictions of optimizing modularity, for instance its resolution limit. We introduce here a novel, formal and well- defined modularity measure based on random walks. We show how this modularity can be computed from paths in- duced by the graph instead of the traditionally used edges. We argue that by computing modularity on paths instead of edges, more informative features can be extracted from the network. We verify this hypothesis on a semi-supervised classification procedure of the nodes in the network, where we show that, under the same settings, the features of the random walk modularity help to classify better than the features of the usual modularity. Additionally, the proposed approach outperforms the classical label propagation proce- dure on two data sets of labeled social networks.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/fp247-devooghtA1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Random Walks Based Modularity: Application to Semi-Supervised Learning
WWW
[u'Amin Mantrach', u'Alejandro Jaimes', u'Robin Devooght', u'Ilkka Kivimaki', u'Hugues Bersini', u'Marco Saerens']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6643/what-tumblr-statistical-overview-and-comparison
found
<h6>
 Abstract
</h6>

<p class="leading">
 Tumblr, as one of the most popular microblogging platforms, has gained momentum recently. It is reported to have 166.4 million users and 73.4 billion posts as of January 2014. While many articles about Tumblr have been published in major press, there is not much scholarly work so far. In this paper, we provide some pioneering analysis on Tumblr from a variety of aspects. We study the social network structure among Tumblr users, analyze its user-generated content, and describe reblogging patterns to analyze its user behavior. We aim to provide a comprehensive statistical overview of Tumblr and compare it with other popular social services, including the blogosphere, Twitter, and Facebook, in answering a couple of keyquestions: What is Tumblr? How is Tumblr different from other social media networks? In short, we find Tumblr has more rich content than other microblogging platforms, and it contains hybrid characteristics of social networking, the traditional blogosphere, and social media. This work serves as an early snapshot of Tumblr that later work can leverage.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TR_tumblr.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TR_tumblr.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/TR_tumblr.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
What is Tumblr: A Statistical Overview and Comparison

[u'Yi Chang', u'Yoshi Inagaki', u'Lei Tang', u'Yan Liu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6657/learning-mid-level-features-object-hierarchy-image-classi%EF%AC%81cation
found
<h6>
 Abstract
</h6>

<p class="leading">
 We propose a new approach for constructing mid-levelvisual features for image classification. We represent animage using the outputs of a collection of binary classifiers.These binary classifiers are trained to differentiate pairs ofobject classes in an object hierarchy. Our feature representationimplicitly captures the hierarchical structure inobject classes. We show that our proposed approach outperformsother baseline methods in image classification.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wacv14.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wacv14.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wacv14.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Mid-Level Features from Object Hierarchy for Image Classication
IEEE Winter Conference on Applications of Computer Vision
[u'Jia Li', u'Somayah Albaradei', u'Yang Wang', u'Liangliang Cao']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6646/distance-oracles-edge-labeled-graphs
found
<h6>
 Abstract
</h6>

<p class="leading">
 A fundamental operation over edge-labeled graphs is the computation of shortest-path distances subject to a constraint on the set of permissible edge labels. Applying exact algorithms for such an operation is not a viable option, especially for massive graphs, or in scenarios where the distance computation is used as a primitive for more complex computations.

In this paper we study the problem of efficient approximation of shortest-path queries with edge-label constraints, for which we devise two indexes based on the idea of landmarks: distances from all vertices of the graph to a selected subset of landmark vertices are pre-computed and then used at query time to efficiently approximate distance queries. The major challenge to face is that, in principle, an exponential number of constraint label sets needs to be stored for each vertex-landmark pair, which makes the index pre-computation and storage far from trivial. We tackle this challenge from two different perspectives, which lead to indexes with different characteristics: one index is faster and more accurate, but it requires more space than the other.

We extensively evaluate our techniques on real and synthetic datasets, showing that our indexes can efficiently and accurately estimate label-constrained distance queries.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distance Oracles in Edge-Labeled Graphs
International Conference on Extending Database Technology (EDBT 14)
[u'Francesco Bonchi', u'Francesco Gullo', u'Aris Gionis', u'Antti Ukkonen']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=19
found
 LINK 
https://labs.yahoo.com/publications/6646/distance-oracles-edge-labeled-graphs
found
<h6>
 Abstract
</h6>

<p class="leading">
 A fundamental operation over edge-labeled graphs is the computation of shortest-path distances subject to a constraint on the set of permissible edge labels. Applying exact algorithms for such an operation is not a viable option, especially for massive graphs, or in scenarios where the distance computation is used as a primitive for more complex computations.

In this paper we study the problem of efficient approximation of shortest-path queries with edge-label constraints, for which we devise two indexes based on the idea of landmarks: distances from all vertices of the graph to a selected subset of landmark vertices are pre-computed and then used at query time to efficiently approximate distance queries. The major challenge to face is that, in principle, an exponential number of constraint label sets needs to be stored for each vertex-landmark pair, which makes the index pre-computation and storage far from trivial. We tackle this challenge from two different perspectives, which lead to indexes with different characteristics: one index is faster and more accurate, but it requires more space than the other.

We extensively evaluate our techniques on real and synthetic datasets, showing that our indexes can efficiently and accurately estimate label-constrained distance queries.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper_269.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distance Oracles in Edge-Labeled Graphs
International Conference on Extending Database Technology (EDBT 14)
[u'Francesco Bonchi', u'Francesco Gullo', u'Aris Gionis', u'Antti Ukkonen']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6958/scaling-private-set-intersection-billion-element-sets
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  We examine the feasibility of private set intersection (PSI) over massive datasets. PSI, which allows two parties to find the intersection of their sets without revealing them to each other, has numerous applications including to privacy-preserving data mining, location-based services and genomic computations. Unfortunately, the most efficient constructions only scale to sets containing a few thousand elements---even in the semi-honest model and over a LAN. In this work, we design PSI protocols in the server-aided setting, where the parties have access to a single untrusted server that makes its computational resources available as a service. We show that by exploiting the server-aided model and by carefully optimizing and parallelizing our implementations, PSI is feasible for billion-element sets even while communicating over the Internet. As far as we know, ours is the first attempt to scale PSI to billion-element sets which represents an increase of five orders of magnitude over previous work. Our protocols are secure in several adversarial models including against a semi-honest, covert and malicious server; and address a range of security and privacy concerns including fairness and the leakage of the intersection size. Our protocols also yield efficient server-aided private equality-testing (PET) with stronger security guarantees than prior work.
 </p>
</p>

<p>
 We examine the feasibility of private set intersection (PSI) over massive datasets. PSI, which allows two parties to find the intersection of their sets without revealing them to each other, has numerous applications including to privacy-preserving data mining, location-based services and genomic computations. Unfortunately, the most efficient constructions only scale to sets containing a few thousand elements---even in the semi-honest model and over a LAN. In this work, we design PSI protocols in the server-aided setting, where the parties have access to a single untrusted server that makes its computational resources available as a service. We show that by exploiting the server-aided model and by carefully optimizing and parallelizing our implementations, PSI is feasible for billion-element sets even while communicating over the Internet. As far as we know, ours is the first attempt to scale PSI to billion-element sets which represents an increase of five orders of magnitude over previous work. Our protocols are secure in several adversarial models including against a semi-honest, covert and malicious server; and address a range of security and privacy concerns including fairness and the leakage of the intersection size. Our protocols also yield efficient server-aided private equality-testing (PET) with stronger security guarantees than prior work.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sapsi2.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sapsi2.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/sapsi2.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Scaling Private Set Intersection to Billion-Element Sets
Financial Cryptography and Data Security 2014
[u'Seny Kamara', u'Payman Mohassel', u'Mariana Raykova', u'Saeed Sadeghian']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6641/user-modeling-search-logs-nonparametric-bayesian-approach
found
<h6>
 Abstract
</h6>

<p class="leading">
 Searchers information needs are diverse and cover a broad range of topics; hence, it is important for search engines to accurately understand each individual users search intents in order to provide optimal search results. Search log data, which records users search behaviors when interacting with search engines, provides a valuable source of information about users search intents. Therefore, properly characterizing the heterogeneity among the users observed search behaviors is the key to accurately understanding their search intents and to further predicting their behaviors.

In this work, we study the problem of user modeling in the search log data and propose a generative model, dpRank, within a non-parametric Bayesian framework. By postulating generative assumptions about a users search behaviors, dpRank identifies each individual users latent search interests and his/her distinct result preferences in a joint manner. Experimental results on a large-scale news search log data set validate the effectiveness of the proposed approach, which not only provides in-depth understanding of a users search intents but also benefits a variety of personalized applications.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm14_UserModeling1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm14_UserModeling1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm14_UserModeling1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
User Modeling in Search Logs via a Nonparametric Bayesian Approach
WSDM 2014
[u'Anlei Dong', u'Yi Chang', u'Hongning Wang', u'Chengxiang Zhai', u'Feng Liang']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6677/improving-efficiency-multi-site-web-search-engines
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    A multi-site web search engine is composed of a number of search sites geographically distributed around the world. Each search site is typically responsible for crawling and indexing the web pages that are in its geographical neighborhood. A query is selectively processed on a subset of search sites that are predicted to return the best matching results. The scalability and efficiency of multi-site web search engines have attracted a lot of research attention in recent years. In particular, research has focused on replicating important web pages across sites, forwarding queries to relevant sites, and caching results of previous queries. Yet, these problems have only been studied in isolation, but no prior work has properly investigated the interplay between them.

In this paper, we take this challenge up and conduct what we believe is the first comprehensive analysis of a full stack of techniques for efficient multi-site web search. Specifically, we propose a document replication technique that improves the query locality of the state-of-the-art approaches with various replication budget distribution strategies. We devise a machine learning approach to decide the query forwarding patterns, achieving a significantly lower false positive ratio than a state-of-the-art thresholding approach with little negative impact on search result quality. We propose three result caching strategies that reduce the number of forwarded queries and analyze the trade-off they introduce in terms of storage and network overheads. Finally, we show that the combination of the best-of-the-class techniques yields very promising search efficiency, rendering multi-site, geographically distributed web search engines an attractive alternative to centralized web search engines.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   A multi-site web search engine is composed of a number of search sites geographically distributed around the world. Each search site is typically responsible for crawling and indexing the web pages that are in its geographical neighborhood. A query is selectively processed on a subset of search sites that are predicted to return the best matching results. The scalability and efficiency of multi-site web search engines have attracted a lot of research attention in recent years. In particular, research has focused on replicating important web pages across sites, forwarding queries to relevant sites, and caching results of previous queries. Yet, these problems have only been studied in isolation, but no prior work has properly investigated the interplay between them.

In this paper, we take this challenge up and conduct what we believe is the first comprehensive analysis of a full stack of techniques for efficient multi-site web search. Specifically, we propose a document replication technique that improves the query locality of the state-of-the-art approaches with various replication budget distribution strategies. We devise a machine learning approach to decide the query forwarding patterns, achieving a significantly lower false positive ratio than a state-of-the-art thresholding approach with little negative impact on search result quality. We propose three result caching strategies that reduce the number of forwarded queries and analyze the trade-off they introduce in terms of storage and network overheads. Finally, we show that the combination of the best-of-the-class techniques yields very promising search efficiency, rendering multi-site, geographically distributed web search engines an attractive alternative to centralized web search engines.
  </div>
 </div>
</div>
<div>
 <div>
  A multi-site web search engine is composed of a number of search sites geographically distributed around the world. Each search site is typically responsible for crawling and indexing the web pages that are in its geographical neighborhood. A query is selectively processed on a subset of search sites that are predicted to return the best matching results. The scalability and efficiency of multi-site web search engines have attracted a lot of research attention in recent years. In particular, research has focused on replicating important web pages across sites, forwarding queries to relevant sites, and caching results of previous queries. Yet, these problems have only been studied in isolation, but no prior work has properly investigated the interplay between them.

In this paper, we take this challenge up and conduct what we believe is the first comprehensive analysis of a full stack of techniques for efficient multi-site web search. Specifically, we propose a document replication technique that improves the query locality of the state-of-the-art approaches with various replication budget distribution strategies. We devise a machine learning approach to decide the query forwarding patterns, achieving a significantly lower false positive ratio than a state-of-the-art thresholding approach with little negative impact on search result quality. We propose three result caching strategies that reduce the number of forwarded queries and analyze the trade-off they introduce in terms of storage and network overheads. Finally, we show that the combination of the best-of-the-class techniques yields very promising search efficiency, rendering multi-site, geographically distributed web search engines an attractive alternative to centralized web search engines.
 </div>
</div>

<div>
 A multi-site web search engine is composed of a number of search sites geographically distributed around the world. Each search site is typically responsible for crawling and indexing the web pages that are in its geographical neighborhood. A query is selectively processed on a subset of search sites that are predicted to return the best matching results. The scalability and efficiency of multi-site web search engines have attracted a lot of research attention in recent years. In particular, research has focused on replicating important web pages across sites, forwarding queries to relevant sites, and caching results of previous queries. Yet, these problems have only been studied in isolation, but no prior work has properly investigated the interplay between them.

In this paper, we take this challenge up and conduct what we believe is the first comprehensive analysis of a full stack of techniques for efficient multi-site web search. Specifically, we propose a document replication technique that improves the query locality of the state-of-the-art approaches with various replication budget distribution strategies. We devise a machine learning approach to decide the query forwarding patterns, achieving a significantly lower false positive ratio than a state-of-the-art thresholding approach with little negative impact on search result quality. We propose three result caching strategies that reduce the number of forwarded queries and analyze the trade-off they introduce in terms of storage and network overheads. Finally, we show that the combination of the best-of-the-class techniques yields very promising search efficiency, rendering multi-site, geographically distributed web search engines an attractive alternative to centralized web search engines.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper3.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper3.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/paper3.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Improving the Efficiency of Multi-site Web Search Engines
The 7th ACM International Conference on Web Search and Data Mining (WSDM'14)
[u'Xiao Bai', u'B. Barla Cambazoglu', u'Ricardo Baeza-yates', u'Guillem F Medina']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6680/efficient-framework-online-advertising-effectiveness-measurement-and-comparison
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the online advertising market it is crucial to provide advertisers with a reliable measurement of advertising effectiveness to make better marketing campaign planning. The basic idea for ad effectiveness measurement is to compare the performance (e.g., success rate) among users who were and who were not exposed to a certain treatment of ads. When a randomized experiment is not available, a naive comparison can be biased because exposed and unexposed populations typically have different features. One solid methodology for a fair comparison is to apply inverse propensity weighting with doubly robust estimation to the observational data. However the existing methods were not designed for the online advertising campaign, which usually suffers from a huge volume of users, high dimensionality, high sparsity and imbalance. We propose an efficient framework to address these challenges in a real campaign circumstance. We utilize gradient boosting stumps for feature selection and gradient boosting trees for model fitting, and propose a subsampling-and-backscaling procedure that enables analysis on extremely sparse conversion data. The choice of features, models and feature selection scheme are validated with irrelevant conversion test. We further propose a parallel computing strategy, combined with the subsampling-and-backscaling procedure to reach computational efficiency. Our framework is applied to an online campaign involving millions of unique users, which shows substantially better model fitting and efficiency. Our framework can be further generalized to comparison of multiple treatments and more general treatment regimes, as sketched in the paper. Our framework is not limited to online advertising, but also applicable to other circumstances (e.g., social science) where a `fair' comparison is needed with observational data.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm238-wang.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm238-wang.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/wsdm238-wang.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
An Efficient Framework for Online Advertising Effectiveness Measurement and Comparison
ACM WSDM Conference 2014 Proceedings
[u'Pengyuan Wang', u'Han-yun Tsao', u'Jimmy Yang', u'Yechao Liu', u'Marsha Meytlis', u'Pei Huang']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6614/aesthetic-capital-what-makes-london-look-beautiful-quiet-and-happy
found
<h6>
 Abstract
</h6>

<p class="leading">
 In the 1960s, Lynchs The Image of the City explored what impression US city neighborhoods left on its inhabitants. The scale of urban perception studies until recently was considerably constrained by the limited number of study participants. We here present a crowdsourcing project that aims to investigate, at scale, which visual aspects of London neighborhoods make them appear beautiful, quiet, and/or happy. We collect votes from over 3.3K individuals and translate them into quantitative measures of urban perception. In so doing, we quantify each neighborhoods aesthetic capital. By then using state-of-the-art image processing techniques, we determine visual cues that may cause a street to be perceived as being beautiful, quiet, or happy. We identify effects of color, texture and visual words. For example, the amount of greenery is the most positively associated visual cue with each of the three qualities; by contrast, broad streets, fortress-like buildings,and council houses tend to be associated with the opposite qualities (ugly, noisy, and unhappy).
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/quercia14aesthetic.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/quercia14aesthetic.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/quercia14aesthetic.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Aesthetic Capital: What Makes London Look Beautiful, Quiet, and Happy?
CSCW 2014
[u'Daniele Quercia', u"Neil O'hare", u'Henriette Cramer']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6639/%E2%80%9Cour-life-farm-and-farming-our-life%E2%80%9D-home%E2%80%93work-coordination-organic-farm-families
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present a qualitative study of 13 farm families who intentionally merge their home and work lives. This is in contrast to most families studied in CSCW, who are urban/suburban, white-collar and often dual-income, where the goal is to balance separate home and work spheres. We analyze the farm families' coordination practices along three dimensions -- space, time, and roles -- and contrast their experiences to what is known in CSCW about family coordination practices. Through this, we reveal blind spots in CSCW's study of and support for family coordination toward building better tools to support such activities. We emphasize considering co-location rather than assuming geographic distribution across life spheres, the value of natural rhythms in understanding and supporting family life, and how taking on simultaneous roles can be viewed as a life goal rather than a source of conflict.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p487-leshed.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p487-leshed.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p487-leshed.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Our Life is the Farm and Farming is Our Life: HomeWork Coordination in Organic Farm Families
Proc. CSCW 2014
[u'Jofish Kaye', u'Gilly Leshed', u'Maria Hakansson']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6947/automatic-semantic-locking
found
<h6>
 Abstract
</h6>

<p class="leading">
 In this paper, we consider concurrent programs in which the shared state consists of instances of linearizable ADTs (abstract data types). We develop a novel automated approach to concurrency control that addresses a common need: the need to atomically execute a code fragment, which may contain multiple ADT operations on multiple ADT instances. In our approach, each ADT implements ADT-specific semantic locking operations that serve to exploit the semantics of ADT operations. We develop a synthesis algorithm that automatically inserts calls to these locking operations in a set of given code fragments (in a client program) to ensure that these code fragments execute atomically without deadlocks, and without rollbacks.

We have implemented the synthesis algorithm and several general-purpose ADTs with semantic locking. We have applied the synthesis algorithm to several Java programs that use these ADTs. Our results show that our approach enables efficient and scalable synchronization.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p385-golangueta.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p385-golangueta.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/p385-golangueta.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Automatic Semantic Locking
PPoPP'14
[u'Guy Golan Gueta', u'G. Ramalingam', u'Mooly Sagiv', u'Eran Yahav']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6700/using-temporal-bursts-query-modeling
found
<h6>
 Abstract
</h6>

<p class="leading">
 We present an approach to query modeling that leverages the temporal distribution of documents in an initially retrieved set of documents. In news-related document collections such distributions tend to exhibit bursts. Here, we define a burst to be a time period where unusually many documents are published. In our approach we detect bursts in result lists returned for a query. We then model the term distributions of the bursts using a reduced result list and select its most descriptive terms. Finally, we merge the sets of terms obtained in this manner so as to arrive at a reformulation of the original query. For query sets that consist of both temporal and non-temporal queries, our query modeling approach incorporates an effective selection method of terms. We consistently and significantly improve over various baselines, such as relevance models, on both news collections and a collection of blog posts.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/irj2013-temporal-1.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/irj2013-temporal-1.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/irj2013-temporal-1.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Using temporal bursts for query modeling
Information Retrieval
[u'Edgar Meij', u'Maria Hendrike Peetz', u'Maarten De Rijke']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6632/covariate-shift-adaptation-discriminative-3d-pose-estimation
found
<h6>
 Abstract
</h6>

<p class="leading">
 Discriminative, or (structured) prediction, methods have proved effective for a variety of problems in computer vision; a notable example is 3D monocular pose estimation. All methods to date, however, relied on an assumption that training (source) and test (target) data come from the same underlying joint distribution. In many real cases, including standard data sets, this assumption is flawed. In the presence of training set bias, the learning results in a biased model whose performance degrades on the (target) test set. Under the assumption of covariate shift, we propose an unsupervised domain adaptation approach to address this problem. The approach takes the form of training instance reweighting, where the weights are assigned based on the ratio of training and test marginals evaluated at the samples. Learning with the resulting weighted training samples alleviates the bias in the learned models. We show the efficacy of our approach by proposing weighted variants of kernel regression (KR) and twin Gaussian processes (TGP). We show that our weighted variants outperform their unweighted counterparts and improve on the state-of-the-art performance in the public (HumanEva) data set.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Covariate-Shift-Adaptation-for-Discriminative-3D-Pose-Estimation.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Covariate-Shift-Adaptation-for-Discriminative-3D-Pose-Estimation.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Covariate-Shift-Adaptation-for-Discriminative-3D-Pose-Estimation.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Covariate Shift Adaptation for Discriminative 3D Pose Estimation
IEEE Transactions on Pattern Analysis and Machine Intelligence
[u'Makoto Yamada', u'Leonid Sigal', u'Michalis Raptis']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=20
found
 LINK 
https://labs.yahoo.com/publications/6655/visual-recognition-exploiting-latent-social-links-image-collections
found
<h6>
 Abstract
</h6>

<p class="leading">
 Social network study has become an important topic in many researchfields. Early works on social network analysis focus on real world social interactionsin either human society or animal world.With the explosion of Internet data,social network researchers start to pay more attention to the tremendous amountof online social network data. There is ample space for exploring social networkresearch on large-scale online visual content. In this paper, we focus on studying a multi-label collective classification problem and develop a model that can harness mutually beneficial information among visual appearance, related semanticcontent and social network structure simultaneously. Our algorithm is thentested on CelebrityNet, a social network constructed by inferring the implicit relationshipof people based on online multimedia content. We apply our model toa few important multimedia applications such as image annotation and communityclassification. We demonstrate that our algorithm significantly outperformstraditional methods on community classification and image annotation.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mmm.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mmm.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/mmm.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Visual Recognition by Exploiting Latent Social Links in Image Collections
20th Anniversary International Conference on MultiMedia Modeling
[u'Jia Li', u'Xiangnan Kong', u'Philip S Yu']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6630/high-dimensional-feature-selection-feature-wise-kernelized-lasso
found
<h6>
 Abstract
</h6>

<p class="leading">
 The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this letter, we consider a feature-wise kernelized Lasso for capturing nonlinear input-output dependency. We first show that with particular choices of kernel functions, nonredundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/High-Dimensional-Feature-Selection-by-Feature-Wise-Kernelized-Lasso.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/High-Dimensional-Feature-Selection-by-Feature-Wise-Kernelized-Lasso.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/High-Dimensional-Feature-Selection-by-Feature-Wise-Kernelized-Lasso.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso
Neural Computation
[u'Makoto Yamada', u'Wittawat Jitkrittum', u'Leonid Sigal', u'Eric P Xing', u'Masashi Sugiyama']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6633/information-maximization-clustering-based-squared-loss-mutual-information
found
<h6>
 Abstract
</h6>

<p class="leading">
 Information-maximization clustering learns a probabilistic classifier in an unsupervised manner so that mutual information between feature vectors and cluster assignments is maximized. A notable advantage of this approach is that it involves only continuous optimization of model parameters, which is substantially simpler than discrete optimization of cluster assignments. However, existing methods still involve nonconvex optimization problems, and therefore finding a good local optimal solution is not straightforward in practice. In this letter, we propose an alternative information-maximization clustering method based on a squared-loss variant of mutual information. This novel approach gives a clustering solution analytically in a computationally efficient way via kernel eigenvalue decomposition. Furthermore, we provide a practical model selection procedure that allows us to objectively optimize tuning parameters included in the kernel function. Through experiments, we demonstrate the usefulness of the proposed approach.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Information-Maximization-Clustering-based-on-Squared-Loss-Mutual-Information.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Information-Maximization-Clustering-based-on-Squared-Loss-Mutual-Information.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Information-Maximization-Clustering-based-on-Squared-Loss-Mutual-Information.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Information-Maximization Clustering Based on Squared-Loss Mutual Information
Neural Computation
[u'Makoto Yamada', u'Masashi Sugiyama', u'Gang Niu', u'Hirotaka Hachiya', u'Manabu Kimura']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6620/domain-adaptation-structured-regression
found
<h6>
 Abstract
</h6>

<p class="leading">
 Discriminative regression models have proved effective for many vision applications (here we focus on 3D full-body and head pose estimation from image and depth data). However,dataset biasis common and is able to significantly degrade the performance of a trained model on target test sets. As we show, covariate shift, a form of unsupervised domain adaptation (USDA), can be used to address certain biases in this setting, but is unable to deal with more severe structural biases in the data. We propose an effective and efficient semi-supervised domain adaptation (SSDA) approach for addressing such more severe biases in the data. Proposed SSDA is a generalization of USDA, that is able to effectively leverage labeled data in the target domain when available. Our method amounts to projecting input features into a higher dimensional space (by construction well suited for domain adaptation) and estimating weights for the training samples based on the ratio of test and train marginals in that space. The resulting augmented weighted samples can then be used to learn a model of choice, alleviating the problems of bias in the data; as an example, we introduce SSDA twin Gaussian process regression (SSDA-TGP) model. With this model we also address the issue ofdata sharing, where we are able to leverage samples from certain activities (e.g., walking, jogging) to improve predictive performance on very different activities (e.g., boxing). In addition, we analyze the relationship between domain similarity and effectiveness of proposed USDA versus SSDA methods. Moreover, we propose a computationally efficient alternative to TGP (Bo and Sminchisescu2010), and its variants, called the direct TGP. We show that our model outperforms a number of baselines, on two public datasets: HumanEva and ETH Face Pose Range Image Dataset. We can also achieve 815 times speedup in computation time, over the traditional formulation of TGP, using the proposed direct formulation, with little to no loss in performance.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main_IJCV20141.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main_IJCV20141.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/main_IJCV20141.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Domain Adaptation for Structured Regression
International Journal of Computer Vision (IJCV)
[u'Makoto Yamada', u'Yi Chang', u'Leonid Sigal']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6626/learning-relevance-web-resources-across-domains-make-recommendations
found
<h6>
 Abstract
</h6>

<p class="leading">
 Most traditional recommender systems focus on the objective of improving the accuracy of recommendations in a single domain. However, preferences of users may extend over multiple domains, especially on the Web where users often have browsing preferences that span across different sites while being unaware of relevant resources on other sites.
 <div dir="ltr">
  This work tackles the problem of recommending resources from various domains by exploiting the semantic content of these resources in combination with patterns of user browsing behavior. We overcome the lack of overlaps between domains by deriving connections based on the explored semantic content of Web resources. We present an approach that applies Support Vector Machines for learning the relevance of resources and predicting which ones are the most relevant to recommend to a user, given that the user is currently viewing a certain page. In real-world datasets of semantically-enriched logs of user browsing behavior at multiple Web sites, we study the impact of structure in generating accurate recommendations and conduct experiments that demonstrate the effectiveness of our approach.
 </div>
</p>

<div dir="ltr">
 This work tackles the problem of recommending resources from various domains by exploiting the semantic content of these resources in combination with patterns of user browsing behavior. We overcome the lack of overlaps between domains by deriving connections based on the explored semantic content of Web resources. We present an approach that applies Support Vector Machines for learning the relevance of resources and predicting which ones are the most relevant to recommend to a user, given that the user is currently viewing a certain page. In real-world datasets of semantically-enriched logs of user browsing behavior at multiple Web sites, we study the impact of structure in generating accurate recommendations and conduct experiments that demonstrate the effectiveness of our approach.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icmla2013.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icmla2013.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icmla2013.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning Relevance of Web Resources across Domains to make Recommendations
Proceedings of the 12th International Conference in Machine Learning and Applications (ICMLA 13)
[u'Peter Mika', u'Roi Blanco', u'Julia Hoxha']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6619/dynasore-efficient-memory-store-social-applications
found
<h6>
 Abstract
</h6>

<p class="leading">
 Social network applications are inherently interactive, creatinga requirement for processing user requests fast. To enable fast responsesto user requests, social network applications typically rely onlarge banks of cache servers to hold and serve most of their content fromthe cache. In this work, we present DynaSoRe: a memory cache systemfor social network applications that optimizes data locality while placinguser views across the system. DynaSoRe storage servers monitor accesstraffic and bring data frequently accessed together closer in the systemto reduce the processing load across cache servers and network devices.Our simulation results considering realistic data center topologies showthat DynaSoRe is able to adapt to traffic changes, increase data locality,and balance the load across the system. The traffic handled by the toptier of the network connecting servers drops by 94% compared to a staticassignment of views to cache servers while requiring only 30% additionalmemory capacity compared to the whole volume of cached data.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
DynaSoRe: Efficient In-memory Store for Social Applications
Proceedings of the ACM/IFIP/USENIX 14th International Conference on Middleware (Middleware)
[u'Xiao Bai', u'Arnaud Jegou', u'Flavio P Junqueira', u'Vincent Leroy']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6971/linear-regression-non-cooperative-game
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  Linear regression amounts to estimating a linear model that maps features (e.g., age or gender) to corresponding data (e.g., the answer to a survey or the outcome of a medical exam). It is a ubiquitous tool in experimental sciences. We study a setting in which features are public but the data is private information. While the estimation of the linear model may be useful to participating individuals, (if, e.g., it leads to the discovery of a treatment to a disease), individuals may be reluctant to disclose their data due to privacy concerns. In this paper, we propose a generic game-theoretic model to express this trade-off. Users add noise to their data before releasing it. In particular, they choose the variance of this noise to minimize a cost comprising two components: (a) a privacy cost, representing the loss of privacy incurred by the release; and (b) an estimation cost, representing the inaccuracy in the linear model estimate. We study the Nash equilibria of this game, establishing the existence of a unique non-trivial equilibrium. We determine its efficiency for several classes of privacy and estimation costs, using the concept of the price of stability. Finally, we prove that, for a specific estimation cost, the generalized least-square estimator is optimal among all linear unbiased estimators in our non-cooperative setting: this result extends the famous Aitken/Gauss-Markov theorem in statistics, establishing that its conclusion persists even in the presence of strategic individuals.
 </div>
</p>

<div dir="ltr">
 Linear regression amounts to estimating a linear model that maps features (e.g., age or gender) to corresponding data (e.g., the answer to a survey or the outcome of a medical exam). It is a ubiquitous tool in experimental sciences. We study a setting in which features are public but the data is private information. While the estimation of the linear model may be useful to participating individuals, (if, e.g., it leads to the discovery of a treatment to a disease), individuals may be reluctant to disclose their data due to privacy concerns. In this paper, we propose a generic game-theoretic model to express this trade-off. Users add noise to their data before releasing it. In particular, they choose the variance of this noise to minimize a cost comprising two components: (a) a privacy cost, representing the loss of privacy incurred by the release; and (b) an estimation cost, representing the inaccuracy in the linear model estimate. We study the Nash equilibria of this game, establishing the existence of a unique non-trivial equilibrium. We determine its efficiency for several classes of privacy and estimation costs, using the concept of the price of stability. Finally, we prove that, for a specific estimation cost, the generalized least-square estimator is optimal among all linear unbiased estimators in our non-cooperative setting: this result extends the famous Aitken/Gauss-Markov theorem in statistics, establishing that its conclusion persists even in the presence of strategic individuals.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/regression_game.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/regression_game.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/regression_game.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Linear Regression as a Non-Cooperative Game
Conference on Web and Internet Economics (WINE)
[u'Stratis Ioannidis', u'Patrick Loiseau']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6617/in%EF%AC%82uence-based-network-oblivious-community-detection
found
<h6>
 Abstract
</h6>

<p class="leading">
 How can we detect communities when social graphs are not available? We tackle this problem by modelingsocial contagion from a log of user activity, that is a dataset oftuples (u; i; t) recording the fact that user u adopted item i attime t. This is the only input to our problem.

We propose a stochastic framework which assumes that itemadoptions are governed by an underlying diffusion process overthe unobserved social network, and that such a diffusion modelis based on community-level inuence. By tting the modelparameters to the user activity log, we learn the communitymembership and the level of inuence of each user in eachcommunity. This allows us to identify for each community the keyusers, i.e., the leaders which are most likely to inuence the restof the community to adopt a certain item.

The general framework can be instantiated with differentdiffusion models. In this paper we dene two models: theextension to the community level of the classic (discrete time)Independent Cascade model, and a model that focuses on thetime delay between adoptions.

To the best of our knowledge, this is the rst work studyingcommunity detection without the network.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icdm13-cwn-CR.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icdm13-cwn-CR.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/icdm13-cwn-CR.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Inuence-based Network-oblivious Community Detection
International Conference on Data Mining
[u'Nicola Barbieri', u'Francesco Bonchi', u'Giuseppe Manco']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6612/adapting-classification-cascades-new-domains
found
<h6>
 Abstract
</h6>

<p class="leading">
 Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers -- using a small number of labeled positive instances from a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both the original cascade and the one trained from scratch using the given training examples.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Adapting Classification Cascades to New Domains
International Conference on Computer Vision (ICCV)
[u'Vidit Jain', u'Sachin Sudhakar Farfade']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5660/near-optimal-entrywise-sampling-data-matrices
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes |AB|2. For large mn matrices, such that n  m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O(1) computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes |AB|2. For large mn matrices, such that n  m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O(1) computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.
  </div>
 </div>
</div>
<div>
 <div>
  We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes |AB|2. For large mn matrices, such that n  m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O(1) computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.
 </div>
</div>

<div>
 We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes |AB|2. For large mn matrices, such that n  m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O(1) computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/matrix_sampling.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/matrix_sampling.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/matrix_sampling.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Near-Optimal Entrywise Sampling for Data Matrices
nips 2013
[u'Zohar Karnin', u'Edo Liberty']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=21
found
 LINK 
https://labs.yahoo.com/publications/6634/class-specific-simplex-latent-dirichlet-allocation-image-classification
found
<h6>
 Abstract
</h6>

<p class="leading">
 An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/class-specific-simplex.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/class-specific-simplex.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/class-specific-simplex.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Class Specific Simplex-Latent Dirichlet Allocation for Image Classification
IEEE International Conference on Computer Vision
[u'Nikhil Rasiwasia', u'M Dixit', u'N Vasconcelos']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6618/cache-refreshing-online-social-news-feeds
found
<h6>
 Abstract
</h6>

<p class="leading">
 Several social networking applications enable users to view the events generated by other users, typically friends in the social network, in the form of &quot;news feeds''. Friends and events are typically maintained per user and cached in memory to enable efficient generation of news feeds. Caching user friends and events, however, raises concerns about the freshness of news feeds as users may not observe the most recent events when cache content becomes stale. Mechanisms to keep cache content fresh are thus critical for user satisfaction while computing news feeds efficiently through caching.

We propose a novel cache scheme called SOCR (Social Online Cache Refreshing) for identifying and refreshing cache entries. SOCR refreshes the cache in an online manner and does not require the backend data store to push updates to the cache. SOCR uses a utility-based strategy to accurately identify cache entries that need to be refreshed. The basic idea is to estimate at the time of each request to generate news feed whether refreshing would lead to different results for a news feed. To make such estimation, we model the rates of changes to social networks and events, and assess the performance of SOCR by analyzing datasets from Facebook and Yahoo News activity. Our experimental evaluation shows that the utility-based strategy ensures fresh news feeds (43% fewer stales) and efficient news feed responses (51% fewer false positives) compared to the TTL-based strategy. SOCR also reduces data transmission between the backend data store and the cache by 27% compared to a hybrid push-pull cache refreshing scheme.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Cache Refreshing for Online Social News Feeds
Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM)
[u'Xiao Bai', u'Flavio P Junqueira', u'Adam Silberstein']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6913/pythia-detection-localization-and-diagnosis-performance-problems
found
<h6>
 Abstract
</h6>

<p class="leading">
 Performance problem diagnosis is a critical part of network operations in ISPs. Service providers use a combination of approaches to troubleshoot performance of their networks, such as active monitoring infrastructure and data collection (SNMP, Netflow, router logs, table dumps, etc.) along with customer trouble tickets. Some of these approaches, however, do not scale to wide area inter-domain networks due to unavailability of such data; moreover, troubleshooting is either reactive (e.g., driven by customer complaints) or (typically) automated using static thresholds. In this article, we describe the design and implementation of a system for root cause analysis and localization of performance problems in ISP networks. Our approach works with legacy monitoring infrastructure (e.g., perfSONAR deployments) and does not need specialized active probing tools or network data. Our system provides a language for network operators to define performance problem signatures, and provides near-real-timeperformance diagnosis and localization. We describe our deployment of Pythia in perfSONAR monitors in production networks in Georgia, covering over 250 inter-domain paths.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/06658653.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/06658653.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/06658653.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Pythia: Detection, Localization, and Diagnosis of Performance Problems
IEEE Communications Magazine
[u'Partha Kanuparthy', u'Danny Lee', u'Warren Matthews', u'Sajjad Zarifzadeh', u'Constantine Dovrolis']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6610/penguins-sweaters-or-serendipitous-entity-search-user-generated-content
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div title="Page 1">
   <div title="Page 1">
    <div title="Page 1">
     <div title="Page 1">
      <div title="Page 1">
       In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div title="Page 1">
  <div title="Page 1">
   <div title="Page 1">
    <div title="Page 1">
     <div title="Page 1">
      In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<div title="Page 1">
 <div title="Page 1">
  <div title="Page 1">
   <div title="Page 1">
    <div title="Page 1">
     In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
    </div>
   </div>
  </div>
 </div>
</div>

<div title="Page 1">
 <div title="Page 1">
  <div title="Page 1">
   <div title="Page 1">
    In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
   </div>
  </div>
 </div>
</div>

<div title="Page 1">
 <div title="Page 1">
  <div title="Page 1">
   In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
  </div>
 </div>
</div>

<div title="Page 1">
 <div title="Page 1">
  In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
 </div>
</div>

<div title="Page 1">
 In many cases, when browsing the Web users are searching for specific information or answers to concrete questions.Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further.What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extractedfrom two sources of user-generated content - Wikipedia, a user-curated online encyclopedia, and Yahoo Answers,a more unconstrained question/answering forum - in promoting serendipitous search. In this work, the content ofeach data source is represented as an entity network, which is further enriched with metadata about sentiment, writingquality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recommendations from the networks. We show that our method provides novel results from both datasets, compared to standard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not increase user interest for many categories of entities, suggesting a more complex relationship between topic matter andthe desirable metadata attributes in serendipitous search.
</div>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Penguins-in-Sweaters-or-Serendipitous-Entity-Search-on-User-generated-Content.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Penguins-in-Sweaters-or-Serendipitous-Entity-Search-on-User-generated-Content.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Penguins-in-Sweaters-or-Serendipitous-Entity-Search-on-User-generated-Content.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Penguins in Sweaters, or Serendipitous Entity Search on User-generated Content
ACM International Conference on Information and Knowledge Management (CIKM 2013)
[u'Ilaria Bordino', u'Yelena Mejova', u'Mounia Lalmas']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6608/online-multitasking-and-user-engagement
found
<h6>
 Abstract
</h6>

<p class="leading">
 Users often access and re-access more than one site during an online session, effectively engaging in multitasking. In this paper, we study the effect of online multitasking on two widely used engagement metrics designed to capture users browsing behavior with a site. Our study is based on browsing data of 2.5M users across 760 sites encompassing diverse types of services such as social media, news and mail. To account for multitasking we need to redefine how user sessions are represented and we need to adapt the metrics under study. We introduce a new representation of user sessions: tree-streams  as opposed to the commonly used click-streams  present a more accurate picture of the browsing behavior of a user that includes how users switch between sites (e.g., hyperlinking, teleporting, backpaging). We then discuss a number of insights on multitasking patterns, and show how these help to better understand how users engage with sites. Finally, we define metrics that characterize multitasking during online sessions and show how they provide additional insights to standard engagement metrics.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm486-lehmann.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm486-lehmann.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/cikm486-lehmann.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Online Multitasking and User Engagement
CIKM 2013
[u'Janette Lehmann', u'Mounia Lalmas', u'Georges Dupret', u'Ricardo Baeza-yates']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6991/rational-protocol-design-cryptography-against-incentive-driven-adversaries
found
<h6>
 Abstract
</h6>

<p class="leading">
 Existing work on rational cryptographicprotocols treats each party (or coalition of parties) runningthe protocol as a selfish agent trying to maximize itsutility. In this work we propose a fundamentally differentapproach that is better suited to modeling a protocolunder attack from an external entity. Specifically, weconsider a two-party game between an protocol designerand an external attacker. The goal of the attacker is tobreak security properties such as correctness or privacy,possibly by corrupting protocol participants; the goal ofthe protocol designer is to prevent the attacker fromsucceeding.

We lay the theoretical groundwork for a study ofcryptographic protocol design in this setting by providinga methodology for defining the problem within thetraditional simulation paradigm. Our framework providesways of reasoning about important cryptographic concepts(e.g., adaptive corruptions or attacks on communicationresources) not handled by previous game-theoretictreatments of cryptography. We also prove compositiontheorems thatfor the first timeprovide a sound way todesign rational protocols assuming ideal communicationresources (such as broadcast or authenticated channels)and then instantiate these resources using standard cryptographictools.

Finally, we investigate the problem of secure functionevaluation in our framework, where the attacker has topay for each party it corrupts. Our results demonstratehow knowledge of the attackers incentives can be used tocircumvent known impossibility results in this setting.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/rpd_focs20131.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/rpd_focs20131.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/rpd_focs20131.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Rational Protocol Design: Cryptography Against Incentive-driven Adversaries
The 54th Annual IEEE Symposium on the Foundations of Computer Science -- FOCS 2013
[u'Juan Garay', u'Jonathan Katz', u'Ueli Maurer', u'Bjoern Tackmann', u'Vassilis Zikas']
Security and Privacy
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/5656/learning-multiple-annotators-varying-expertise
found
<h6>
 Abstract
</h6>

<p class="leading">
 <span style="font-size:13px;">
  Learning from multiple annotators or knowledge sources has become an important problem in machine learning and data mining. This is in part due to the ease with which data can now be shared/collected among entities sharing a common goal, task, or data source; and additionally the need to aggregate and make inferences about the collected information. This paper focuses on the development of probabilistic approaches for statistical learning in this setting. It specially considers the case when annotators may be unreliable, but also when their expertise vary depending on the data they observe. That is, annotators may have better knowledge about different parts of the input space and therefore be inconsistently accurate across the task domain. The models developed address both the supervised and the semi-supervised settings and produce classification and annotator models that allow us to provide estimates of the true labels and annotator expertise when no ground-truth is available. In addition, we provide an analysis of the proposed models, tasks, and related practical problems under various scenarios. In particular, we address how to evaluate annotators and how to consider cases where some ground-truth may be available. We show experimentally that annotator expertise can indeed vary in real tasks and that the presented approaches provide clear advantages over previously introduced multi-annotator methods, which only consider input-independent annotator characteristics, and over alternative approaches that do not model multiple annotators.
 </span>
</p>

<span style="font-size:13px;">
 Learning from multiple annotators or knowledge sources has become an important problem in machine learning and data mining. This is in part due to the ease with which data can now be shared/collected among entities sharing a common goal, task, or data source; and additionally the need to aggregate and make inferences about the collected information. This paper focuses on the development of probabilistic approaches for statistical learning in this setting. It specially considers the case when annotators may be unreliable, but also when their expertise vary depending on the data they observe. That is, annotators may have better knowledge about different parts of the input space and therefore be inconsistently accurate across the task domain. The models developed address both the supervised and the semi-supervised settings and produce classification and annotator models that allow us to provide estimates of the true labels and annotator expertise when no ground-truth is available. In addition, we provide an analysis of the proposed models, tasks, and related practical problems under various scenarios. In particular, we address how to evaluate annotators and how to consider cases where some ground-truth may be available. We show experimentally that annotator expertise can indeed vary in real tasks and that the presented approaches provide clear advantages over previously introduced multi-annotator methods, which only consider input-independent annotator characteristics, and over alternative approaches that do not model multiple annotators.
</span>
<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning from Multiple Annotators with Varying Expertise

[u'Chris Yan', u'Romer Rosales', u'Glenn Fung', u'Jennifer Dy', u'Ramanathan Subramanian']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6499/automatically-embedding-newsworthy-links-articles-implementation-evaluation
found
<h6>
 Abstract
</h6>

<p class="leading">
 <p>
  News portals are a popular destination for web users. News providers are therefore interested in attaining higher visitor rates and promoting greater engagement with their content. One aspect of engagement deals with keeping users on site longer by allowing them to have enhanced click-through experiences. News portals have invested in ways to embed links within news stories but so far these links have been curated by news editors. Given the manual effort involved, the use of such links is limited to a small scale. In this article, we evaluate a system-based approach that detects newsworthy events in a news article and locates other articles related to these events. Our system does not rely on resources like Wikipedia to identify events, and it was designed to be domain independent. A rigorous evaluation, using Amazon's Mechanical Turk, was performed to assess the system-embedded links against the manually-curated ones. Our findings reveal that our system's performance is comparable with that of professional editors, and that users find the automatically generated highlights interesting and the associated articles worthy of reading. Our evaluation also provides quantitative and qualitative insights into the curation of links, from the perspective of users and professional editors.
 </p>
</p>

<p>
 News portals are a popular destination for web users. News providers are therefore interested in attaining higher visitor rates and promoting greater engagement with their content. One aspect of engagement deals with keeping users on site longer by allowing them to have enhanced click-through experiences. News portals have invested in ways to embed links within news stories but so far these links have been curated by news editors. Given the manual effort involved, the use of such links is limited to a small scale. In this article, we evaluate a system-based approach that detects newsworthy events in a news article and locates other articles related to these events. Our system does not rely on resources like Wikipedia to identify events, and it was designed to be domain independent. A rigorous evaluation, using Amazon's Mechanical Turk, was performed to assess the system-embedded links against the manually-curated ones. Our findings reveal that our system's performance is comparable with that of professional editors, and that users find the automatically generated highlights interesting and the associated articles worthy of reading. Our evaluation also provides quantitative and qualitative insights into the curation of links, from the perspective of users and professional editors.
</p>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/asi22959.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/asi22959.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/asi22959.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Automatically Embedding Newsworthy Links to Articles: From Implementation to Evaluation
Journal of the American Society for Information Science and Technology (JASIST)
[u'Ioannis Arapakis', u'Mounia Lalmas', u'Hakan Ceylan', u'Pinar Donmez']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6928/learning-multiple-annotators-varying-expertise
found
<h6>
 Abstract
</h6>

<p class="leading">
 <span style="font-size:13px;">
  Learning from multiple annotators or knowledge sources has become an important problem in machine learning and data mining. This is in part due to the ease with which data can now be shared/collected among entities sharing a common goal, task, or data source; and additionally the need to aggregate and make inferences about the collected information. This paper focuses on the development of probabilistic approaches for statistical learning in this setting. It specially considers the case when annotators may be unreliable, but also when their expertise vary depending on the data they observe. That is, annotators may have better knowledge about different parts of the input space and therefore be inconsistently accurate across the task domain. The models developed address both the supervised and the semi-supervised settings and produce classification and annotator models that allow us to provide estimates of the true labels and annotator expertise when no ground-truth is available. In addition, we provide an analysis of the proposed models, tasks, and related practical problems under various scenarios. In particular, we address how to evaluate annotators and how to consider cases where some ground-truth may be available. We show experimentally that annotator expertise can indeed vary in real tasks and that the presented approaches provide clear advantages over previously introduced multi-annotator methods, which only consider input-independent annotator characteristics, and over alternative approaches that do not model multiple annotators.
 </span>
</p>

<span style="font-size:13px;">
 Learning from multiple annotators or knowledge sources has become an important problem in machine learning and data mining. This is in part due to the ease with which data can now be shared/collected among entities sharing a common goal, task, or data source; and additionally the need to aggregate and make inferences about the collected information. This paper focuses on the development of probabilistic approaches for statistical learning in this setting. It specially considers the case when annotators may be unreliable, but also when their expertise vary depending on the data they observe. That is, annotators may have better knowledge about different parts of the input space and therefore be inconsistently accurate across the task domain. The models developed address both the supervised and the semi-supervised settings and produce classification and annotator models that allow us to provide estimates of the true labels and annotator expertise when no ground-truth is available. In addition, we provide an analysis of the proposed models, tasks, and related practical problems under various scenarios. In particular, we address how to evaluate annotators and how to consider cases where some ground-truth may be available. We show experimentally that annotator expertise can indeed vary in real tasks and that the presented approaches provide clear advantages over previously introduced multi-annotator methods, which only consider input-independent annotator characteristics, and over alternative approaches that do not model multiple annotators.
</span>
<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Learning from Multiple Annotators with Varying Expertise

[u'Chris Yan', u'Romer Rosales', u'Glenn Fung', u'Jennifer Dy', u'Ramanathan Subramanian']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6665/localization-points-interest-georeferenced-and-oriented-photographs
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    <p>
     The geographic coordinates automatically assigned to photos by the built-in GPS devices of cameras register the location a photo was taken at, rather than the location of the main point of interest within the depicted scene. While scientific advances have been made in the efforts to accurately locate the actual positions of such points of interest, these resort to either crude clustering-based approaches or expensive content-based approaches in order to estimate their geographic coordinates. In this paper we propose a novel technique that incorporates the compass direction supplied by modern cameras, allowing us to compute the most probable locations of the point of interest by analyzing intersections between the lines of sight originating from the cameras focusing on the same scene. Since the accuracy of the digital devices that supply the geographic coordinates and the compass direction can vary, we take these imprecisions into account when estimating the location.
    </p>
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   <p>
    The geographic coordinates automatically assigned to photos by the built-in GPS devices of cameras register the location a photo was taken at, rather than the location of the main point of interest within the depicted scene. While scientific advances have been made in the efforts to accurately locate the actual positions of such points of interest, these resort to either crude clustering-based approaches or expensive content-based approaches in order to estimate their geographic coordinates. In this paper we propose a novel technique that incorporates the compass direction supplied by modern cameras, allowing us to compute the most probable locations of the point of interest by analyzing intersections between the lines of sight originating from the cameras focusing on the same scene. Since the accuracy of the digital devices that supply the geographic coordinates and the compass direction can vary, we take these imprecisions into account when estimating the location.
   </p>
  </div>
 </div>
</div>
<div>
 <div>
  <p>
   The geographic coordinates automatically assigned to photos by the built-in GPS devices of cameras register the location a photo was taken at, rather than the location of the main point of interest within the depicted scene. While scientific advances have been made in the efforts to accurately locate the actual positions of such points of interest, these resort to either crude clustering-based approaches or expensive content-based approaches in order to estimate their geographic coordinates. In this paper we propose a novel technique that incorporates the compass direction supplied by modern cameras, allowing us to compute the most probable locations of the point of interest by analyzing intersections between the lines of sight originating from the cameras focusing on the same scene. Since the accuracy of the digital devices that supply the geographic coordinates and the compass direction can vary, we take these imprecisions into account when estimating the location.
  </p>
 </div>
</div>

<div>
 <p>
  The geographic coordinates automatically assigned to photos by the built-in GPS devices of cameras register the location a photo was taken at, rather than the location of the main point of interest within the depicted scene. While scientific advances have been made in the efforts to accurately locate the actual positions of such points of interest, these resort to either crude clustering-based approaches or expensive content-based approaches in order to estimate their geographic coordinates. In this paper we propose a novel technique that incorporates the compass direction supplied by modern cameras, allowing us to compute the most probable locations of the point of interest by analyzing intersections between the lines of sight originating from the cameras focusing on the same scene. Since the accuracy of the digital devices that supply the geographic coordinates and the compass direction can vary, we take these imprecisions into account when estimating the location.
 </p>
</div>

<p>
 The geographic coordinates automatically assigned to photos by the built-in GPS devices of cameras register the location a photo was taken at, rather than the location of the main point of interest within the depicted scene. While scientific advances have been made in the efforts to accurately locate the actual positions of such points of interest, these resort to either crude clustering-based approaches or expensive content-based approaches in order to estimate their geographic coordinates. In this paper we propose a novel technique that incorporates the compass direction supplied by modern cameras, allowing us to compute the most probable locations of the point of interest by analyzing intersections between the lines of sight originating from the cameras focusing on the same scene. Since the accuracy of the digital devices that supply the geographic coordinates and the compass direction can vary, we take these imprecisions into account when estimating the location.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GeoMM2013-Final-Submission.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GeoMM2013-Final-Submission.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/GeoMM2013-Final-Submission.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Localization of points of interest from georeferenced and oriented photographs
ACM International Workshop on Geotagging and Its Applications in Multimedia
[u'Bart Thomee']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
https://labs.yahoo.com/publications?field_publications_research_area_tid=All&field_publications_date_value[value][year]=&page=22
found
 LINK 
https://labs.yahoo.com/publications/6627/federated-entity-search-using-fly-consolidation
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in different sources, which may provide only limited information about their content and are therefore called uncooperative. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of federated entity search, where redundant information about entities is reduced on-the-fly through entity consolidation performed at query time. We propose a novel method for entity consolidation that is based on using language models and completely unsupervised, hence more suitable for this on-the-fly uncooperative setting than state-of-the-art methods that require training data. Further, we apply the same language model technique to deal with the federated search problem of ranking results returned from dierent sources. Particularly novel are the mechanisms we propose to incorporate consolidation results into this ranking. We perform experiments using real Web queries and data sources. Our experiments show that our approach for federated entity search with on-the-fly consolidation improves upon the performance of a state-of-the-art preference aggregation baseline and also benefits from consolidation.
 </div>
 <div dir="ltr">
 </div>
 <div dir="ltr">
 </div>
</p>

<div dir="ltr">
 Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in different sources, which may provide only limited information about their content and are therefore called uncooperative. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of federated entity search, where redundant information about entities is reduced on-the-fly through entity consolidation performed at query time. We propose a novel method for entity consolidation that is based on using language models and completely unsupervised, hence more suitable for this on-the-fly uncooperative setting than state-of-the-art methods that require training data. Further, we apply the same language model technique to deal with the federated search problem of ranking results returned from dierent sources. Particularly novel are the mechanisms we propose to incorporate consolidation results into this ranking. We perform experiments using real Web queries and data sources. Our experiments show that our approach for federated entity search with on-the-fly consolidation improves upon the performance of a state-of-the-art preference aggregation baseline and also benefits from consolidation.
</div>

<div dir="ltr">
</div>

<div dir="ltr">
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013b.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013b.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013b.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Federated Entity Search using On-The-Fly Consolidation
The 12th International Semantic Web Conference
[u'Peter Mika', u'Roi Blanco', u'Daniel M Herzig', u'Than Tran Duc']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6628/entity-recommendations-web-search
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div dir="ltr">
  While some web search users know exactly what they are looking for, others are willing to explore other topics related to an initial interest. Often, the user's initial interest can be uniquely linked to an entity in a knowledge base, and in this case it is natural to recommend the explicitly linked entities for further exploration. In real world knowledge bases, however, the number of linked entities may be very large and not all related entities may be equally relevant. Thus there is a need for ranking related entities. In this paper, we describe Spark, a recommendation engine that links a users' initial query to an entity within a knowledge base and provides a ranking of the related entities. Spark extracts several signals from a variety of data sources, including user sessions, Twitter and Flickr, using a large cluster of computers running Hadoop. These signals are combined with a machine-learned ranking model in order to produce a final recommendation of entities to user queries, which is currently powering Yahoo Search results pages.
 </div>
</p>

<div dir="ltr">
 While some web search users know exactly what they are looking for, others are willing to explore other topics related to an initial interest. Often, the user's initial interest can be uniquely linked to an entity in a knowledge base, and in this case it is natural to recommend the explicitly linked entities for further exploration. In real world knowledge bases, however, the number of linked entities may be very large and not all related entities may be equally relevant. Thus there is a need for ranking related entities. In this paper, we describe Spark, a recommendation engine that links a users' initial query to an entity within a knowledge base and provides a ranking of the related entities. Spark extracts several signals from a variety of data sources, including user sessions, Twitter and Flickr, using a large cluster of computers running Hadoop. These signals are combined with a machine-learned ranking model in order to produce a final recommendation of entities to user queries, which is currently powering Yahoo Search results pages.
</div>
<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013a.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013a.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/iswc2013a.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Entity Recommendations in Web Search
The 12th International Semantic Web Conference
[u'Roi Blanco', u'B. Barla Cambazoglu', u'Peter Mika', u'Nicolas Torzec']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6609/stock-trade-volume-prediction-yahoo-finance-user-browsing-behavior
found
<h6>
 Abstract
</h6>

<p class="leading">
 <div title="Page 1">
  <div>
   <div>
    Web traffic represents a powerful mirror for various real-world phenomena. For example, volumes of web searches have been shown to have a positive correlation with stock trading volumes and with the sentiment of investors. Our hypothesis is that user browsing behavior on a domain-specific portal is a better predictor of user intent than web searches.

We focus on the financial domain and we analyze the web browsing and trading data of more than 2,600 stocks traded on NYSE, Nasdaq, and S&P. The web browsing data consists of user page views related to stock S on Yahoo Finance, while the trading data includes the trading volume of S. We study the correlation and causality between web browsing and trading data while varying the time granularity (hourly, daily) and financial segmentation (individual tickers, industries, sectors).

We find that web browsing on Yahoo Finance can anticipate stock trading volumes by two or three days, resulting in a higher predictive power than that of previous work that used web searches to predict trading volume. We also observe that grouping stocks into industries or sectors decreases the predictive power, whereas moving from hourly to daily time series granularity improves predictive power. We corroborate our findings with a theoretical intuition and extensive statistical and causality tests.
   </div>
  </div>
 </div>
</p>

<div title="Page 1">
 <div>
  <div>
   Web traffic represents a powerful mirror for various real-world phenomena. For example, volumes of web searches have been shown to have a positive correlation with stock trading volumes and with the sentiment of investors. Our hypothesis is that user browsing behavior on a domain-specific portal is a better predictor of user intent than web searches.

We focus on the financial domain and we analyze the web browsing and trading data of more than 2,600 stocks traded on NYSE, Nasdaq, and S&P. The web browsing data consists of user page views related to stock S on Yahoo Finance, while the trading data includes the trading volume of S. We study the correlation and causality between web browsing and trading data while varying the time granularity (hourly, daily) and financial segmentation (individual tickers, industries, sectors).

We find that web browsing on Yahoo Finance can anticipate stock trading volumes by two or three days, resulting in a higher predictive power than that of previous work that used web searches to predict trading volume. We also observe that grouping stocks into industries or sectors decreases the predictive power, whereas moving from hourly to daily time series granularity improves predictive power. We corroborate our findings with a theoretical intuition and extensive statistical and causality tests.
  </div>
 </div>
</div>
<div>
 <div>
  Web traffic represents a powerful mirror for various real-world phenomena. For example, volumes of web searches have been shown to have a positive correlation with stock trading volumes and with the sentiment of investors. Our hypothesis is that user browsing behavior on a domain-specific portal is a better predictor of user intent than web searches.

We focus on the financial domain and we analyze the web browsing and trading data of more than 2,600 stocks traded on NYSE, Nasdaq, and S&P. The web browsing data consists of user page views related to stock S on Yahoo Finance, while the trading data includes the trading volume of S. We study the correlation and causality between web browsing and trading data while varying the time granularity (hourly, daily) and financial segmentation (individual tickers, industries, sectors).

We find that web browsing on Yahoo Finance can anticipate stock trading volumes by two or three days, resulting in a higher predictive power than that of previous work that used web searches to predict trading volume. We also observe that grouping stocks into industries or sectors decreases the predictive power, whereas moving from hourly to daily time series granularity improves predictive power. We corroborate our findings with a theoretical intuition and extensive statistical and causality tests.
 </div>
</div>

<div>
 Web traffic represents a powerful mirror for various real-world phenomena. For example, volumes of web searches have been shown to have a positive correlation with stock trading volumes and with the sentiment of investors. Our hypothesis is that user browsing behavior on a domain-specific portal is a better predictor of user intent than web searches.

We focus on the financial domain and we analyze the web browsing and trading data of more than 2,600 stocks traded on NYSE, Nasdaq, and S&P. The web browsing data consists of user page views related to stock S on Yahoo Finance, while the trading data includes the trading volume of S. We study the correlation and causality between web browsing and trading data while varying the time granularity (hourly, daily) and financial segmentation (individual tickers, industries, sectors).

We find that web browsing on Yahoo Finance can anticipate stock trading volumes by two or three days, resulting in a higher predictive power than that of previous work that used web searches to predict trading volume. We also observe that grouping stocks into industries or sectors decreases the predictive power, whereas moving from hourly to daily time series granularity improves predictive power. We corroborate our findings with a theoretical intuition and extensive statistical and causality tests.
</div>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Stock Trade Volume Prediction with Yahoo Finance User Browsing Behavior
ICDE 2014
[u'Ilaria Bordino', u'Nicolas Kourtellis', u'Nikolay Laptev', u'Youssef Billawala']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6583/distributed-confidence-weighted-classification-mapreduce
found
<h6>
 Abstract
</h6>

<p class="leading">
 Explosive growth in data size, data complexity, and data rates, triggered by the emergence of high-throughput technologies such as remote sensing, crowd-sourcing, social networks, and computational advertising, in recent years has led to an increasing availability of data sets of unprecedented scales, with billions of high-dimensional data examples stored on hundreds of terabytes of memory. In order to make use of this large-scale data and extract useful knowledge, researchers in machine learning and data mining communities are faced with numerous challenges, since the classification algorithms designed for standard desktop computers are not capable of addressing these problems due to memory and time constraints. As a result, there exists an evident need for the development of novel, more scalable algorithms that can handle large data sets. In this paper we propose such a method, called AROW- MR, a linear SVM solver for efficient training of recently proposed confidence-weighted (CW) classifiers. Linear CW models maintain a Gaussian distribution over parameter vectors, thus allowing a user to estimate parameter confidence, in addition to separating hyperplanes between two classes. The proposed method employs the MapReduce framework to train CW classifiers in a distributed way, obtaining significant improvements in both training time and accuracy. This is achieved through training local CW classifiers on each mapper, followed by optimally combining local classifiers on the reducer to obtain an aggregated, more accurate CW linear model. We validate the proposed algorithm on synthetic data, and further show that the AROW-MR algorithm outperforms the baseline classifiers on the industrial, large-scale task of Ad Latency prediction, with nearly one billion examples.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2013bigdata.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0-doc" title="">
 <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2013bigdata.pdf">
  Download
 </a>
</section>

<a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/djuric2013bigdata.pdf">
 Download
</a>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Distributed Confidence-Weighted Classification on MapReduce
IEEE International Conference on Big Data
[u'Nemanja Djuric', u'Mihajlo Grbovic', u'Slobodan Vucetic']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6584/large-scale-ad-latency-analysis
found
<h6>
 Abstract
</h6>

<p class="leading">
 Late web display advertisements are problematic for both the user experience perspective and the monetary machinery powering the display advertising industry. If a web page is delivered to a user but the ad fails to load in time, the publisher cannot charge the advertiser for that impression. Detecting whether a specific ad will render in time could give the publisher a choice to show that ad or another one. Further, discovering the root causes of the latency, possibly over time as new violators emerge, would allow the publisher to address the actionable issues. We propose a system that predicts, at serve time, which ads are likely to have high latency. Once identified we can either ignore those ads, even if they win the auction, or apply a penalty to those ads. In addition, our system collects the daily impression logs, which consist of different types of observations measured at serve time and the associated latency in milliseconds, and analyzes the data to identify the features associated with late ads likely to be causing the delay.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Large Scale Ad Latency Analysis
2013 IEEE International Conference on Big Data
[u'Mihajlo Grbovic', u'Jon Malkin', u'Hirakendu Das']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6590/measuring-inter-site-engagement
found
<h6>
 Abstract
</h6>

<p class="leading">
 Many large online providers offer a variety of content sites (e.g. news, sport, e-commerce). These providersendeavor to keep users accessing and interacting with their sites, that is to engage users by spending time using their sitesand to return regularly to them. They do so by serving users the most relevant content in an attractive and enticing manner.Due to their highly varied content, each site is usually studied and optimized separately. However, these online providers aimnot only to engage users with individual sites, but across all sites in their network. In these cases, site engagement should beexamined not only within individual sites, but also across the entire content provider network. This paper investigatesinter siteengagement, that is, site engagement within a network of sites, by defining a global measure of engagement that capturesthe effect sites have on the engagement on other sites. As an application, we look at the effect of web page layout andstructure, which we refer to as web page stylistics, on inter-site engagement on Yahoo properties. Through the analysisof 50 popular Yahoo sites, a sample of 265,000 users, and 19.4M online sessions, we demonstrate that the stylisticcomponents of a web page on a site can be used to predict inter-site engagement across the Yahoo network of sites. Intersiteengagement is a new big data problem as overall it implies analyzing dozen of sites visited by hundreds of millions ofpeople generating billions of sessions.
</p>

<div>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

EXCEPTION IN (main.py, LINE 81 "print abspt.prettify()"): 'Comment' object has no attribute 'prettify'
*********************
Measuring Inter-Site Engagement
IEEE International Conference on Big Data (BigData 2013), Santa Clara, CA, USA
[u'Mounia Lalmas', u'Ricardo Baeza-yates', u'Georges Dupret']
Undefined
Not FOUND
^^^^^^^^^^^^^^^^^^^^
 LINK 
https://labs.yahoo.com/publications/6603/probabilistic-topic-models-sequence-data
found
<h6>
 Abstract
</h6>

<p class="leading">
 Probabilistic topic models are widely used in different contexts to uncover the hidden structure in large text corpora. One of the main (and perhaps strongest) assumptions of these models is that generative process follows a bag-of-words assumption, i.e. each token is independent from the previous one. We extend the popular Latent Dirichlet Allocation model by exploiting three different conditional Markovian assumptions: (i)the token generation depends on the current topic and on the previous token; (ii)the topic associated with each observation depends on topic associated with the previous one; (iii)the token generation depends on the current and previous topic. For each of these modeling assumptions we present a Gibbs Sampling procedure for parameter estimation. Experimental evaluation over real-word data shows the performance advantages, in terms of recall and precision, of the sequence-modeling approaches.
</p>

<div>
 <section data-variation="v0-doc" title="">
  <a class="f-a00v0" href="https://s.yimg.com/ge/labs/v2/uploads/Probabilistic-topic-models-for-sequence-data.pdf">
   Download
  </a>
 </section>
 <!--<section title="" data-variation="v1-doc-example">
                                <div class="f-a02v1">
                                    <a class="f-a02_link" href="labs.publication.html#link">Previous Publication</a>
                                    <a class="f-a02_link" href="labs.publication.html#link">Next Publication</a>
                                </div>
                            </section>-->
</div>

<section data-variation="v0